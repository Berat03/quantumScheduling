
%% L4-project-paper-template.tex
%% v1.1
%% Dec, 2022
%% Craig Stewart
%% for Durham University, Computer Science Project paper templates
%% contact craig.d.stewart at durham.ac.uk for support
%%
%% Based on IEEE Template: bare_jrnl_compsoc.tex, V1.4b, by Michael Shell
%%
%% Notice from original IEEE Template:
%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


\documentclass[10pt,journal,compsoc]{IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


%% ---------------------------------------------- START OF USEFUL PACKAGES ----------------------------------------------

%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e

%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.

%% ---------------------------------------------- END OF USEFUL PACKAGES ----------------------------------------------

\usepackage{comment}
\usepackage{float}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Fair Entanglement Swapping Policies in Multi-User Quantum Networks}
%
%
% author  

\author{Student Name: Berat Bulbul\\Supervisor Name: Dr Thirupathaiah Vasantam\\
Submitted as part of the degree of MSci Computer Science \& Mathematics to the\\
Board of Examiners in the Department of Computer Science, Durham University
}



% The paper headers
\markboth{DURHAM UNIVERSITY, DEPARTMENT OF COMPUTER SCIENCE}%
{Shell \MakeLowercase{\textit{et al.}}}

\IEEEtitleabstractindextext{
\begin{abstract}
As quantum networks scale, ensuring fair and timely access to entanglement becomes increasingly challenging—particularly due to the probabilistic and short-lived nature of quantum resources. This paper explores fairness-aware entanglement scheduling in multi-user quantum networks, where greedy or throughput-maximizing policies can lead to persistent user starvation. We model the problem as a Markov Decision Process (MDP) and apply reinforcement learning techniques—including Q-learning, Value Iteration, and N-step SARSA—to dynamically manage entanglement swapping under memory constraints and stochastic link behaviour. Our approach incorporates a log-ratio fairness reward based on the historical Entanglement Distribution Rate (EDR), designed to reflect the temporal fragility of entanglement. Through simulations on structured network topologies and varying physical parameters, we show that learning-based policies substantially improve runtime fairness with minimal efficiency loss. These results highlight the potential of adaptive control strategies in future quantum networks, laying the foundation for more equitable, large-scale quantum communication systems.
\end{abstract}


\begin{IEEEkeywords} 
Network modeling, Quantum computing, Reinforcement learning, Scheduling 
\end{IEEEkeywords}}
%% --------------------------------------------- DO NOT CHANGE ---------------------------------------------

% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.
% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.

%% --------------------------------------------- DO NOT CHANGE --------------------------------
\subsection{Introduction}
\IEEEPARstart{Q}{uantum} networks have the potential to fundamentally transform how information is transmitted, processed, and secured. These systems interconnect quantum devices via entangled qubits—quantum bits whose states are intrinsically correlated, enabling non-classical communication protocols rooted in quantum phenomena. Applications such as Quantum Key Distribution (QKD) \cite{bennett1984quantum}, distributed quantum computing, and privacy-preserving communication rely critically on the ability to generate and manage entanglement across geographically separated nodes. However, quantum information is inherently fragile: qubits cannot be cloned, measured without disturbance, or perfectly relayed without loss.

Recent advances have led to small-scale experimental quantum networks, motivated in part by the threat of "store now, decrypt later" attacks, where adversaries harvest encrypted classical data today for decryption by future quantum computers. This urgency highlights the need for quantum-secured communication systems that are fundamentally resistant to eavesdropping.

A central challenge in quantum networking is the fair allocation of entangled resources among competing users, under conditions where links are unreliable, memory coherence is limited, and entanglement decays rapidly. Unlike classical networks, where bandwidth or buffer space can be reliably shared or delayed, entangled qubits degrade over time and cannot be copied or buffered indefinitely. Without fairness-aware scheduling, some users may experience persistent service denial, leading to severe under-utilization and application failure. For example, users relying on longer, lower-reliability paths may be systematically starved of entanglement opportunities compared to users on direct or higher-fidelity links.

We define \textbf{entanglement scheduling} as the process of dynamically deciding when and how to consume available entangled links, through actions such as swapping or waiting, in order to optimize network objectives under uncertainty. In this work, our primary focus is on fairness as a first-order concern, in contrast to classical systems where fairness can often be retroactively enforced.

To address these challenges, we model entanglement scheduling as a dynamic decision-making problem under uncertainty and propose reinforcement learning (RL) as a solution framework. RL enables the development of adaptive policies that reason over the stochastic and temporal constraints unique to quantum systems. To isolate the core scheduling challenges, we abstract away physical-layer complexities such as entanglement purification, quantum error correction, and fidelity drift, enabling tractable learning and evaluation under structured network models.

Our solution operates within a simulation-based environment, where time is discretized, and link and memory behaviours are parametrized to reflect near-term experimental capabilities. While this abstraction simplifies physical details, it enables a focused exploration of scheduling strategies essential for scalable, real-time quantum network control. Addressing scalability to larger, more heterogeneous networks remains an important direction for future research.

\subsection{Quantum Phenomena}
Quantum networks operate using \textbf{qubits}, which, unlike classical bits, exhibit superposition and entanglement—fundamentally different behaviours critical for quantum communication. Photons are a common physical realization of qubits, with polarization encoding the quantum state, allowing direct integration with existing fibre-optic infrastructure.

\textbf{Entanglement} is a foundational resource, wherein two qubits are correlated such that measurement outcomes are instantaneously linked, regardless of distance. This phenomenon underpins secure communication protocols and is constrained by the \textbf{no-cloning theorem}, which prohibits copying unknown quantum states. Consequently, quantum information cannot be replicated or passively intercepted, reinforcing the need for careful, fairness-aware management of entangled resources. Throughout this work, we refer specifically to entangled qubits forming the operational backbone of quantum networking.

Another critical operation is \textbf{quantum teleportation}, which enables transferring an unknown qubit state between nodes using a shared entangled pair and classical communication. Teleportation highlights the practical necessity of reliable, dynamically scheduled entanglement distribution.

\subsection{Quantum Networking}
Because quantum states cannot be amplified, long-distance quantum communication relies on \textbf{quantum repeaters}, which extend entanglement through \textbf{entanglement swapping} at intermediate nodes.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/basicSwapBell.jpg}
  \caption{Entanglement swapping between Alice, Bob, and Carol, creating long-range entanglement through local operations.}
  \label{fig:my_plot1}
\end{figure}

Recent progress in entangled photon sources and quantum memories is making hybrid quantum-classical network architectures feasible, leveraging existing optical fibre infrastructure for early deployment.

However, quantum communication remains fragile: qubits are susceptible to \textbf{decoherence} from environmental noise, which degrades entanglement quality over time. Techniques such as \textbf{entanglement purification} seek to mitigate these effects, but practical limitations persist. Moreover, entanglement generation and swapping are \textbf{probabilistic} operations, limited by photon loss, imperfect gates, and finite memory coherence.

To capture these constraints, we model quantum networks with discretized time and introduce \textbf{entanglement age}, allowing entangled links to persist temporarily before decohering. These abstractions enable realistic yet tractable simulation of quantum networks.

\subsection{Fairness in Networking}
Fairness in quantum networks requires ensuring that all users have timely access to entanglement resources. Unlike classical networks, where fairness can be achieved over long time scales, quantum fairness is inherently \textbf{temporal}: missed entanglement opportunities are irrecoverable.

Greedy or purely throughput-maximizing scheduling policies risk \textbf{starvation}, where certain users are perpetually denied service due to network topology or probabilistic failures. For instance, a user relying on a longer multi-hop path may see exponentially lower success rates than one on a direct link, unless fairness is explicitly enforced.

Achieving fairness in quantum networks thus demands anticipatory, memory-aware scheduling that prioritizes equitable access under uncertainty.

\subsection{Reinforcement Learning}
Reinforcement learning (RL) is naturally suited to quantum network scheduling, where actions such as entanglement swaps have delayed, probabilistic, and irreversible outcomes. Unlike simple heuristics, RL agents can learn policies that optimize long-term fairness and resource utilization.

We model the scheduling task as a \textbf{Markov Decision Process (MDP)}, defined by a state space \(\mathcal{S}\), action space \(\mathcal{A}\), a stochastic transition model, and a reward function. The Markov property ensures that decisions depend only on the current state and action, facilitating principled policy learning.

Learned policies adapt in real-time to dynamic network conditions, such as memory availability, link reliability, and contention, providing a scalable basis for fairness-aware quantum control.

\subsection{Research Focus}
This work investigates the design of \textbf{fair entanglement scheduling policies} in multi-user quantum networks. Unlike classical networks, fairness in quantum systems must be enforced in real-time, as entangled resources decay irreversibly.

Prior research has primarily optimized throughput or fidelity, often neglecting fairness among competing users. In contrast, we frame entanglement scheduling as an MDP and explore three RL strategies—\textbf{Q-learning}, \textbf{Value Iteration}, and \textbf{N-step SARSA}—to develop fairness-aware scheduling policies.

We introduce a \textbf{log-ratio fairness reward} structure that dynamically incentivizes serving underserved users, embedding fairness directly into the learning process rather than as an external metric. Our experiments use discrete-time simulation environments to isolate the core scheduling dynamics under structured topologies representative of near-term quantum networks.

While our present work focuses on mid-scale, structured networks, future research will explore scalable, decentralized scheduling strategies and incorporate richer physical-layer effects such as fidelity decay and entanglement purification.

\subsection{Summary of Contributions}
We make the following key contributions:
\begin{itemize}
    \item We propose a log-ratio-based fairness reward function that dynamically incentivizes equitable entanglement scheduling under stochastic link conditions.
    \item We model fairness-aware entanglement scheduling as a Markov Decision Process and implement two reinforcement learning strategies suited to delayed and probabilistic outcomes.
    \item We introduce runtime fairness metrics tailored to quantum networks, emphasizing the non-recoverability of missed entanglement opportunities.
    \item We conduct extensive empirical evaluations across structured topologies and physical parameter regimes, demonstrating that learning-based policies outperform greedy baselines in both fairness and efficiency.
    \item We systematically explore the effects of swapping strategies and physical parameters, providing insight into the dynamics of quantum network behavior and informing the design of future protocols.
    \item We develop an extensible simulation framework that enables future research into scalability, decentralized control, richer physical modelling, and broader network architectures.
\end{itemize}

Our work highlights the unique challenges of fairness-aware scheduling in quantum networks, establishes a reproducible evaluation framework, and provides a foundation for equitable resource management in hybrid and future distributed quantum systems. Future extensions of this work promise a wide range of open research directions across network scalability, topology diversity, physical-layer modelling, and decentralized decision-making.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\subsection{Fairness in Networking}
\subsubsection{Fairness Metrics}
Fairness in networking is typically formalized through metrics that quantify how evenly resources are allocated across users. Common examples include Jain’s Fairness Index \cite{jain1984quantitative}, max-min fairness \cite{radunovic2007unified}, and proportional fairness \cite{kelly1997charging}, each reflecting a different equity–efficiency trade-off.

Jain’s Index,
\[
J = \frac{\left( \sum_i x_i \right)^2}{n \sum_i x_i^2},
\]
is simple and bounded in $[1/n, 1]$, but it does not capture temporal starvation or path-specific disparities. Max-min fairness guarantees the most disadvantaged user receives the highest possible minimum allocation, often at the cost of overall throughput. Proportional fairness ensures that any resource gain by one user would incur a proportional loss to others, providing a balanced compromise between fairness and efficiency.

The general framework of $\alpha$-fairness \cite{mo2000alphafair} expresses these trade-offs through a parametric utility function:
\[
U(x) =
\begin{cases}
\sum_{i=1}^n \dfrac{x_i^{1 - \alpha}}{1 - \alpha}, & \text{if } \alpha \neq 1 \\
\sum_{i=1}^n \log x_i, & \text{if } \alpha = 1
\end{cases}
\]
Here, $x_i$ denotes the allocation for user $i$, and $\alpha \geq 0$ controls the trade-off between fairness and efficiency. When $\alpha = 0$, the formulation maximizes total throughput; $\alpha = 1$ recovers proportional fairness; and as $\alpha \to \infty$, the allocation converges to max-min fairness. This tunable approach is particularly relevant in multi-tenant or resource-constrained quantum networks.

Utility-based formulations extend these models by allowing for application-specific reward functions. While flexible, they often require careful shaping and prior knowledge, which can be challenging in environments characterized by uncertainty, partial observability, or temporal decay—conditions typical of quantum networks.

The Gini coefficient \cite{gini1912variability} quantifies inequality by computing the average pairwise difference in allocations, normalized by the mean. It is more sensitive to mid-rank disparities than Jain’s Index and can reveal distributional skew that Jain may miss. However, it requires careful normalization in the presence of zero allocations—a common occurrence in quantum networks due to probabilistic failures or entanglement scarcity.

Entropy-based fairness metrics, typically derived from Shannon entropy \cite{shannon1948mathematical},
\[
H = -\sum_i p_i \log p_i, \quad \text{where} \quad p_i = \frac{x_i}{\sum_j x_j},
\]
capture the unpredictability or diversity of service allocation across users. High entropy indicates more uniform and equitable usage patterns, which may be useful for analyzing runtime fairness. However, entropy becomes undefined when any $x_i = 0$, and its interpretation depends on normalization (e.g., $H/\log n$) to keep values in $[0,1]$.

Because quantum entanglement allocation is subject to stochastic loss and may result in prolonged starvation for some user pairs, zero allocations are not rare. This makes Gini and entropy-based metrics harder to interpret or apply directly. Accordingly, we adopt a log-ratio utility function that embeds proportional fairness without the normalization difficulties associated with Gini or entropy, and which provides smooth gradients and stable learning signals.


\subsubsection{Applications \& Algorithms}
Fairness principles underpin the design of many core networking algorithms. For example, Weighted Fair Queuing (WFQ) allocates bandwidth among competing flows proportionally, allowing higher-priority traffic to receive more resources while maintaining equity \cite{demers1990analysis}. Congestion control protocols like TCP Reno and TCP Cubic adjust flow rates based on packet loss, enabling decentralized fairness across competing connections \cite{ha2008cubic}. In wide-area internet routing, fairness considerations typically arise indirectly—through traffic engineering, policy routing, and congestion avoidance mechanisms—rather than through explicit equity metrics.

Fairness-aware scheduling is also critical in multi-tenant cloud and data centre environments, where resource allocation must balance performance with tenant isolation to prevent monopolization \cite{danna2012practical}. These systems often rely on hierarchical schedulers, share caps, or burst buffers to enforce runtime fairness under dynamic load. 

Security and fairness are closely linked in practice. Without proper safeguards, users may exploit scheduling policies to gain disproportionate access—e.g., by spamming low-cost requests or occupying bottleneck paths. Isolation mechanisms, including token-bucket shaping or cryptographic authentication, are often necessary to maintain fairness guarantees in adversarial settings. While quantum networks introduce additional trust assumptions (see QKD \cite{bennett1984quantum}), especially in repeater-based architectures, fairness must still be robust to malicious or misbehaving tenants.


\subsubsection{Fairness Trade-offs}
Fairness in networking often comes at the cost of reduced throughput or computational efficiency, particularly in large-scale or time-sensitive systems. Algorithms that enforce equitable access may slow down faster flows or under-utilize high-capacity paths. For instance, TCP Cubic prioritizes throughput, which can disadvantage competing flows \cite{ha2008cubic}.

To balance these trade-offs, many systems adopt multi-objective optimization strategies. In reinforcement learning, a common approach is to use a composite reward function of the form:
\[
R_t = \theta \cdot \text{Throughput}_t + (1 - \theta) \cdot \text{Fairness}_t
\]
where \( \theta \in [0, 1] \) adjusts the weight between utility and equity. While flexible, this formulation often suffers from gradient conflict and scale inconsistency during training, especially in stochastic or sparse environments.

Alternative reward structures attempt to address these issues. Partial-credit rewards, for example, grant non-zero utility for near-successful entanglement attempts, encouraging exploration in low-probability settings. Threshold rewards, by contrast, only provide feedback once a minimum fidelity or distribution count is achieved—emphasizing reliability but increasing reward sparsity.

Lastly, it is important to distinguish between \textit{final} and \textit{runtime} fairness. Final fairness evaluates long-term distribution, whereas runtime fairness captures short-term inequities or starvation events. In quantum networks—where entanglement rapidly decays—runtime fairness is particularly important, as delayed compensation cannot recover lost opportunities.










\subsection{Quantum Networking}

\subsubsection{Current Applications \& Research}
Quantum networking remains in an early deployment phase, with progress largely confined to controlled experimental settings due to hardware fragility, limited coherence times, and high operational costs. A notable milestone is China’s Micius satellite, which successfully demonstrated quantum key distribution (QKD) over long distances via satellite links \cite{liao2017satellite}. However, such demonstrations typically depend on trusted relay nodes rather than achieving true end-to-end entanglement, which remains elusive due to the challenges of photon loss and decoherence over large distances.

To move beyond point-to-point demonstrations, recent efforts have focused on designing scalable, multi-user quantum network architectures that support dynamic entanglement distribution. Pant et al.~\cite{pant2017routing} introduce routing strategies that exploit path diversity and repeater placement to improve entanglement throughput under realistic physical constraints. Complementary work by Iñesta et al.~\cite{inesta2023optimal} focuses on single-user applications, formulating optimal scheduling in linear repeater chains using MDPs and showing that globally informed policies consistently outperform heuristic strategies such as swap-as-soon-as-possible in noisy environments. Their approach serves as a key inspiration for our work, motivating our use of decision-theoretic methods in fairness-aware multi-user scheduling. Related work by Kumar et al.~\cite{kumar2023optimal} applies MDPs to quantum switches, demonstrating that distillation-aware scheduling improves throughput and jitter under decoherence.

Together, these studies mark a shift from hardware-only considerations toward algorithmically enhanced quantum networking—where intelligent control policies and decision models increasingly guide the design of resilient, scalable quantum internet.




\subsubsection{Quantum Repeaters \& Switches}
Quantum networks may incorporate repeaters or switches to support multi-hop entanglement, though these components differ significantly in architecture and scheduling logic.

Repeaters facilitate entanglement swapping across intermediate nodes and are commonly classified into three generations \cite{yan2021generation}: first-generation repeaters perform only swapping; second-generation incorporate purification; and third-generation implement full quantum error correction. Due to current hardware constraints, first-generation repeaters—often relying on trusted nodes—are the most widely used. Swapping strategies in these systems include greedy, nested, and wait-based approaches such as "swap-as-soon-as-possible," each offering distinct trade-offs under constraints like memory size and link reliability \cite{inesta2023optimal}.

Switches, by contrast, have been modelled using tools such as queueing theory and Markov Decision Processes. Bhambay et al. analyse general switch topologies, showing that optimal scheduling aligns with average-reward MDP solutions \cite{thiru2025optimal}. Kumar investigates distillation-aware scheduling in bipartite switches under decoherence, comparing the effectiveness of MDP-based and reinforcement learning policies \cite{kumar2023optimal}. Vardoyan et al. explore capacity bounds in switches subject to purification constraints, characterizing their performance in noisy settings \cite{vardoyan2023capacity}.

While our study centers on scheduling within repeater-based networks, insights from switch-based models remain highly relevant. Issues such as fairness, memory management, and probabilistic decision-making are common to both architectures, suggesting that advances in switch-level policy design can inform scalable strategies for broader quantum networking.


\subsubsection{Limitations in Implementation} \label{sec:timeTaken}
Quantum networks face physical and algorithmic challenges that severely constrain entanglement reliability. Key limitations include photon loss, decoherence, detector inefficiencies, and the probabilistic nature of entanglement generation and swapping. Reported success probabilities are often as low as $10^{-3}$ to $10^{-6}$, depending on link length and hardware type \cite{bhaskar2020experimental}. While trapped-ion platforms can support near-deterministic Bell-state measurements, optical systems typically achieve only $\sim$50\% swap fidelity \cite{duan2001long}.

To counter fidelity loss, purification protocols can improve entangled pair quality but reduce throughput by up to 50\% and introduce delays \cite{bennett1996purification}. Quantum memories further constrain scheduling, with coherence times ranging from milliseconds to a few seconds and fidelity decaying exponentially with age \cite{lvovsky2009optical}. These factors motivate our discrete-time abstraction, where each timestep models 10–100 microseconds, mapping 1,000 steps to roughly 0.01–0.1 seconds of real time—enough for memory decay to occur in volatile systems.

Because expired links are unrecoverable, scheduling decisions must prioritize fairness over short windows, rather than relying on long-term averages as in classical systems. This requires balancing between early, low-fidelity swaps (favoring throughput) and delayed, high-fidelity swaps (risking starvation). Even fairness-aware classical algorithms like TCP or WFQ assume moderately reliable links—assumptions that break down under the stochastic failure rates and aging behaviors of quantum settings.

Control signalling also adds latency, requiring microsecond-level synchronization across distant nodes. Even with optimistic assumptions about classical signalling, coordination delays can cause link expiries independent of scheduling choices \cite{bhaskar2020experimental}.

Together, these constraints demand fairness-aware scheduling policies that:
\begin{itemize}
    \item Operate under low, variable success rates;
    \item React to ageing via memory-aware prioritization;
    \item Balance purification trade-offs between fidelity and rate;
    \item Function effectively despite control-plane jitter or feedback delays.
\end{itemize}
Our use of discrete-time simulation and proportional fairness rewards reflects these conditions, offering a tractable yet representative model of early-stage quantum networks.


\subsubsection{Simulation Environments}
Quantum network simulators such as NetSquid \cite{netsquid2023} and QuNetSim \cite{qunetsim2020} are widely used for modeling low-level quantum behavior. NetSquid offers detailed physical-layer fidelity, including gate noise, memory decoherence, and time synchronization—making it well-suited for protocol verification and hardware-specific benchmarking. QuNetSim, by contrast, emphasizes modularity and rapid prototyping of higher-level quantum protocols.

However, neither tool is tailored to study fairness in entanglement scheduling. Their fine-grained hardware modelling introduces significant overhead and variance, which complicates learning-based policy evaluation. Moreover, integrating reinforcement learning with these platforms often requires non-trivial interfaces ,and, adds complexity and noise unrelated to scheduling logic.

\subsubsection{Road Map}
Quantum networking is advancing from isolated experiments toward scalable, real-world deployment \cite{liao2017satellite}. Key priorities include developing robust quantum repeaters and transducers to extend entanglement over long distances without excessive loss. 

A parallel effort focuses on defining a full-stack quantum network architecture, analogous to the classical OSI model. Standardizing interfaces between quantum and classical layers is essential for modularity, interoperability, and scaling. Software abstractions and intelligent control policies—such as fairness-aware schedulers—will be critical for adapting to hardware variability and supporting multi-user access.

National and international initiatives are now emphasizing infrastructure investment, common metrics, and early-stage hybrid deployments that integrate quantum technologies with existing fiber networks. As hardware matures, these hybrid systems will support secure communication, distributed sensing, and eventually cloud-based quantum computing at scale \cite{nqiac2024}.


\subsection{Reinforcement Learning}
\subsubsection{Reinforcement Learning for Networking \& Fairness}
Reinforcement Learning (RL) is particularly well-suited to networking environments characterized by dynamic, sequential decision-making under uncertainty. In classical networks, RL has been applied to routing, congestion control, scheduling, and spectrum management, where agents iteratively optimize performance based on feedback such as throughput, latency, or fairness. Standard algorithms—including Q-learning, SARSA, and Deep Q-Networks (DQN)—have achieved notable success in wireless and multi-path scenarios \cite{sutton2018reinforcement, ha2008cubic, lopez2022latency}.

In quantum networking, RL methods offer key advantages over rule-based strategies, particularly in the presence of probabilistic entanglement generation and limited observability. Deep RL models like QuDQN have demonstrated superior performance in entanglement routing tasks constrained by fidelity and memory, outperforming heuristics under stochastic link behavior \cite{jallow2025qudqn}. Multi-step approaches such as N-step SARSA further enhance learning in sparse-reward settings—critical for quantum networks, where successful entanglement events are infrequent and temporally delayed \cite{sutton2018reinforcement}.

Integrating fairness into RL has also gained traction, especially in systems with shared, constrained resources. In classical networks, fairness-aware strategies often incorporate Jain’s Index or $\alpha$-fairness into the reward signal, enabling agents to balance throughput with equity. Advanced techniques—such as Double DQN, actor-critic models, and Pareto-efficient multi-agent learning—have improved fairness in heterogeneous and resource-limited settings \cite{emara2022pareto, chen2021actorcritic, han2025jury}.

In the context of quantum networks, fairness-aware RL is increasingly relevant. The short-lived and stochastic nature of entanglement makes it easy for certain users to be persistently deprived of access. By embedding fairness directly into the reward structure, RL agents can learn policies that equitably allocate entanglement, even as they adapt to dynamic network conditions and probabilistic link behavior.


\subsubsection{State-of-the-Art Methods}
Recent advances in fairness-aware reinforcement learning show strong potential for managing resource allocation in both classical and quantum networks. In the quantum setting, QuDQN \cite{jallow2025qudqn} introduces a Deep Q-Network-based routing framework that adapts to dynamic network conditions while respecting fidelity and memory constraints. The model integrates quantum-specific parameters—such as qubit availability, fidelity thresholds, and entanglement success probabilities—and uses a learned Q-value function to guide routing decisions. Compared to heuristic baselines, QuDQN significantly improves throughput, request completion, and resource efficiency, making it well-suited to the challenges of quantum network environments.

In classical networks, RL has been applied to fairness-sensitive tasks like congestion control and scheduling. For example, $\beta$-M-LWDF \cite{lopez2022latency} uses deep Q-learning to dynamically tune fairness-control parameters in 5G systems, improving both delay and fairness outcomes. Beyond single-agent models, recent approaches apply multi-objective optimization—such as Pareto front methods \cite{emara2022pareto}—to train distributed agents using shared fairness-aware rewards, optimizing trade-offs between throughput and equity. Jury \cite{han2025jury} decouples fairness from raw throughput decisions, generalizing well to unseen conditions via a learned post-processing fairness layer. In actor-critic frameworks, Chen et al. \cite{chen2021actorcritic} apply reward-scaling strategies based on $\alpha$-fairness, showing convergence to fair equilibria in wireless networks.

Together, these works reflect a broader shift toward embedding fairness directly into RL training—laying the groundwork for applying similar ideas to quantum networks, where fairness is further complicated by stochastic dynamics and short-lived entanglement.


%\subsubsection{Defining Fairness in Quantum Contexts}
%While fairness is a well-established goal in classical networks, its role in quantum systems remains relatively underexplored. Most existing work prioritizes metrics like fidelity, success rate, and throughput, overlooking equity in entanglement access or resource allocation. However, fairness issues do arise — such as unequal access to entanglement, variable queuing delays, and stochastic link performance. These concerns highlight the need for fairness definitions that account for the probabilistic and short-lived nature of quantum resources.

%A key distinction is temporal fragility: entangled links decohere quickly, making delayed fairness ineffective. Unlike classical systems, where fairness can be averaged over time, quantum networks require fairness to be enforced over short windows. Similar constraints appear in stochastic job-shop scheduling and MDP-based systems, where fairness must account for uncertainty and missed opportunities \cite{zhang2017realtime}.

\subsubsection{Challenges}
Reinforcement learning in quantum networks presents several distinct challenges arising from the environment’s inherent stochasticity, temporal fragility, and lack of established benchmarks. Entanglement generation and swapping are fundamentally probabilistic and often delayed, making it difficult for agents to attribute outcomes to actions. These rewards are also highly dynamic—fluctuating with user demands, hardware reliability, and decoherence rates—which requires adaptive policies capable of handling non-stationary environments.

Partial observability adds further complexity. In real-world or large-scale scenarios, agents often operate with only local or delayed information, limiting the effectiveness of tabular methods and favouring deep RL approaches with function approximation. Moreover, memory decay and low entanglement success rates increase variance and slow convergence—especially in sparse-reward settings.

Scalability is another major obstacle. As quantum networks grow in size and complexity, the state-action space grows exponentially, making exhaustive learning impractical. Multi-agent deployments further amplify this challenge by requiring coordination, decentralized decision-making, and consistent fairness enforcement across distributed agents.

Finally, a key research bottleneck is the absence of standardized quantum networking metrics, evaluation protocols, and benchmarking environments. Unlike classical networking, where throughput, latency, and fairness are well-defined and widely accepted, quantum networking lacks unified metrics—particularly for fairness and efficiency under decoherence and stochastic delivery. We explore this issue in Section~\ref{sec:performanceMetrics}, where we adopt EDR-based metrics and Jain’s Index as tractable, interpretable proxies for runtime fairness in probabilistic quantum settings.


\subsubsection{Alternative Solutions}
While our approach focuses on model-based reinforcement learning, several alternative learning frameworks offer complementary advantages—particularly for large-scale or distributed quantum networks.

\textbf{Actor-critic methods} combine value estimation with direct policy optimization, enabling smoother updates and improved convergence in complex environments. These models can incorporate fairness objectives directly into the policy gradient, allowing agents to learn nuanced trade-offs between throughput and equity. However, actor-critic approaches are sensitive to reward shaping and can suffer from instability in sparse-reward regimes typical of quantum networks, where successful entanglement events are rare and delayed.

\textbf{Deep Q-Networks (DQN)} use neural networks to approximate value functions in high-dimensional spaces. Extensions like Double DQN and Dueling DQN improve stability and exploration efficiency, making them well-suited for quantum environments with dynamic link states and aging quantum memories. Still, deep RL models often require large training datasets, careful hyperparameter tuning, and significant compute resources—factors that may limit their practicality in early-stage or real-time quantum systems.

\textbf{Multi-agent reinforcement learning (MARL)} frameworks allow distributed agents—such as repeaters or switches—to learn decentralized policies while interacting with a shared environment. These systems are particularly promising for quantum networks lacking a central controller. However, MARL introduces new coordination challenges: non-stationarity from concurrent learning agents, fairness drift due to policy divergence, and increased sample complexity. Synchronizing agents across probabilistic quantum links can also add communication overhead and jitter, exacerbating temporal fairness issues. While well-suited to decentralized control, we avoid multi-agent RL in this work due to coordination complexity, sample inefficiency, and synchronization overhead—especially problematic in sparse, probabilistic quantum networks.



\textbf{Heuristic strategies} like round-robin, greedy swapping, or fixed priorities remain widely used in practice due to their simplicity, low computational cost, and interpretability. While they are effective in small or balanced networks, they lack the adaptability required for dynamic, fairness-sensitive quantum scheduling and may lead to persistent starvation under skewed load conditions.

\textbf{Optimization-based methods}, such as linear programs embedding $\alpha$-fairness objectives, provide provable guarantees under fully known models. However, they assume centralized control and full observability, limiting their utility in partially observable or latency-sensitive networks.

Overall, these alternative approaches offer rich design flexibility, but they also introduce trade-offs in complexity, interpretability, and robustness. Their success in quantum settings depends on careful integration with the physical and temporal constraints unique to entanglement-based communication.








\subsection{Summary of Literature Review}
[WRITE BEFORE SUBMISSION, AS LIT REVIEW WILL CHANGE]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{Problem Specification and System Design}

This section formalizes the quantum entanglement scheduling problem as a reinforcement learning (RL) task and outlines the system used to develop and evaluate fairness-aware policies. We model the environment as a Markov Decision Process (MDP), define key abstractions that simplify the physical layer, and describe the scheduling semantics and reward structure that shape agent behavior.

Our goal is to learn entanglement scheduling strategies that balance fairness and throughput under stochastic, resource-constrained conditions. We develop a simulation-based framework in which RL agents interact with dynamic network environments, allowing for quantitative evaluation of fairness and efficiency metrics across varying physical regimes and topologies.

\subsection{Modelling Assumptions and Abstractions}

\subsubsection*{Temporal and Observability Assumptions}

We assume discrete-time operation ($t = 0, 1, 2, \ldots$), with each timestep representing a quantum network "cycle" during which entanglement generation, swapping, and memory aging occur. Entanglement swaps are modelled as instantaneous operations, abstracting away real-world physical timescales. This facilitates synchronous learning, simplifies temporal reasoning around multi-hop swaps, and reduces the complexity of coordination.

Full global observability is assumed: the agent has immediate, perfect knowledge of the network’s entanglement state via classical communication channels. Although idealized, this mirrors emerging hybrid quantum-classical architectures \cite{bennett1984quantum}, where classical systems coordinate quantum operations. Full observability is critical for embedding fairness-sensitive statistics (e.g., delivery rates) into the agent’s observation space, and avoids the challenges of decentralized credit assignment—a significant open problem in multi-agent RL.

While these assumptions simplify the environment and may exceed practical capabilities of real-world systems, they prioritize computational tractability and isolate the core scheduling problem, providing a controlled and reproducible basis for evaluating fairness-aware policies.

\subsubsection*{Entanglement Modelling and Memory Constraints}

Each entangled link is represented by a discrete-valued age counter that increments with each timestep. Links are discarded when their age exceeds a maximum threshold, \texttt{maxAge}, thereby simulating decoherence. Swap success probability remains constant throughout a link’s lifetime; fidelity degradation over time is omitted to preserve interpretability and simplify the environment’s transition structure.

We allow only a single entangled qubit per edge at any time, preventing multiplexed entanglement. This bounds the state space and emphasizes strategic foresight under tight memory constraints. The network state is compactly encoded as a tuple of edge-age pairs, capturing the full quantum memory landscape while enabling scalable linear function approximation methods (see Section~\ref{sec:linearapprox}).

Initially, we experimented with precomputing all possible transition outcomes (generation, failure, and expiration) for each edge-age pair to accelerate simulation. However, this approach was ultimately abandoned due to prohibitive memory demands in larger networks. Furthermore, it assumes static transition dynamics and does not generalize to settings with non-stationary physical parameters.

\subsubsection*{Fairness Encoding and State Representation}

To enable fairness-aware reinforcement learning without violating the Markov property, the agent’s observation space is augmented with normalized Entanglement Delivery Rates (EDRs) computed over a fixed rolling window. These continuous statistics represent the recent success history for each goal and capture the sufficient statistics required for the fairness-driven reward function.

Early attempts at discretizing EDRs into coarse bins proved unstable, particularly in low-success environments where delivery events are sparse and highly sensitive. Sparse topologies exacerbated this volatility, as few opportunities exist to compensate for under-service. Consequently, we employ continuous normalized EDRs, enabling finer temporal tracking and improved learning stability.

We compute EDRs using a fixed-length rolling window rather than global averages. This promotes \textit{temporal fairness}—equalizing service over recent history rather than across the entire learning horizon. Window sizes of 100--1000 steps were chosen to align with simulated qubit coherence times (\texttt{maxAge} between 1 and 10 timesteps), corresponding to nanoseconds to milliseconds in real quantum devices (see Section~\ref{sec:literatureLifetimes}).

Without this augmentation, the fairness-aware reward function would depend on unobserved historical information, violating the Markov assumption and severely limiting the efficacy of reinforcement learning. Empirical validation confirmed this: early experiments relying solely on structural state features performed worse than naive baselines due to poor reward alignment.

\subsubsection*{Physical Simplifications and Routing Constraints}

We abstract away several physical-layer mechanisms common in practical quantum networks, such as purification, quantum error correction, and fidelity decay, to focus purely on the scheduling problem. Entanglement generation and swapping are modelled with fixed per-edge success probabilities, \(p_{\text{gen}}\) and \(p_{\text{swap}}\), maintaining environment stationarity across time and topology.

Routing paths are statically assigned and known a priori. Swaps are only attempted when all required links along a path are entangled. Dynamic routing, while important in practice, is deferred to future work to avoid conflating scheduling decisions with path optimization strategies. This design mirrors the modular separation of routing and transport logic in classical network architectures.

If multiple asymmetric paths exist for the same goal, the fairness-aware reward function resolves conflicts implicitly by prioritizing under-served goals, without requiring explicit path selection logic.

\subsubsection*{Swap Failure and Fidelity Semantics}

Swapping is modelled as a compound Bernoulli process: each individual swap succeeds independently with probability \(p_{\text{swap}}\), and the total success probability for an $n$-hop path is \(P_{\text{success}} = p_{\text{swap}}^n\). This formulation captures the exponential fragility of long-range entanglement.

Crucially, swap failure is modelled as terminal: if any hop fails, all involved entanglements are discarded. No partial success or fallback is considered. Fidelity remains constant until expiration; age-dependent fidelity decay is omitted. These choices avoid the substantial complexity that would arise from modelling intermediate or degraded states and maintain the tractability of the state space.

While modelling partial swap outcomes could improve physical realism, it would introduce significant complexity and shift the problem towards fault-tolerant recovery—an orthogonal challenge. Given our focus on fairness-aware scheduling, we maintain a binary success/failure model for swaps, reserving more granular fidelity modelling for future work.

\subsubsection*{Network Topology and Environment Design}

We model networks as static, undirected graphs with homogeneous link reliability. All edges share fixed entanglement generation and swap success probabilities. This setting captures the key dynamics of contention and probabilistic failure while simplifying evaluation and analysis.

We restrict evaluation to structured topologies (e.g., line and dumbbell graphs) that naturally induce contention and shared-resource competition between goals. Each edge attempts entanglement generation independently each timestep if unoccupied, and successful links increment their age until used or expired.

The environment state space scales exponentially with the number of edges $E$:
\[
|\mathcal{S}| = (\texttt{maxAge} + 1)^E
\]
where each edge can be unoccupied or hold an entangled qubit of age up to \texttt{maxAge}. This formulation remains tractable for small topologies but motivates the use of linear function approximation in larger environments.

Multiplexed entanglement (i.e., multiple qubits per link) is not supported, as it would substantially alter fairness dynamics and introduce different scheduling regimes. Furthermore, multiplexing would reduce contention, diminishing the need for fairness-sensitive scheduling.

Nodes and edges not part of any goal path are omitted to minimize redundant state complexity. Partial rewards for intermediate swap progress are also excluded: rewards are only assigned upon completing end-to-end deliveries. Supporting partial rewards would require tracking and crediting sub-path completions, effectively transforming the task into a hybrid scheduling-routing problem—a direction reserved for future work.

\subsection{Entanglement Scheduling Policies}

Two principal scheduling strategies are evaluated:

\begin{itemize}
    \item \textbf{Wait-swap:} Swaps are deferred until an entire end-to-end path is entangled, minimizing failure risk.
    \item \textbf{Nested-swap (ASAP):} Intermediate swaps are performed opportunistically whenever local conditions allow, incrementally building long-range entanglement.
\end{itemize}

Wait-swap strategies perform well under low-resource or short-memory conditions by conserving fragile entanglement. Nested-swap strategies, by contrast, exploit intermediate opportunities and bypass congestion, often yielding better fairness and throughput under high contention.

Memory constraints—specifically, the single-qubit-per-link assumption—make opportunistic swapping particularly valuable in contention-heavy environments, where early partial progress can prevent starvation.

Figures~\ref{fig:polciyCompare} and~\ref{fig:polciyCompare} illustrate examples of these strategies operating under dynamic conditions.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/policy_compare.png}
  \caption{Pgen vs pSwap for Throuhgput... we achieve high fairness far before we achieve higher throughput (which is more linear)}
  \label{fig:polciyCompare}
\end{figure}

\subsection{Scope of Experimental Evaluation}\label{sec:scope}

Our experimental evaluation focuses on quantifying how fairness-aware scheduling policies perform under varying network conditions. We systematically vary key parameters, including network topology (e.g., sparse, imbalanced), maximum memory lifetime (\texttt{maxAge}), entanglement generation probability ($p_{\text{gen}}$), and swap success probability ($p_{\text{swap}}$), to assess the robustness and generality of learned policies.

Performance is evaluated using multiple metrics, including delivery throughput and Jain's fairness index, capturing both efficiency and equity of entanglement distribution across competing goals. We also compare different scheduling strategies, such as wait-swap and nested-swap policies, to understand trade-offs between opportunism and risk under resource constraints.

By exploring a range of physical settings and policy behaviours, we aim to characterize how environmental factors shape the fairness-efficiency trade-off, providing insights into policy generalizability and informing future design of fairness-aware quantum network protocols.


\subsection{Limitations} \label{sec:limitations}

While our modelling framework captures the core scheduling dynamics and fairness challenges, it intentionally simplifies several physical aspects of real-world quantum networks. We assume full global observability, fixed routing paths, constant swap success probabilities, and terminal swap failures without partial recovery. These idealizations exceed the capabilities of near-term quantum systems but are essential for preserving the Markov property, maintaining environment stationarity, and enabling tractable fairness-aware reinforcement learning.

These choices were made deliberately to abstract the scheduling problem from specific hardware constraints, ensuring broader future relevance as quantum network technologies evolve. By decoupling scheduling behaviour from physical-layer noise, our framework remains adaptable across different architectures, topologies, and protocols, avoiding overfitting to today's rapidly changing hardware landscape. This design philosophy is inspired by classical layered models, such as the OSI framework, where transport-level abstractions enable long-term scalability despite advances at the physical layer; we anticipate that future quantum networks will similarly define modular, application-level scheduling semantics.

Although these assumptions limit immediate deployment realism, they are appropriate for this first-stage investigation of fairness in quantum entanglement distribution. Future extensions can progressively relax these constraints to bridge toward more physically detailed models without undermining the core methodological insights established here.


\subsection{Formal MDP Formulation}
To support reinforcement learning, we model the quantum scheduling environment as a Markov Decision Process (MDP), enabling principled policy optimization under stochastic constraints. The MDP is defined by the tuple \((\mathcal{S}, \mathcal{A}, T, R, \gamma)\), where:

\begin{itemize}
    \item \(\mathcal{S}\) is the state space. Each state is a tuple \((s_{\text{e}}, s_{\text{EDR}})\), where:
    \begin{itemize}
        \item \(s_{\text{e}}\) is a list of \((i, j, a_{ij})\) tuples, where \((i, j)\) denotes an edge and \(a_{ij} \in \{-1, 1, \dots, \texttt{maxAge}\}\) is the current entanglement age (with \(-1\) indicating no entanglement),
        \item \(s_{\text{EDR}}\) is a vector of recent delivery rates (EDRs) for each user goal.
    \end{itemize}

    \item \(\mathcal{A}\) is the action space. Each action \(a\) is a tuple:
    \[
    a = (\texttt{path}, \texttt{goal})
    \]
    where:
    \begin{itemize}
        \item \texttt{path} is an list of edges forming a valid swap path,
        \item \texttt{goal}: the source-destination pair.
    \end{itemize}
    
    The null action \(([], \texttt{None})\) represents a no-op.

    
    \item \(T(s' \mid s, a)\) is the transition probability function, capturing:
    \begin{itemize}
        \item stochastic swap outcomes, governed by the success probability \(p_{\text{swap}}^{n-1}\) for a length-\(n\) path,
        \item aging and expiration of existing entanglements, and
        \item independent Bernoulli trials for new entanglement generation on unoccupied edges with probability \(p_{\text{gen}}\).
    \end{itemize}
    These transitions can be formally decomposed as:
    \[
    P(s' \mid s, a) = P_{\text{swap}}(o \mid s, a) \cdot P_{\text{gen}}(s' \mid s, a, o)
    \]
    Here, \(P_{\text{swap}}\) determines whether the swap succeeds or fails, consuming entanglement regardless of outcome. The result \(o \in \{\text{success}, \text{failure}\}\) influences the post-action state, which is further modified by entanglement aging and regeneration through \(P_{\text{gen}}\). This layered structure highlights the non-triviality of reaching a target state, even when the agent takes the "right" action.
    

    \item \(R(s, a, s')\) is the reward function. We use a log-ratio fairness objective comparing expected utility (based on swap success and path length) with historical delivery performance, computed via rolling EDR windows (see Section~\ref{sec:rewardFunction}).

    \item \(\gamma \in [0, 1)\) is the discount factor, controlling the agent’s preference for long-term versus immediate fairness improvements.
\end{itemize}

Each simulation step proceeds as follows:
\begin{enumerate}
    \item The agent selects an action \(a_t = (\texttt{path}, \texttt{goal})\),
    \item Entanglements are consumed depending on the action and regardless of outcome,
    \item All entanglements age; those exceeding \texttt{maxAge} are discarded,
    \item Entanglement is generated on free edges with probability \(p_{\text{gen}}\),
    \item EDR statistics are updated and included in the new state.
\end{enumerate}

This MDP formulation directly reflects our scheduling environment and aligns with the abstractions and constraints defined in earlier sections. It enables learning agents to reason over fairness, reliability, and memory limitations in a principled and consistent manner.


\subsubsection*{Linear Function Approximation} \label{sec:linearapprox}

To handle the high-dimensional state space of our quantum network MDP, we adopt a linear function approximator for Q-value estimation. This avoids the combinatorial explosion of full Q-tables, which explicitly store a Q-value for every state-action pair. Since the Q-table effectively defines the agent's policy—selecting the highest-value action in each state—this approach becomes impractical as the state space scales with network size and memory dynamics.

Linear approximation generalizes across similar states while maintaining tractability. Though potentially limited in highly non-linear or high-dimensional settings, we reduce this risk by bounding key parameters such as graph size, the number of goals, and entanglement lifespan (\texttt{maxAge}). These constraints help preserve model expressiveness without compromising training stability.



Each state is encoded as a real-valued feature vector \(\phi(s)\), comprising:
\begin{itemize}
    \item Normalized entanglement ages for each edge, with missing links represented as \(-1.0\),
    \item Smoothed entanglement delivery rates (EDRs) for each goal, providing local fairness context.
\end{itemize}

Each action \(a = (\texttt{path}, \texttt{goal})\) is assigned a dedicated weight vector \(\mathbf{w}_a\), and the corresponding Q-value is estimated as:
\[
Q(s, a) = \mathbf{w}_a^\top \phi(s)
\]

This structure enables goal-specific generalization while maintaining per-action distinctions. It supports efficient online updates and avoids overfitting to sparsely visited states.

We selected linear function approximation over discretized binning due to the latter’s poor resolution in capturing fairness dynamics. EDRs often change incrementally—especially in sparse or low-success regimes—making coarse bins insensitive to meaningful variation. While finer discretization could address this, it would drastically inflate the state space and reduce generalization. 

We also considered deep neural networks as an alternative. While they offer greater expressiveness, they were ultimately unnecessary in our setting: linear models often suffice in smaller or structured state spaces, as demonstrated in classic RL benchmarks [CITE]. Additionally, linear methods provide theoretical convergence guarantees under standard assumptions[CITE], are easier to debug, and are less prone to instability or overfitting—issues especially relevant in our stochastic environment with sparse rewards. We prioritized stability, simplicity, and interpretability, making linear approximation a well-aligned and empirically effective choice.





\subsection{Reward Function Design} \label{sec:rewardFunction}

Effective quantum network scheduling must optimize both fairness and throughput. Fairness ensures equitable access across users, while throughput—defined as the sum of entanglement delivery rates (EDRs) across all goals—captures the system’s overall efficiency. Unlike classical settings, fairness in quantum systems must be enforced in real time: entanglement is stochastic and ephemeral, and missed opportunities cannot be retroactively corrected. Our reward function must therefore encode fairness precisely at the point when entanglement is consumed via a swap.

We define a reward based on proportional fairness:
\[
R_t = \sum_{g \in \mathcal{G}} \log\left( 1 + \frac{r^{\text{inst}}_g + \epsilon}{\bar{r}^{\text{global}}_g + \epsilon} \right)
\]
Here, \( r^{\text{inst}}_g \) is the instantaneous number of entangled pairs delivered to goal \( g \), and \( \bar{r}^{\text{global}}_g \) is a rolling average of historical delivery. The constant \(\epsilon > 0\) prevents division by zero, while the \(+1\) ensures non-negative rewards even for low-delivery regimes. We experimented with omitting the offset, but found it discouraged exploration of underserved goals and destabilized early learning.

Notably, \( r^{\text{inst}}_g \) also implicitly acts as a proxy for cost: goals with longer paths or lower swap success probabilities naturally yield fewer successful entanglements, leading to smaller \( r^{\text{inst}}_g \) values and thus lower rewards. This means the reward function inherently down-weights actions that are more resource-intensive, without requiring an explicit penalty term. While we do track such resource inefficiencies—e.g., the number of wasted or failed entanglement attempts—they are not the primary optimization target of this work. Instead, our focus is on fairness and throughput at the point of successful delivery. Introducing additional cost-based shaping could risk double-counting these effects and compromise the interpretability and fairness guarantees of the proportional fairness objective.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/globalInstantRate.png}
  \caption{Impact of instantaneous vs. historical delivery on reward. The function sharply rewards goals with low past delivery when entanglement is successfully routed, but flattens as delivery becomes more frequent.}
  \label{fig:param}
\end{figure}

Rewards are only issued when entanglement is successfully consumed through a swap. In our setting with fixed topology and homogeneous success probabilities \(p_{\text{swap}}\), each goal’s swap success rate is given by:
\[
p^{\text{inst}}_g = p_{\text{swap}}^{L_g - 1}
\]
where \(L_g\) is the number of links on the path to goal \(g\). This value is constant per goal and implicitly reflected in \(r^{\text{inst}}_g\).

To encourage attempts toward underserved goals, we also explored issuing \emph{partial rewards} for failed swaps, scaled by a fixed constant (e.g., 0.5). Although more adaptive schemes could improve performance, we opted for static scaling to retain simplicity and interpretability.

This reward formulation inherits several desirable mathematical properties:
\[
f(x) = \log\left(1 + \frac{r_{\text{inst}}}{x + \epsilon}\right)
\]
\begin{itemize}
    \item \textbf{Monotonicity:} Rewards increase as historical service \(x = \bar{r}^{\text{global}}_g\) decreases.
    \item \textbf{Concavity:} Diminishing returns penalize monopolization of delivery.
    \item \textbf{Bounded and Smooth:} Facilitates stable learning under stochastic dynamics.
    \item \textbf{Relative Scaling:} Robust to delivery scale, focusing on equitable ratios.
\end{itemize}

We compared alternative reward forms:
\[
R = \sum_g \frac{r^{\text{inst}}_g}{\bar{r}^{\text{global}}_g + \epsilon}, \quad R = r^{\text{inst}}_g - \bar{r}^{\text{global}}_g
\]
These proved too volatile, overreacting to rare delivery spikes. The log-ratio design better stabilizes updates and maintains fairness across goals.

Our formulation is grounded in the well-established \(\alpha\)-fairness framework:
\[
U_\alpha(x) = \begin{cases}
\frac{x^{1 - \alpha}}{1 - \alpha}, & \alpha \neq 1 \\
\log(x), & \alpha = 1
\end{cases}
\]
This framework naturally captures the trade-off between fairness and throughput. We adopt \(\alpha = 1\) for proportional fairness in this section, but explore other values empirically (see Section~\ref{sec:results}) to understand the sensitivity and fairness-efficiency trade-offs of our model. Importantly, this removes the need for explicit reward shaping schemes like \(\theta \cdot \text{Fairness} + (1 - \theta) \cdot \text{Throughput}\): varying \(\alpha\) implicitly balances both. This also makes it easier in practice to tailor the reward behaviour to a user's specific policy objectives.

Finally, the reward remains valid in dynamic settings where \(p_{\text{swap}}\) or \(p_{\text{gen}}\) change over time. Because the reward depends only on observed instantaneous delivery, it adapts naturally. However, this assumes that \(r^{\text{inst}}_g\) is observable or can be estimated in real time—introducing a modelling trade-off in systems where such metrics are unavailable.

We intentionally restrict rewards to occur only upon successful goal delivery—i.e., when a complete entangled path is consumed. While partial progress (e.g., intermediate nested swaps) may aid future success, we do not issue sub-goal rewards. This decision ensures reward sparsity aligns with fairness outcomes and avoids introducing credit assignment noise. Extending the reward model to include intermediate nesting steps would require additional mechanisms for tracking edge-level visitation and outcome attribution, complicating both learning dynamics and fairness calibration.

Overall, our reward design is interpretable, well-founded in fairness theory, and highly adaptive—supporting both equity and efficiency in quantum networks.









\subsection{Formulating Fairness} \label{sec:performanceMetrics}

Fairness in quantum networks must be evaluated over short temporal windows due to the fragility of entanglement—links decoheres quickly, and missed opportunities cannot be recovered later. Unlike classical systems, where fairness can be averaged over time, quantum fairness must be enforced in the moment. This motivates our use of a rolling window fairness formulation, inspired by similar constraints in stochastic scheduling and MDP-based systems \cite{zhang2017realtime}.

We evaluate fairness using \textbf{Jain’s Index}, a widely adopted metric for measuring relative equity across multiple goals:
\[
J = \frac{\left(\sum_g r_g\right)^2}{|\mathcal{G}| \cdot \sum_g r_g^2}
\]
Here, \( r_g \) is the entanglement delivery rate (EDR) for goal \( g \), computed as a rolling average over a window of size \( w \):
\[
r_g = \frac{N^{(\text{success})}_g}{w}
\]
where \( N^{(\text{success})}_g \) is the number of successful deliveries to goal \( g \) in the past \( w \) timesteps. This balances responsiveness and stability, smoothing transient noise while tracking recent fairness performance.

Jain’s Index ranges from \(1/|\mathcal{G}|\) (maximal unfairness) to 1 (perfect fairness). It is scale-invariant and well-suited to stochastic quantum networks, where delivery rates fluctuate due to probabilistic entanglement and contention. Most importantly, it captures \emph{relative} fairness—focusing on equitable service distribution under limited, unreliable resources.

To complement fairness, we report \textbf{total throughput}, defined as the sum of all EDRs:
\[
\text{Throughput} = \sum_{g \in \mathcal{G}} r_g
\]
Throughput reflects the system’s efficiency—i.e., the total number of successful entanglement deliveries per timestep.

To visualize the trade-off between fairness and efficiency, we use \textbf{Pareto plots} of Jain’s Index vs. throughput. A desirable Pareto frontier lies closer to the top-right—indicating both high equity and high performance—while dominating lower curves across both metrics.

\textbf{Min-max fairness}, defined as \( \min_g r_g \), performs poorly under stochastic variation. A single unlucky or long-path goal can dominate the signal, leading to misleading evaluations and unstable learning.

\textbf{Expected delivery time}, defined as \( \mathbb{E}[T_g] = 1 / r_g \), is similarly problematic. It becomes unstable as \( r_g \rightarrow 0 \), and overreacts to temporary gaps in service.

In contrast, EDR-based metrics are smoother and more resilient to random fluctuations, making them better suited for fairness-aware scheduling in probabilistic quantum environments.

In summary, we adopt Jain’s Index as our primary fairness metric and total throughput as a complementary efficiency signal—evaluated jointly through Pareto analysis to assess trade-offs between equitable service and overall system performance.





\subsection{Summary of System Design}

To summarise, we model quantum entanglement distribution as a discrete-time reinforcement learning problem, abstracting key physical processes while preserving the core challenges of fairness, reliability, and coordination. Our system simplifies low-level quantum dynamics to focus on scheduling behaviour under uncertainty.

\textbf{Key features of the environment:}

\begin{itemize}
  \item \textbf{Time:} Discrete steps; entanglement ages deterministically and expires after a fixed \texttt{maxAge}.
  \item \textbf{Entanglement Generation:} Stochastic, per-edge with fixed probability \(p_{\text{gen}}\).
  \item \textbf{Swapping:} Instantaneous, atomic, and probabilistic with compound success across hops.
  \item \textbf{Memory:} Single-qubit per edge; no multiplexing.
  \item \textbf{Observability:} Full global state access, reflecting classical-quantum hybrid control.
  \item \textbf{Topology:} Static, known paths; no dynamic routing.
  \item \textbf{State Representation:} Tuple of entanglement ages and smoothed per-goal EDRs.
  \item \textbf{Reward:} Log-ratio proportional fairness function, balancing equity and throughput.
  \item \textbf{Policy Learning:} Linear function approximation for scalable, interpretable decision-making.
\end{itemize}

This setup enables tractable experimentation with fairness - aware policies under stochastic, resource-constrained conditions typical of early-stage quantum networks.










\section{Algorithms}

Fairness-aware entanglement scheduling requires agents to reason about delayed rewards—specifically, deciding when to act and when to wait. Unlike greedy heuristics that optimize immediate outcomes, RL enables policies that link early actions to future fairness rewards under uncertainty.

We evaluate three RL approaches: Value Iteration, Q-learning, and N-step SARSA. Value Iteration, while analytically tractable, proved ineffective because it assumes immediate reward structures and complete transition models—whereas in our environment, fairness objectives depend on successful, stochastic delivery events often occurring many steps after initial actions. Nevertheless, it provided valuable verification of simulator correctness.

Q-learning, a model-free method, improves flexibility by learning from sampled experience without requiring explicit models. However, it estimates value through single-step updates, making it short-sighted when rewards are delayed. This limitation is particularly acute in quantum networks, where fairness outcomes depend on sequences of preparatory actions, not immediate events.

To address this, we adopt N-step SARSA, which propagates rewards across multiple future steps. This enables earlier actions—such as preparing entanglement links or deferring swaps—to be appropriately credited if they eventually enable successful delivery. Such multi-step lookahead is critical for learning patient, fairness-sensitive scheduling strategies in probabilistic environments, when 'waiting' might be the best action to take.

The sections that follow detail the design, implementation, and limitations of each method in our quantum scheduling environment.



\subsection{Greedy Baselines}

We implement two greedy baseline policies to serve as references for evaluating our reinforcement learning agents. Both act purely based on the current observation without planning ahead or reasoning over future outcomes. However, they are not fully memoryless: because the environment encodes EDRs into the state, even greedy agents can react to historical fairness imbalance to some extent.

A fundamental limitation of greedy methods is their inability to defer action: they immediately execute available swaps without considering the future consequences. As a result, they prioritize short-term throughput over long-term fairness, missing opportunities where strategic patience could improve service distribution.

We evaluate two types of greedy strategies:

\begin{enumerate}
    \item \textbf{Greedy with Proportional Fairness Reward:}  
    This agent selects the swap action that maximizes the immediate proportional fairness reward (see Section~\ref{sec:rewardFunction}). At each timestep, it greedily chooses the action yielding the highest fairness gain based on the current state. Although fairness-sensitive through its reward function, it remains entirely myopic, lacking the ability to plan over future opportunities. This baseline isolates the value of multi-step lookahead by comparing purely reactive fairness decisions against learned, temporally-aware policies.

    \item \textbf{Greedy First-Come-First-Served (with EDR Tie-Breaking):}  
    This agent follows a simple first-come-first-served (FCFS) approach, executing the first available swap action it encounters. When multiple actions are simultaneously possible, it breaks ties by prioritizing the goal with the lowest historical entanglement delivery rate (EDR), introducing minimal fairness sensitivity. Unlike the proportional fairness greedy agent, this baseline does not use the reward function directly, allowing us to assess the impact of the fairness reward design itself versus opportunistic scheduling.
\end{enumerate}


Both baselines lack the capacity to build multi-step strategies, defer swaps to avoid congestion, or conserve entanglement for future delivery. Their fairness behaviour arises solely from immediate state features, not from long-term policy optimization.

Despite these limitations, greedy baselines serve three important purposes:
\begin{itemize}
    \item \textbf{Sanity checks:} Ensuring that learning agents meaningfully outperform simple heuristics.
    \item \textbf{Fast benchmarking:} Providing interpretable, training-free reference points.
    \item \textbf{Reward structure isolation:} Testing whether our proportional fairness reward itself, or the ability to reason temporally, is critical for achieving fairness.
\end{itemize}

\subsection{Value Iteration}

We initially explored Value Iteration~\cite{sutton2018reinforcement} as a simple, principled starting point for solving MDPs in our scheduling environment. Value Iteration is analytically tractable, converges reliably in small discrete spaces, and provided a valuable check for verifying our simulator's correctness, including reward computation and entanglement evolution. Its use was further motivated by previous applications in quantum networking, where it has been successfully employed for single-user scheduling tasks~\cite{inesta2023optimal}.

However, applying Value Iteration to our fairness-driven environment revealed a fundamental incompatibility. To preserve the Markov property, we embedded rolling entanglement delivery rates (EDRs) directly into the state representation. While this preserves Markovian properties during interaction-based learning, it creates difficulties for planning methods like Value Iteration, which require exhaustive enumeration over all possible states. Because EDRs evolve based on the agent's actual behaviour over time, they cannot be treated as static during training. This dynamic dependence between actions and fairness statistics breaks the assumption that future states are independent of the policy being optimized, undermining the validity of Bellman updates.

Moreover, fairness rewards are triggered only upon successful, stochastic deliveries after multi-step preparation, and entanglement is consumed immediately upon swaps, introducing further non-stationarity. Traditional Bellman backups, which assume immediate and predictable rewards, failed to capture this delayed, event-driven reward structure.

Thus, although Value Iteration was valuable for simulator verification, MDP Formulation and initial algorithm exploration, it proved structurally unsuitable for fairness-aware quantum scheduling. These limitations motivated a transition toward interaction-driven reinforcement learning methods—specifically, Q-learning and N-step SARSA—which learn directly from sampled trajectories where fairness metrics evolve naturally with experience.

\subsection{Exploration Strategy}

Greedy baseline policies do not require explicit exploration, as they are inherently myopic and deterministic. However, reinforcement learning agents—such as those trained with Q-learning and N-step SARSA—depend on exploration to discover long-term strategies, particularly under sparse and delayed reward conditions.

We adopt \textbf{softmax exploration} to guide exploration more purposefully than uniform random methods like $\epsilon$-greedy. In softmax, the probability of selecting an action is proportional to its estimated Q-value:
\[
P(a \mid s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s,a')/\tau}}
\]
where $\tau$ controls the exploration-exploitation tradeoff. Higher temperatures induce broader exploration; lower temperatures encourage exploitation of high-value actions.

Softmax is particularly well-suited to fairness-sensitive quantum environments, where many actions yield similar immediate rewards and naive random exploration risks wasting scarce entanglement resources. The inherent stochasticity of the environment—arising from probabilistic entanglement generation and swap outcomes—already provides natural variability in observed transitions. This reduces the need for aggressive artificial exploration noise, making guided approaches like softmax especially effective.

In practice, we found that softmax exploration led to greater learning stability and improved fairness performance, particularly under nested swapping conditions where the action space expands significantly. We apply an \textbf{annealed temperature schedule}, starting with high $\tau$ to encourage exploration and gradually reducing it to focus learning as policies stabilize.

Although we considered $\epsilon$-greedy exploration, its uniform randomness introduced unnecessary instability in our already stochastic environment. Adding additional random noise through $\epsilon$-greedy degraded both learning speed and final policy quality.

In summary, softmax enables targeted, resource-efficient exploration that better suits the probabilistic, fairness-constrained demands of quantum entanglement scheduling.


\subsection{Q-Learning}

We selected Q-learning as the first learning-based alternative to Value Iteration. Unlike planning-based methods, which require enumerating all possible transitions, Q-learning learns directly from sampled interaction with the environment. This model-free, trajectory-driven learning approach is well-suited to fairness-aware quantum scheduling, where dynamic entanglement generation and delivery statistics (EDRs) evolve over time and cannot be reliably planned in advance.

Q-learning estimates action values ($Q$-values) by updating them based on experienced transitions. At each step, the agent observes a state $s$, selects an action $a$, receives a reward $R$, and moves to a next state $s'$. The core update rule is:

\[
Q(s, a) \mathrel{+}= \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]

where $\alpha$ is the learning rate and $\gamma$ the discount factor. This one-step temporal difference (TD) learning enables the agent to gradually approximate the optimal action-value function by sampling transitions, rather than requiring full knowledge of the environment.

In our setting, states are augmented with rolling-window EDR statistics, allowing Q-learning to capture fairness-sensitive information directly in the observation. Moreover, because the full network state—including intermediate entanglements, path progress, and fairness statistics—is explicitly stored in the observation, Q-learning can still implicitly learn multi-step strategies. Although it does not perform explicit planning or 'look ahead' at future rewards, the availability of detailed state information allows the agent to associate preparatory actions—such as initiating nested swaps or building partial paths—with higher future rewards during training.

We implement Q-learning with a tabular representation indexed by state-action pairs $(s,a)$, combined with \textbf{softmax exploration} (see Section~\ref{sec:exploration}). Softmax sampling biases exploration toward higher-value actions while maintaining diversity, which is particularly important given the stochastic nature of entanglement generation and swapping. To support continuous operation with no natural episode boundaries, we fix the number of steps per training episode, ensuring the agent can learn over long horizons where reward signals may be sparse and delayed.

\begin{algorithm}[h]
\caption{\textbf{Q-Learning (Fixed Step Horizon)}}
\label{alg:qlearning-fixed}
\textbf{Algorithm parameters:} step size $\alpha \in (0, 1]$, discount factor $\gamma \in [0,1)$, small $\epsilon > 0$\\
Initialize $Q(s, a)$ arbitrarily for all $s \in \mathcal{S}^+, a \in \mathcal{A}(s)$, except that $Q(\text{terminal}, \cdot) = 0$

\ForEach{episode}{
    Initialize $S$\\
    \For{$t = 0$ \KwTo maxStep}{
        Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy or softmax)\\
        Take action $A$, observe reward $R$ and next state $S'$
        \[
        Q(S, A) \mathrel{+}= \alpha \left[ R + \gamma \max_{a} Q(S', a) - Q(S, A) \right]
        \]
        $S \leftarrow S'$
    }
}
\end{algorithm}

%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.8\linewidth]{paper/figures/qLearning.png}
%  \caption{Illustration of Q-Learning updates over a sample entanglement scheduling trajectory.}
%  \label{fig:qLearning}
%\end{figure}

\subsubsection*{Limitations and Challenges}

Despite its adaptability, Q-learning faces key limitations in fairness-aware scheduling environments. Because it updates Q-values based only on immediate rewards and next-state estimates, it struggles to assign credit to early actions whose benefits materialize only after several steps. This limitation is particularly pronounced for strategies like nested swapping or building long entanglement paths, where coordinated multi-step preparation is required before any reward is issued.

In environments with very short memory lifetimes (e.g., \texttt{maxAge} = 1–2), this short-sightedness is less problematic, as outcomes follow quickly from actions. However, with longer entanglement lifetimes and deeper path structures, the inability to propagate delayed rewards often leads to reactive, throughput-focused behaviour that fails to sustain fairness over time.

While it might seem natural to extend Q-learning to $n$-step updates, this introduces bias in off-policy settings unless corrections like importance sampling are applied \cite{sutton2018reinforcement}. Standard Q-learning is not easily adapted to $n$-step temporal difference learning without significant theoretical and practical complications. For these reasons, we instead adopt N-step SARSA, an on-policy multi-step method better aligned with fairness-driven quantum scheduling.








\subsection{N-Step SARSA}

\subsubsection{Justification}

N-step SARSA was adopted as a principled alternative to Q-learning for fairness-aware quantum scheduling. In environments where rewards are sparse and delayed—such as multi-hop entanglement delivery—standard one-step TD updates struggle to assign credit properly to preparatory actions. N-step SARSA addresses this by propagating reward across multiple real actions experienced by the agent, enabling fairness improvements to be attributed back to early decisions like link generation or nested swaps.

Unlike Q-learning, which updates based on the hypothetical best action in the next state, SARSA learns directly from observed behavior. This grounding is critical in quantum networks, where entanglement generation and swapping are probabilistic, and hypothetical trajectories often diverge from actual outcomes. Learning from real transitions better aligns fairness incentives with network stochasticity.

Moreover, N-step SARSA naturally fits our rolling-window fairness objectives, since rewards must be aligned with recent history rather than accumulated over infinite horizons. It implicitly provides a form of temporal abstraction, encouraging the agent to recognize that intermediate progress—such as establishing partial entanglement chains—contributes to future fairness rewards even if immediate success is not achieved. These properties make SARSA well-suited for balancing the need for long-term coordination with the fragile, event-driven nature of quantum environments.

\subsubsection{Model Implementation}

We implement a continuous version of N-step SARSA following Sutton and Barto \cite{sutton2018reinforcement}, adapted for environments with no terminal states. At each timestep, the agent collects a buffer of $(S, A, R)$ tuples and periodically updates past state-action pairs using a bootstrapped $n$-step return.

The update rule is:
\[
Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha \left[ G - Q(S_\tau, A_\tau) \right]
\]
where \( G \) is the $n$-step return:
\[
G = \sum_{i=\tau+1}^{\tau+n} \gamma^{i - \tau - 1} R_i + \gamma^n Q(S_{\tau+n}, A_{\tau+n})
\]

Training episodes are defined by fixed step horizons rather than episode termination, ensuring the agent experiences sufficient temporal spans for fairness credit assignment. Exploration during training uses softmax sampling with an annealed temperature schedule, while evaluation uses deterministic policy selection to isolate learned behavior.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/sarsa.png}
  \caption{Illustration of N-step SARSA update over an actual agent trajectory.}
  \label{fig:sarsa}
\end{figure}

\begin{algorithm}[h]
\caption{\textbf{Continuous N-Step SARSA}}
\label{alg:nstep-sarsa-continuing}
Initialize $Q(s, a)$ arbitrarily for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$\;
Initialize $S_0$\;
Choose $A_0$ using softmax or $\epsilon$-greedy exploration derived from $Q$\;
$t \gets 0$\;
\While{True}{
    Take action $A_t$, observe $R_{t+1}$ and $S_{t+1}$\;
    Choose $A_{t+1}$ using the same exploration policy\;
    Store $(S_t, A_t, R_{t+1})$\;
    $\tau \gets t - n + 1$\;
    \If{$\tau \geq 0$}{
        $G \gets \sum\limits_{i=\tau+1}^{\tau+n} \gamma^{i - \tau - 1} R_i$\;
        $G \gets G + \gamma^n Q(S_{\tau+n}, A_{\tau+n})$\;
        $Q(S_\tau, A_\tau) \gets Q(S_\tau, A_\tau) + \alpha \left(G - Q(S_\tau, A_\tau)\right)$\;
    }
    $t \gets t + 1$\;
}
\end{algorithm}

\subsubsection{Challenges}

While N-step SARSA effectively supports delayed fairness credit assignment, it introduces trade-offs between bias, variance, and exploration sensitivity. Small $n$ values risk short-sightedness, while large $n$ approximates Monte Carlo returns, increasing variance and destabilizing updates—particularly problematic when successful delivery events are rare. 

The effective planning horizon of the agent—how far future fairness rewards influence current decisions—is determined jointly by the $n$-step lookahead, the memory lifetime (\texttt{maxAge}), and the discount factor $\gamma$. Selecting $n$ proportional to \texttt{maxAge} ensures that rewards are propagated within coherence windows before entanglement decays. Larger $\gamma$ values amplify the weight of delayed rewards, making longer $n$ more impactful but also more volatile. Typically, $n$ values between $2$ and $5$ were used depending on system parameters, consistent with standard reinforcement learning practice \cite{sutton2018reinforcement}.

Strict on-policy learning further limits SARSA’s ability to recover from early suboptimal exploration: agents that initially favour short-term gains may become trapped in locally unfair strategies without off-policy correction. Moreover, in sparse-reward regimes, N-step SARSA can accumulate low-value transitions before reaching meaningful delivery events, delaying policy improvement. Although eligibility traces could offer finer-grained propagation, they were omitted due to potential instability in continuous, rolling environments.

Despite these challenges, N-step SARSA naturally grounds fairness rewards in observed network dynamics, provides resilience to stochastic failures, and enables fairness-aware scheduling without relying on optimistic hypothetical futures. These properties make it particularly well-suited for fairness-driven quantum entanglement distribution.




\subsection{Verification, Validation, and Testing}

Verification, validation, and testing are performed to ensure both the correctness of our simulation environment and the reliability of the learning outcomes. Verification checks whether the system was implemented correctly; validation checks whether the learned behaviors align with fairness objectives; testing evaluates policy robustness under stochastic conditions.

\subsubsection{Verification}

Verification focuses on confirming correct implementation of environment mechanics and learning algorithms. In the absence of pre-existing codebases, we performed modular unit testing and manual trace inspection of key components such as entanglement aging, swap execution, and path resolution. Deterministic behavior was confirmed under controlled settings (e.g., $p_{\text{swap}} \in \{0,1\}$) to verify transition logic and reward triggering.

All agents shared a unified Q-table structure mapping $(\text{state}, \text{action}) \rightarrow$ Q-values, simplifying debugging and ensuring consistency. Early-stage episodes were manually traced to verify that reward computations, entanglement consumptions, and EDR updates behaved as expected. During development, detection of pathological behaviors—such as persistent goal starvation—was used as a practical indicator of potential model or reward logic errors.

\subsubsection{Validation}

Validation ensures that learned policies satisfy fairness-driven scheduling objectives. Convergence is assessed through stabilization of Q-values and EDRs over time, rather than strict numerical thresholds, acknowledging that absolute convergence is unattainable under stochastic dynamics.

Each model is trained across 10 random seeds and evaluated on independent random seeds, isolating learning stability from evaluation noise. Metrics are reported as mean values with confidence intervals, and individual trajectories are inspected for anomalous cases. Greedy baselines serve as anchors, ensuring learned agents consistently outperform reactive heuristics under identical conditions. Analytical throughput bounds are computed from $p_{\text{gen}}$, $p_{\text{swap}}$, and network topology; any observed throughput exceeding these limits flags potential simulation or reward calculation errors.

Exploration coverage is evaluated by tracking action selection frequencies across goals and nodes, ensuring all relevant states are visited. In nested-swap environments, we additionally monitor the creation counts of partial entanglement paths, verifying that intermediate swap states required for full deliveries are meaningfully explored. In simpler non-nested settings, EDR tracking alone suffices to assess exploration adequacy. Under softmax exploration, probabilistic coverage of reachable states is expected over sufficient training time, reinforcing exploration completeness.

\subsubsection{Testing}

Testing evaluates policy robustness after training. Average absolute Q-value changes ($\Delta Q$) are monitored throughout, and EDR plateaus are used as complementary indicators of fairness stabilization.

Exploration sufficiency is assessed directly from EDR histories. In wait-swap strategies, persistently low EDR for specific goals signals under-exploration; in nested-swap cases, frequent realization of intermediate entangled paths confirms meaningful multi-hop exploration rather than greedy short-term behavior.

Training is conducted with softmax exploration and annealed temperature schedules; evaluation uses deterministic policy selection without online exploration noise. Each trained model is re-evaluated under fresh random seeds to confirm generalization beyond training randomness.

These layered verification, validation, and testing procedures ensure that observed results reflect genuine fairness-aware learning, rather than artifacts of stochastic fluctuations or implementation bias.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results and Evaluation}

To isolate core behaviours and enable meaningful comparisons, we begin with a fixed environment configuration. Rather than exhaustively varying all system parameters, we select a representative setup that emphasizes fairness under contention. Table~\ref{tab:fixedParams} summarizes the parameters used across baseline experiments.

We adopt a 6-node topology with two user pairs, (0,5) and (2,4), whose paths intersect at a single bottleneck edge (3,4)—a scenario designed to induce competition and expose fairness dynamics. We fix \( p_{\text{gen}} = p_{\text{swap}} = 0.6 \), based on ablation results showing that higher entanglement rates make fairness trivially achievable. In contrast, the 0.6 setting maintains sufficient entanglement for learning while still presenting meaningful stochastic challenges.

\begin{table}[H]
  \centering
  \label{tab:fixedParams}
  \begin{tabular}{ll}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Network topology   & 6 nodes (see Figure~\ref{fig:fixedResults}) \\
    User pairs         & (0,5) and (2,4) \\
    \(p_{\text{gen}}\) & 0.6 \\
    \(p_{\text{swap}}\)& 0.6 \\
    \texttt{maxAge}    & 2 \\
    \bottomrule
  \end{tabular}
\end{table}

We choose this topology, as it make it clear to highlight the contention edge, and allows us to also easily explore the benefits of nested swapping -  as edge 1 and 5 can form a bypass for applciation (3,4) to run through.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/fixedResults.jpg}
  \caption{Topology used for fixed-parameter experiments. Two user goals (A–B and C–B) contend over edge (3,4), making it a fairness-critical bottleneck.}
  \label{fig:fixedResults}
\end{figure}

We begin with two user goals to isolate the core fairness-throughput trade-off and avoid interaction effects that could obscure policy evaluation. This also mirrors scenarios like shared bottleneck links between two clients. Our code generalizes to multiple goals, and Section~\ref{sec:tripleGoal} presents experiments with three competing goals to validate scalability.





\subsection{Convergence and Stability}

In reinforcement learning, convergence refers to the point where value estimates or policy behaviour cease to change meaningfully. In our work, we define \textbf{value convergence} as the point at which the average Q-value update $\Delta Q$ across training episodes falls below a predefined threshold, while \textbf{policy stability} refers to consistent action selection and performance metrics (e.g., EDR) across repeated simulations.

Due to the stochastic nature of entanglement generation ($p_{\text{gen}}$) and swap success ($p_{\text{swap}}$), \textbf{true convergence is unattainable in the strictest sense}. In such systems, convergence must be understood \emph{in expectation}, not as an absolute guarantee. Even when Q-values appear to stabilize, they may continue to fluctuate due to environmental randomness. Crucially, Q-value stability does not always indicate policy stability—especially when multiple actions yield near-identical expected returns.


To ensure robustness, we run extended post-training simulations using fixed Q-values and fresh random seeds. A policy is considered \textbf{stable in expectation} if its per-goal EDR and fairness index vary by no more than 5\% across simulations.

\subsubsection*{Q-value Convergence by Goal Length}

We monitor convergence using the average absolute Q-value change per timestep.\footnote{
Greedy agents, which do not update Q-values, do not exhibit value convergence. However, their policy behavior remains stable, as discussed in Section~\ref{sec:policyEvaluation}.
}

Figure~\ref{fig:perGoalConvergence} shows convergence for two goals—(2,4) and (0,5)—under identical training conditions. We observe:

\begin{itemize}
    \item Shorter goals converge quickly with low-variance Q-value updates.
    \item Longer goals show more persistent fluctuations, as transitions involve more stochastic steps.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/perGoalConvergence.png}
  \caption{Per-goal Q-value convergence for N-step SARSA with $p_{\text{gen}} = p_{\text{swap}} = 0.7$. Goal (0,5) shows more persistent fluctuations due to compounded stochasticity.}
  \label{fig:perGoalConvergence}
\end{figure}

\subsubsection*{Global Convergence Sensitivity to Environment}

We examine convergence behavior under varying $p_{\text{swap}}$ values. Figure~\ref{fig:compareConvergence} shows that:

\begin{itemize}
    \item High $p_{\text{swap}} = 0.9$ leads to rapid convergence, driven by consistent rewards.
    \item Low $p_{\text{swap}} = 0.1$ yields flat Q-values—not due to successful learning, but due to sparse reward signals.
    \item Intermediate $p_{\text{swap}} = 0.5$ results in slower, noisier convergence due to inconsistent feedback.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/compareConvergence.png}
  \caption{Global Q-value magnitude over time for varying swap probabilities. High $p_{\text{swap}}$ accelerates convergence; low $p_{\text{swap}}$ causes reward starvation and premature flattening.}
  \label{fig:compareConvergence}
\end{figure}

\subsubsection*{Effect of Entanglement Memory Lifetime (\texttt{maxAge})}

Figure~\ref{fig:maxageConvergence} examines how convergence is affected by entanglement memory duration. Higher \texttt{maxAge} values initially lead to slightly larger Q-value fluctuations due to an expanded state space and increased temporal complexity. However, all models eventually stabilize to comparable magnitudes.

In particular, agents trained with \texttt{maxAge} values of 1 and 20 reach similar stability levels within a comparable number of training steps. This suggests that while long memory lifetimes increase early training variance, they do not significantly impact convergence time under our reward structure.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/maxageConvergence.png}
  \caption{Q-value convergence for different \texttt{maxAge} settings. Longer-lived entanglements increase early variance but all converge similarly.}
  \label{fig:maxageConvergence}
\end{figure}

\subsubsection*{Stability of System Metrics}

To assess behavioral stability, we track EDR and Jain’s fairness index across long post-training simulations:

\begin{itemize}
    \item With a rolling window applied, both metrics appear smooth and consistent under fixed policies.
    \item Larger windows improve signal stability but dampen responsiveness to short-term imbalance.
    \item Smaller windows reveal higher volatility, reflecting sensitivity to short-term noise.
\end{itemize}

This highlights a trade-off: longer horizons promote fairness stability, but short-term guarantees may degrade under noise.

\subsubsection*{Effect of Fairness Sensitivity Parameter $\alpha$}

Varying the fairness sensitivity parameter $\alpha$ significantly affects convergence behavior. Very low values dilute the reward signal, while excessively high values make the agent overly cautious and reactive. We found that $\alpha = 1$ provides a balanced trade-off between throughput and equity. Further analysis is presented in Section~\ref{sec:alphaFairness}.

\subsubsection*{[TODO] Convergence in Nested Swap Policies}

[TODO] This subsection will explore how convergence patterns differ in nested swap scenarios—particularly where intermediate entanglements must be sequentially generated before final delivery is possible. We will analyze whether these policies converge more slowly due to deeper temporal dependencies, and whether specific entanglement sets or action chains are underexplored.


\subsection{Exploration Behaviour}

We also investigated whether agents were sufficiently exploring the state space during training. For the wait-only policy (i.e., without intermediate swaps), we guaranteed state visitation through the simplicity of our non-cyclic topologies. Here, exploration could be trivially confirmed by observing whether entanglement occurred between all goal nodes over time and have converged \ref{fig:finaledrExmaple}. 

For policies employing nested swapping, we observe [TODO].








\subsection{Reward Function}

Our reward function plays a central role in guiding learning behavior. In this section, we compare and evaluate several reward formulations for fairness-aware entanglement scheduling: logarithmic utility, linear utility, shifted-log variants (to address undefined log at zero), and partial reward schemes.

\subsubsection*{Reward Variants}

We evaluated the following reward functions:

\begin{itemize}
    \item \textbf{Logarithmic utility (default)}: $\log(1 + \text{EDR}_i)$ — promotes fairness with stable gradients even at low EDR.
    \item \textbf{Linear utility}: $\text{EDR}_i$ — prioritizes throughput but weakens fairness incentives.
    \item \textbf{Partial reward variant}: Modified log reward granting partial credit for failed swaps (e.g., $0.5\times$ reward on unsuccessful attempts).
    \item \textbf{Unshifted log}: $\log(\text{EDR}_i)$ — sensitive to small EDR values; included for completeness but prone to instability.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/sparseRewards.png}
  \caption{Pareto comparison of fairness and throughput across reward schemes. Shifted-log performs best. Partial rewards show high throughput but collapse in fairness as $p_{\text{swap}}$ increases.}
  \label{fig:sparseRewards}
\end{figure}

Figure~\ref{fig:sparseRewards} shows that our main reward function—based on $\log(1 + \text{EDR}_i)$—offers the best trade-off between fairness and throughput. It combines the curvature of logarithmic utility with stability near zero, supporting equitable policies even in sparse delivery regimes.

\textbf{Logarithmic reward ($\log(1 + \text{EDR}_i)$):} This variant consistently dominates the Pareto frontier. It prioritizes underserved goals without destabilizing gradients, making it well-aligned with proportional fairness.

\textbf{Linear utility ($\text{EDR}_i$):} Though it achieves slightly higher throughput, it weakens fairness pressure. The lack of curvature leads agents to over-serve high-frequency paths, causing service imbalance.

\textbf{Partial reward variant:} Though helpful early on, rewarding failed attempts distorts incentives. Agents favor short, high-probability swaps that earn credit without completing deliveries. This marginally boosts throughput but significantly degrades fairness—especially as $p_{\text{swap}}$ increases.

\textbf{Outlook for partial rewards:} These results do not imply partial rewards are universally flawed. In highly sparse environments or curriculum learning setups, partial credit could aid convergence. They may also prove useful in systems with retry penalties or delayed planning feedback. However, in our fairness-focused setting, they misalign agent behavior.

\textbf{Summary:} In fairness-aware quantum scheduling, \textbf{reward density must be handled with care}. Denser signals—like partial rewards—can accelerate learning but often incentivize superficial progress over meaningful delivery. Their impact is also topology-dependent: in some environments, these rewards perform comparably to full-reward schemes, but in sparse or high-contention settings, they degrade fairness by over-rewarding easy, high-probability actions while ignoring harder, underserved goals.




\subsubsection*{Effects of Fairness Window Size}

In our framework, fairness is computed using a rolling average window $w$ over entanglement delivery rates (EDR). This window defines how recent delivery events influence fairness estimates and can be configured independently for training and evaluation.

We selected window sizes of 100–100000 to mirror realistic temporal behaviour in near-term quantum hardware. For instance, with \texttt{maxAge} values between 1 and 10 timesteps (representing microsecond to millisecond-scale coherence times), a fairness window of \( w = 1000 \) reflects system behaviour over approximately 0.01–1 seconds \ref{sec:qubitLifetimes}. This ensures that the EDR signal reflects meaningful, real-time delivery dynamics while remaining computationally tractable.


To isolate the impact of $w$, we trained and evaluated agents with \textbf{matching window sizes}—avoiding mismatches that could distort interpretation. Table~\ref{tab:fairness-windows} summarizes results across four settings, with identical $w$ for training and simulation:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Window} & \textbf{Jain’s Index} & \textbf{Throughput} \\
\hline
100      & 0.6971 & 0.1770 \\
1000     & 0.5890 & 0.2485 \\
10000    & 0.5803 & 0.2850 \\
100000   & 0.6491 & 0.2604 \\
\hline
\end{tabular}
\end{table}


We observe a trade-off between responsiveness and stability:

\begin{itemize}
    \item \textbf{Short windows} (e.g., $w = 100$) yield reactive agents that respond quickly to imbalances but may overcompensate, leading to fairness volatility and lower throughput.
    \item \textbf{Longer windows} (e.g., $w = 10000$ or $100000$) smooth fluctuations, promoting more stable learning and higher overall throughput, though possibly at the cost of short-term equity.
    \item Notably, $w = 100000$ achieves a strong balance between fairness (Jain’s = 0.6491) and throughput (0.2604), suggesting that large windows can succeed if used consistently during both learning and evaluation.
\end{itemize}

We recommend aligning $w_{\text{train}}$ and $w_{\text{eval}}$ to ensure interpretability. Fairness metrics are not invariant to $w$, and comparing policies trained with different windows can misrepresent their behaviour.



\subsubsection*{Effect of Varying \(\alpha\)}

Our primary reward function corresponds to proportional fairness (\(\alpha = 1\)), which offers a well-known trade-off between efficiency and equity. To understand the sensitivity of our system to this trade-off, we varied \(\alpha\) across a range of values and plotted the resulting outcomes.

\begin{itemize}
    \item \(\alpha < 1\): Prioritizes efficiency (e.g., higher throughput) but at the cost of fairness, often resulting in highly unequal outcomes.
    \item \(\alpha > 1\): Promotes equity but sacrifices throughput and can destabilize learning.
\end{itemize}

By sweeping \(\alpha\), we aimed to visualize the \textit{Pareto frontier} between fairness and efficiency. This analysis helps identify whether improvements in one metric necessarily come at the cost of the other, and if so, how steep that trade-off is. Understanding this trade-off curve offers insight into:

\begin{itemize}
    \item Whether proportional fairness (\(\alpha = 1\)) truly offers the best balance.
    \item Whether small sacrifices in throughput can yield large fairness gains.
    \item Whether the system exhibits diminishing returns beyond a certain fairness level.
\end{itemize}

This kind of insight is valuable both for justifying the choice of fairness objective and for revealing the structural behavior of the learning system under different reward dynamics.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/alphaFair.png}
  \caption{Impact of \(\alpha\)-fairness on equity-efficiency trade-off. \(\alpha=1\) (proportional fairness) offers best balance.}
  \label{fig:alphaFAIR}
\end{figure}

\subsection{[TODO]Evaluation of Learning Algorithms} \label{sec:policyEvaluation}
remember to mention some concerete failure cases... where does greedy win, where does q-learning win.. why. You want $n$ to be long enough to capture the delayed reward effects that arise from preparing and holding entanglement, but short enough to avoid high variance or crediting expired entanglements. as if maxage < n-lookahead... it starts to before to prpbabalsititc, hence high variance. 

\begin{itemize}
    \item N-Step Finding the Optimal Lookahead for n-Step
    \item Q-Learning vs N-Step SARSA vs Greedy... plot the pareto curves for each as we vary pGen and/or pSwap
    \item Where does each struggle / succeed?
    \item Grid of pSwap vs pGen measuring Jain's \& Throughput
    \item Which polices use/waste more entanglements
    \item As nLookahead increase, must vary gamma and exploration parameters
\end{itemize}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{paper/figures/grid.png}
  \caption{Pgen vs pSwap for Throuhgput... we achieve high fairness far before we achieve higher throughput (which is more linear)}
  \label{fig:grid}
\end{figure}






\subsection{Theoretical Bounds and Empirical Evaluation}

To understand how effective our learned policies are, we compare observed throughput with theoretical upper bounds on the entanglement distribution rate (EDR). These bounds reflect idealized scenarios and are useful for assessing policy efficiency.

\subsubsection*{Ideal Throughput in Linear Paths}

Consider a $d$-hop linear path. Let $p_{\text{gen}}$ be the success probability of entanglement generation on each link and $p_{\text{swap}}$ the success rate of Bell-state measurements.

\paragraph*{Strict Synchronization Bound (No Memory)}
When \texttt{maxAge} = 1, entanglement must be freshly generated on every link in a single timestep:
\[
\text{EDR}_{\text{max}}^{\text{strict}} = p_{\text{gen}}^d \cdot p_{\text{swap}}^{d-1}
\]

\paragraph*{Steady-State Bound (Perfect Memory)}
With infinite entanglement lifetime, each round only needs to generate the missing link(s). Assuming most links are pre-entangled:
\[
\text{EDR}_{\text{max}}^{\text{steady}} = p_{\text{gen}} \cdot p_{\text{swap}}^{d-1}
\]

\paragraph*{Intermediate Case: Limited Lifetime}
When \texttt{maxAge} = 2, entanglement can persist across one additional timestep, increasing the likelihood of usable paths. Although closed-form analysis is more complex, the improvement over \texttt{maxAge} = 1 is substantial. For instance, the chance that two adjacent links are both entangled within a two-timestep window becomes:
\[
P(\text{both ready in 2 steps}) = 1 - (1 - p_{\text{gen}}^2)^2
\]

This non-trivially boosts the EDR.

\subsubsection*{Worked Example: Single Goal, 3-Hop Line Network}
Assume a path from node 0 to node 3 via 1 and 2 (\texttt{maxAge} = 1):
\begin{align*}
d &= 3, \quad p_{\text{gen}} = 0.5, \quad p_{\text{swap}} = 0.8 \\
\text{EDR}_{\text{max}} &= 0.5^3 \cdot 0.8^2 = 0.125 \cdot 0.64 = \mathbf{0.08}
\end{align*}

Under simulation with our trained RL policy, we observed an average throughput of \textbf{YYYY}, corresponding to a \textbf{utilization ratio of XXXX\%}. This is to be expected due to [TODO]

\subsubsection*{Effect of \texttt{maxAge} on Policy Efficiency}
To evaluate memory impact, we trained policies under varying \texttt{maxAge} values and compared empirical throughput to theoretical bounds.

\begin{table}[H]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
\texttt{maxAge} & Empirical EDR & Expected Max & Utilization \\
\midrule
1 & XXXX & 0.080 & 81.25\% \\
2 & XXXX & 0.128 (approx) & 71.9\% \\
$\infty$ & XXXX & 0.125 & 88.0\% \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that:
\begin{itemize}
    \item Memory increases success opportunities by allowing partial paths to accumulate over time.
    \item Even modest memory (\texttt{maxAge}=2) significantly reduces synchronization pressure.
    \item However, benefits saturate: longer memories approach diminishing returns unless the network topology or traffic patterns justify it.
\end{itemize}

\subsubsection*{Takeaway}
Entanglement memory (i.e., \texttt{maxAge}) provides a strong lever for increasing throughput, particularly on longer paths. Our empirical results closely track the theoretical upper bounds, demonstrating that RL agents can approach optimal behavior under realistic constraints. However, utilization never reaches 100\%, due to stochastic delays and action exploration. This gap represents potential for optimization or protocol-level enhancements like purification.




\subsection{Topology and Structural Fairness}

Topology is a first-order factor in fairness-aware quantum scheduling. In our experiments, goals $(0,5)$ and $(2,4)$ share a bottleneck edge $(3,4)$—creating asymmetric access by design. Despite equal rewards, shorter or less contended paths dominate. Goal $(0,5)$ consistently underperforms, highlighting how structure alone can embed inequity.

Shared links introduce unavoidable contention. Even with an optimal policy, fairness is structurally constrained when some goals depend on more edges or face higher competition. This makes topology not just a substrate for scheduling—but an active determinant of fairness outcomes.

Centralized (switch-based) topologies avoid these issues. By giving each application a direct link to a hub, contention is controlled and decision-making simplified. While not scalable, such layouts enable fair scheduling with minimal coordination—making them ideal for small or near-term networks.

In contrast, line, star, or dumbbell topologies require policies to reason over path depth, memory reuse, and stochastic delays. Our results show that:

\begin{itemize}
    \item Longer or overlapping paths are disproportionately penalized.
    \item Nested swaps and memory (\texttt{maxAge} > 1) partially compensate, but do not eliminate structural bias.
    \item Fairness-aware policies are topology-sensitive—without structural symmetry, reward signals alone are insufficient.
\end{itemize}

\textbf{Takeaway:} Fairness cannot be meaningfully evaluated without accounting for topology. Structural constraints—such as path length, contention points, and edge overlap—directly shape agent behaviour, often overpowering reward design alone. This represents a core limitation of our work: performance is highly sensitive to environmental parameters like $p_{\text{swap}}$, $p_{\text{gen}}$, and \texttt{maxAge}, all of which interact non-linearly. Moreover, effective learning often requires environment-specific tuning of algorithmic parameters such as $n$-step lookahead, exploration schedules, and learning rates. As a result, generalizing across topologies or deployments remains an open challenge. Future systems must treat fairness as a joint function of topology, protocol design, and adaptive control.




\subsection{Generalization Discussion}
- What happens if we remove a node in our simulation?
- What happens if we change pGen or pSwap values in our simulation?
- What happens if we remove a goal in our simulation?







\subsection{[TODO] Implementation Challenges and Limitations}
Lack of benchmarks:
No prior work or reference baselines exist for fairness-aware quantum entanglement scheduling, making validation and comparison difficult.

Manual debugging required:
Extensive manual inspection of logs and trajectories was needed to verify correct behavior, especially for reward computation and entanglement logic.

High parameter complexity:
Multiple interacting parameters (e.g., pswap, pgen, maxAge,n-step lookahead, reward shape, topology) made it difficult to isolate cause-effect relationships.

Stochastic environment complicates learning:
Randomness in entanglement generation and swap success introduces noise, making convergence less predictable and reproducibility more challenging.

Sparse reward signals:
Entanglement delivery events are rare, especially in low-availability settings, requiring careful reward shaping to support learning.

Credit assignment is hard:
Q-learning struggled to assign credit to early actions like waiting or preparing entanglement. This limited the learning of patient or anticipatory strategies.

State representation is non-trivial:
Fairness history (EDRs) had to be manually encoded to preserve the Markov property, requiring design decisions with limited literature support.

Limited policy generalization:
Policies trained in one topology may not transfer to others, especially under structural changes like node removal or altered paths.

Static topology assumption:
The environment assumes fixed, known paths. No support for dynamic topologies, node failures, or real-time path changes.

No fidelity decay modelling:
Entanglement fidelity is held constant until expiration. While simplifying, this limits realism and omits important physical effects.

No purification or multiplexing support:
These were abstracted away to focus on scheduling, but their omission means findings may not generalize to more advanced networks.

Evaluation is computationally expensive:
Long training and testing cycles are needed to detect fairness outcomes, especially when rewards are delayed or delivery is rare.

Many results are topology-specific:
Structural bottlenecks and path lengths strongly shape agent behavior. Without accounting for topology, fairness analysis may be misleading.


% Did you verify your environment and agents?
% How did you test edge cases?
% Did your logs help confirm correct behavior?
% Polict generelaisaiton... will it work well on another topology.. obv not when adding new ndoes.. but hwat about failure case or deletion? something more dynmaic would have to update as it's run

\subsection{Summary of Evaluation}

\section{Conclusion}
% Overall findings, tradeoffs, strengths/weaknesses
% Key takeaways for future work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Future Extensions.}  
One promising direction involves issuing partial rewards for nested swap progress or edge-level visitation. This would allow agents to be rewarded for preparing segments of a goal path—particularly valuable in high-contention or low-success regimes. However, doing so would require tracking intermediate contributions, potentially inflating the state space and complicating fairness trade-offs. Techniques such as eligibility traces, edge visit counters, or structured credit assignment could help—but would demand significant tuning and possibly a move toward deep function approximators. This could also enable more advanced routing behavior, where agents learn to select not only when but \emph{where} to act under fairness and contention constraints.


Support for dynamic topologies (e.g., node failure, link reconfiguration).

Incorporation of fidelity decay models for more realistic entanglement aging.

Integration of entanglement purification and error correction processes.

Support for multiplexed entanglement (multiple qubits per link).

Generalization to heterogeneous hardware (variable pgen, pswap)
Exploration of hierarchical or modular RL to enable generalization across topologies.

Introduction of adaptive exploration strategies that adjust based on delivery success rates.

Integration with routing algorithms to combine dynamic path selection with fairness-aware scheduling.

Development of multi-agent RL frameworks for decentralized scheduling under partial observability.

Formal analysis of theoretical throughput bounds in more complex or dynamic networks.

\newpage

\bibliographystyle{IEEEtran}
\bibliography{references}
% that's all folks
\end{document}


