{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea260e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c302268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SARSA run...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque, defaultdict\n",
    "from EDR_UTILITY_FUNCTIONS import *\n",
    "\n",
    "def run_n_step_sarsa(initialEdges, goalEdges, totalSteps, nLookahead, epsilon, gamma, alpha, pGen, pSwap, maxAge, edr_window_size=100, convergence_epsilon=0.001, plot=True, bin_size=0.05):\n",
    "    Q = qTable()\n",
    "    q_value_diffs = []\n",
    "\n",
    "    goal_success_queues = {goal: deque(maxlen=edr_window_size) for goal in goalEdges}\n",
    "\n",
    "    raw_state = [(edge, -1) for edge in initialEdges]\n",
    "    rolling_edrs = {goal: 0.0 for goal in goalEdges}\n",
    "    current_state = get_augmented_state(raw_state, rolling_edrs, bin_size, goal_order=goalEdges)\n",
    "\n",
    "    print(\"\\nStarting SARSA run...\")\n",
    "\n",
    "    # Buffers for n-step\n",
    "    state_buffer = deque(maxlen=nLookahead + 1)\n",
    "    action_buffer = deque(maxlen=nLookahead + 1)\n",
    "    reward_buffer = deque(maxlen=nLookahead)\n",
    "\n",
    "    state_buffer.append(current_state)\n",
    "    action_buffer.append(([], None))  # dummy action at t=0\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        prev_action = action_buffer[-1]\n",
    "        current_state = performAction(prev_action, state_buffer[-1])\n",
    "        current_state = ageEntanglements(current_state, maxAge)\n",
    "        current_state = generateEntanglement(current_state, pGen)\n",
    "\n",
    "        # Compute reward BEFORE next action\n",
    "        reward = getReward(prev_action, goal_success_queues, t + 1, pSwap)\n",
    "        reward_buffer.append(reward)\n",
    "        state_buffer.append(current_state)\n",
    "\n",
    "        # Update goal success queues\n",
    "        for g in goal_success_queues:\n",
    "            goal_success_queues[g].append(0)\n",
    "\n",
    "        consumed_edges, goal = prev_action\n",
    "        if goal is not None and consumed_edges:\n",
    "            success = random.random() < (pSwap ** (len(consumed_edges) - 1))\n",
    "            if success:\n",
    "                goal_success_queues[goal][-1] = 1\n",
    "\n",
    "        # Generate next action\n",
    "        raw_ent_state, _ = current_state\n",
    "        edr_snapshot = {\n",
    "            goal: sum(goal_success_queues[goal]) / max(1, len(goal_success_queues[goal]))\n",
    "            for goal in goalEdges\n",
    "        }\n",
    "        aug_state = get_augmented_state(raw_ent_state, edr_snapshot, bin_size, goal_order=goalEdges)\n",
    "        next_action = getEpsilonGreedyAction(aug_state, Q, epsilon, goalEdges)\n",
    "        action_buffer.append(next_action)\n",
    "\n",
    "        # Perform n-step update if enough history\n",
    "        if t >= nLookahead:\n",
    "            G = 0.0\n",
    "            for i in range(nLookahead):\n",
    "                G += (gamma ** i) * reward_buffer[i]\n",
    "\n",
    "            s_tau = state_buffer[0]\n",
    "            a_tau = action_buffer[0]\n",
    "            s_next = state_buffer[-1]\n",
    "            a_next = action_buffer[-1]\n",
    "            G += (gamma ** nLookahead) * Q.get_q_value(s_next, a_next)\n",
    "\n",
    "            current_q = Q.get_q_value(s_tau, a_tau)\n",
    "            new_q = current_q + alpha * (G - current_q)\n",
    "            Q.set_q_value(s_tau, a_tau, new_q)\n",
    "            q_value_diffs.append(abs(new_q - current_q))\n",
    "\n",
    "        # Slide buffers\n",
    "        # deque will automatically pop from the left once full, no manual trimming needed\n",
    "\n",
    "    # Final updates for remaining state-action pairs in buffer\n",
    "    T = totalSteps\n",
    "    for t_rem in range(1, len(reward_buffer)):\n",
    "        n = len(reward_buffer) - t_rem\n",
    "        G = 0.0\n",
    "        for i in range(n):\n",
    "            G += (gamma ** i) * reward_buffer[t_rem + i]\n",
    "\n",
    "        s_tau = state_buffer[t_rem]\n",
    "        a_tau = action_buffer[t_rem]\n",
    "        s_end = state_buffer[-1]\n",
    "        a_end = action_buffer[-1]\n",
    "        G += (gamma ** n) * Q.get_q_value(s_end, a_end)\n",
    "\n",
    "        current_q = Q.get_q_value(s_tau, a_tau)\n",
    "        new_q = current_q + alpha * (G - current_q)\n",
    "        Q.set_q_value(s_tau, a_tau, new_q)\n",
    "        q_value_diffs.append(abs(new_q - current_q))\n",
    "\n",
    "    return current_state, Q, edr_snapshot\n",
    "\n",
    "\n",
    "def train_sarsa_policy(\n",
    "    edges, goal_edges, p_swap, p_gen, max_age,\n",
    "    totalSteps, nLookahead, epsilon, gamma, alpha,\n",
    "    edr_window_size, bin_size=0.05, seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    _, Q, _ = run_n_step_sarsa(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        nLookahead=nLookahead,\n",
    "        epsilon=epsilon,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=p_gen,\n",
    "        pSwap=p_swap,\n",
    "        maxAge=max_age,\n",
    "        bin_size=0.05,\n",
    "        edr_window_size=edr_window_size,\n",
    "        plot=False\n",
    "    )\n",
    "    return Q\n",
    "\n",
    "\n",
    "# === Training Configuration ===\n",
    "edges = [(0, 1), (1, 2), (3, 2), (2, 4)]\n",
    "goalEdges = [(3, 4), (0, 4)]\n",
    "pSwap = 0.5\n",
    "pGen = 0.5\n",
    "maxAge = 2\n",
    "totalSteps = 30000000\n",
    "nLookahead = 1\n",
    "epsilon = 0.2\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "bin_size = 0.01\n",
    "\n",
    "_, Q, _ = run_n_step_sarsa(\n",
    "    initialEdges=edges,\n",
    "    goalEdges=goalEdges,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    epsilon=epsilon,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha,\n",
    "    pGen=pGen,\n",
    "    pSwap=pSwap,\n",
    "    maxAge=maxAge,\n",
    "    edr_window_size=100,\n",
    "    plot=False,\n",
    "    bin_size =bin_size\n",
    ")\n",
    "\n",
    "\n",
    "#simulate_policy(Q_table =Q, edges=edges, goal_edges=goalEdges, p_swap=pSwap, p_gen=pGen, max_age=maxAge, num_steps=100000, edr_window_size=100, bin_size=bin_size, plot=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431ea9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique states visited: 50262\n",
      "Average visits per state: 601.24\n",
      "States visited only once: 5267\n",
      "States visited less than 5 : 13808\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique states visited: {len(Q.state_visits)}\")\n",
    "visits = list(Q.state_visits.values())\n",
    "print(f\"Average visits per state: {np.mean(visits):.2f}\")\n",
    "print(f\"States visited only once: {sum(1 for v in visits if v == 1)}\")\n",
    "print(f\"States visited less than 5 : {sum(1 for v in visits if v <= 5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0662706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique states visited: 57759\n",
      "Average visits per state: 1510.56\n",
      "States visited only once: 5295\n",
      "States visited less than 5 : 14028\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique states visited: {len(Q.state_visits)}\")\n",
    "visits = list(Q.state_visits.values())\n",
    "print(f\"Average visits per state: {np.mean(visits):.2f}\")\n",
    "print(f\"States visited only once: {sum(1 for v in visits if v == 1)}\")\n",
    "print(f\"States visited less than 5 : {sum(1 for v in visits if v <= 5)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
