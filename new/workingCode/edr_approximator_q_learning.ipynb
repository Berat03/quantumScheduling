{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ecdddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearApprox import *\n",
    "\n",
    "class QuantumNetworkQLearning:\n",
    "    def __init__(self, edges, goalEdges, pSwap, pGen, maxAge, alpha, gamma, epsilon, softmax, temperature, temperature_decay):\n",
    "        self.edges = edges\n",
    "        self.goalEdges = goalEdges\n",
    "        self.pSwap = pSwap\n",
    "        self.pGen = pGen\n",
    "        self.maxAge = maxAge\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.softmax = softmax\n",
    "        self.temperature = temperature\n",
    "        self.temperature_decay = temperature_decay\n",
    "\n",
    "        feature_size = len(edges) + len(goalEdges)\n",
    "        self.Q = LinearQApproximator(feature_size=feature_size)\n",
    "\n",
    "        self.goal_success_queues = {goal: [] for goal in self.goalEdges}\n",
    "\n",
    "\n",
    "    def choose_action(self, state, training=True): # GET RID OF THIS TRAINING PARAMETER, WHATST HE POINT\n",
    "        ent_state, _ = state\n",
    "        possible_actions = getPossibleActions(ent_state, self.goalEdges)\n",
    "\n",
    "        features = featurize_state(state, self.goalEdges)\n",
    "        q_scores = [(action, self.Q.get_q_value(features, action)) for action in possible_actions]\n",
    "\n",
    "\n",
    "        if self.softmax and training:\n",
    "            q_vals = [q for (_, q) in q_scores]\n",
    "            probs = softmax_probs(q_vals, self.temperature)\n",
    "            chosen_action = random.choices([a for a, _ in q_scores], weights=probs)[0]\n",
    "        else:\n",
    "            if training and random.random() < self.epsilon:\n",
    "                chosen_action = random.choice(possible_actions)\n",
    "            else:\n",
    "                chosen_action = max(q_scores, key=lambda x: x[1])[0]\n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def train(self, num_episodes=10, max_steps=1000, plot=False):\n",
    "        for episode in range(num_episodes):\n",
    "            ent_state = [(edge, -1) for edge in self.edges]\n",
    "            edrs = {goal: 0.0 for goal in self.goalEdges}\n",
    "            state = get_augmented_state(ent_state, edrs, goal_order=self.goalEdges)\n",
    "\n",
    "            self.goal_success_queues = {goal: [] for goal in self.goalEdges}\n",
    "            total_timesteps = 1\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.choose_action(state, training=True)\n",
    "                next_state = performAction(action, state)\n",
    "                next_state = ageEntanglements(next_state, self.maxAge)\n",
    "                next_state = generateEntanglement(next_state, self.pGen)\n",
    "                \n",
    "                # Determine success before reward (but DO NOT manually update the success queues here)\n",
    "                consumed_edges, goal = action\n",
    "                success = False\n",
    "                if goal is not None and consumed_edges:\n",
    "                    success_prob = self.pSwap ** (len(consumed_edges) - 1)\n",
    "                    success = random.random() < success_prob\n",
    "\n",
    "\n",
    "                reward = getReward(\n",
    "                    action=action,\n",
    "                    goal_success_queues=self.goal_success_queues,\n",
    "                    total_timesteps=total_timesteps,\n",
    "                    pSwap=self.pSwap,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "                # Augment new state with updated EDR snapshot\n",
    "                edr_snapshot = {\n",
    "                    g: sum(self.goal_success_queues[g]) / max(1, len(self.goal_success_queues[g]))\n",
    "                    for g in self.goalEdges\n",
    "                }\n",
    "                next_state = get_augmented_state(next_state[0], edr_snapshot, goal_order=self.goalEdges)\n",
    "\n",
    "                # Q-learning update\n",
    "                features = featurize_state(state, self.goalEdges)\n",
    "                next_features = featurize_state(next_state, self.goalEdges)\n",
    "                possible_next_actions = getPossibleActions(next_state[0], self.goalEdges)\n",
    "\n",
    "                max_next_q = max([self.Q.get_q_value(next_features, a) for a in possible_next_actions], default=0.0)\n",
    "                target = reward + self.gamma * max_next_q\n",
    "\n",
    "                self.Q.update(features, action, target, self.alpha)\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                total_timesteps += 1\n",
    "\n",
    "            # Optional: Decay temperature\n",
    "            if self.softmax and self.temperature_decay:\n",
    "                self.temperature = max(0.01, self.temperature * self.temperature_decay)\n",
    "\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode + 1}\")\n",
    "\n",
    "# === Q-Learning Wrapper for Experiment Framework ===\n",
    "def train_q_learning_policy(edges, goal_edges, p_swap, p_gen, max_age, seed=None, **kwargs):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    agent = QuantumNetworkQLearning(\n",
    "        edges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        pSwap=p_swap,\n",
    "        pGen=p_gen,\n",
    "        maxAge=max_age,\n",
    "        alpha=kwargs.get(\"alpha\", 0.1),\n",
    "        gamma=kwargs.get(\"gamma\", 0.99),\n",
    "        epsilon=kwargs.get(\"epsilon\", 0.001),\n",
    "        softmax=kwargs.get(\"softmax\", False),\n",
    "        temperature=kwargs.get(\"temperature\", 1.0),\n",
    "        temperature_decay=kwargs.get(\"temperature_decay\", 0.9),\n",
    "    )\n",
    "    agent.train(\n",
    "        num_episodes=kwargs.get(\"num_episodes\", 5),\n",
    "        max_steps=kwargs.get(\"max_steps\", 1000),\n",
    "        plot=False\n",
    "    )\n",
    "    return agent.Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\n"
     ]
    }
   ],
   "source": [
    "# === Setup parameters ===\n",
    "edges = [(0, 1), (1,2), (2,3), (3,4)]\n",
    "goal_edges = [(0, 2), (1, 4)]\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 2\n",
    "temperature = 3\n",
    "temperature_decay = 0.99\n",
    "num_episodes = 30\n",
    "max_steps = 100000\n",
    "epsilon = 0.05\n",
    "# === Train Q-Learning agent ===\n",
    "Q = train_q_learning_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=42,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    max_steps=max_steps,\n",
    "    softmax=False,\n",
    "    temperature=temperature,\n",
    "    temperature_decay= temperature_decay\n",
    ")\n",
    "\n",
    "# === Simulate policy (with EDR tracking) ===\n",
    "simulate_policy(\n",
    "    Q_table=Q,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=100000,\n",
    "    edr_window_size=1000,\n",
    "    plot=True\n",
    ")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Q-Learning for pGen = 0.1 ===\n",
      "\n",
      "=== Q-Learning (pGen=0.1) Policy Training Run 1 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.1) Policy Training Run 2 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.1) Policy Training Run 3 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.1) Policy Training Run 4 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.1) Policy Training Run 5 ===\n",
      "Episode 10\n",
      "\n",
      "=== Evaluating Q-Learning for pGen = 0.2 ===\n",
      "\n",
      "=== Q-Learning (pGen=0.2) Policy Training Run 1 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.2) Policy Training Run 2 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.2) Policy Training Run 3 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.2) Policy Training Run 4 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.2) Policy Training Run 5 ===\n",
      "Episode 10\n",
      "\n",
      "=== Evaluating Q-Learning for pGen = 0.30000000000000004 ===\n",
      "\n",
      "=== Q-Learning (pGen=0.30000000000000004) Policy Training Run 1 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.30000000000000004) Policy Training Run 2 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.30000000000000004) Policy Training Run 3 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.30000000000000004) Policy Training Run 4 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.30000000000000004) Policy Training Run 5 ===\n",
      "Episode 10\n",
      "\n",
      "=== Evaluating Q-Learning for pGen = 0.4 ===\n",
      "\n",
      "=== Q-Learning (pGen=0.4) Policy Training Run 1 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.4) Policy Training Run 2 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.4) Policy Training Run 3 ===\n",
      "Episode 10\n",
      "\n",
      "=== Q-Learning (pGen=0.4) Policy Training Run 4 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m pGen = \u001b[32m0.6\u001b[39m\n\u001b[32m      6\u001b[39m maxAge = \u001b[32m2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mcompare_policies_across_param\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQ-Learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_train_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_q_learning_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpGen\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 0.1 to 1.0\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# gets overridden\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp_swap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_age\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxAge\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43malpha\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepsilon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_episodes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msoftmax\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# optional\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mDone\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/quantumScheduling/new/workingCode/linearApprox.py:636\u001b[39m, in \u001b[36mcompare_policies_across_param\u001b[39m\u001b[34m(policy_name, policy_train_fn, param_name, param_values, edges, goal_edges, p_gen, p_swap, max_age, train_kwargs, validate_kwargs, plot, num_runs, num_steps, num_simulations)\u001b[39m\n\u001b[32m    620\u001b[39m         plt.show()\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    623\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33medrs\u001b[39m\u001b[33m\"\u001b[39m: final_edrs_by_goal,\n\u001b[32m    624\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33medts\u001b[39m\u001b[33m\"\u001b[39m: final_edts_by_goal,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maction_ratio_histories\u001b[39m\u001b[33m\"\u001b[39m: all_action_ratio_histories\n\u001b[32m    630\u001b[39m     }\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompare_policies_across_param\u001b[39m(\n\u001b[32m    634\u001b[39m     policy_name,\n\u001b[32m    635\u001b[39m     policy_train_fn,\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     param_name,\n\u001b[32m    637\u001b[39m     param_values,\n\u001b[32m    638\u001b[39m     edges,\n\u001b[32m    639\u001b[39m     goal_edges,\n\u001b[32m    640\u001b[39m     p_gen,\n\u001b[32m    641\u001b[39m     p_swap,\n\u001b[32m    642\u001b[39m     max_age,\n\u001b[32m    643\u001b[39m     train_kwargs={},\n\u001b[32m    644\u001b[39m     validate_kwargs={},\n\u001b[32m    645\u001b[39m     plot=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    646\u001b[39m     num_runs=\u001b[32m5\u001b[39m,\n\u001b[32m    647\u001b[39m     num_steps=\u001b[32m10000\u001b[39m,\n\u001b[32m    648\u001b[39m     num_simulations=\u001b[32m10\u001b[39m\n\u001b[32m    649\u001b[39m ):\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m param_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mpGen\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpSwap\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mparam_name must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpGen\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpSwap\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     all_results = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/quantumScheduling/new/workingCode/linearApprox.py:481\u001b[39m, in \u001b[36mrun_policy_experiments\u001b[39m\u001b[34m(train_policy_fn, policy_name, edges, goal_edges, p_swap, p_gen, max_age, num_runs, num_steps, num_simulations, train_kwargs, validate_kwargs, plot)\u001b[39m\n\u001b[32m    463\u001b[39m         plt.show()\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    466\u001b[39m         mean_final_edrs_by_goal,\n\u001b[32m    467\u001b[39m         mean_final_edts_by_goal,\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m         all_action_ratio_histories\n\u001b[32m    473\u001b[39m     )\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_policy_experiments\u001b[39m(\n\u001b[32m    476\u001b[39m     train_policy_fn,\n\u001b[32m    477\u001b[39m     policy_name,\n\u001b[32m    478\u001b[39m     edges,\n\u001b[32m    479\u001b[39m     goal_edges,\n\u001b[32m    480\u001b[39m     p_swap,\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     p_gen,\n\u001b[32m    482\u001b[39m     max_age,\n\u001b[32m    483\u001b[39m     num_runs=\u001b[32m10\u001b[39m,\n\u001b[32m    484\u001b[39m     num_steps=\u001b[32m30000\u001b[39m,\n\u001b[32m    485\u001b[39m     num_simulations=\u001b[32m20\u001b[39m,\n\u001b[32m    486\u001b[39m     train_kwargs={},\n\u001b[32m    487\u001b[39m     validate_kwargs={},\n\u001b[32m    488\u001b[39m     plot=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    489\u001b[39m ):\n\u001b[32m    490\u001b[39m     final_edrs_by_goal = {goal: [] \u001b[38;5;28;01mfor\u001b[39;00m goal \u001b[38;5;129;01min\u001b[39;00m goal_edges}\n\u001b[32m    491\u001b[39m     final_edts_by_goal = {goal: [] \u001b[38;5;28;01mfor\u001b[39;00m goal \u001b[38;5;129;01min\u001b[39;00m goal_edges}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mtrain_q_learning_policy\u001b[39m\u001b[34m(edges, goal_edges, p_swap, p_gen, max_age, seed, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m np.random.seed(seed)\n\u001b[32m    107\u001b[39m agent = QuantumNetworkQLearning(\n\u001b[32m    108\u001b[39m     edges=edges,\n\u001b[32m    109\u001b[39m     goalEdges=goal_edges,\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m     temperature_decay=kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mtemperature_decay\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.9\u001b[39m),\n\u001b[32m    119\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_episodes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    124\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent.Q\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mQuantumNetworkQLearning.train\u001b[39m\u001b[34m(self, num_episodes, max_steps, plot)\u001b[39m\n\u001b[32m     86\u001b[39m max_next_q = \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mself\u001b[39m.Q.get_q_value(next_features, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m possible_next_actions], default=\u001b[32m0.0\u001b[39m)\n\u001b[32m     87\u001b[39m target = reward + \u001b[38;5;28mself\u001b[39m.gamma * max_next_q\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m state = next_state\n\u001b[32m     93\u001b[39m total_timesteps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/quantumScheduling/new/workingCode/linearApprox.py:150\u001b[39m, in \u001b[36mupdate\u001b[39m\u001b[34m(self, features, action, target, alpha)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeaturize_state\u001b[39m(state, goal_order):\n\u001b[32m    149\u001b[39m     ent_state, edrs = state  \u001b[38;5;66;03m# edrs is a tuple now\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     edge_features = [age / \u001b[32m10.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m age >= \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, age \u001b[38;5;129;01min\u001b[39;00m ent_state]\n\u001b[32m    151\u001b[39m     edr_features = \u001b[38;5;28mlist\u001b[39m(edrs)  \u001b[38;5;66;03m# just unpack the tuple\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(edge_features + edr_features, dtype=np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/quantumScheduling/new/workingCode/linearApprox.py:138\u001b[39m, in \u001b[36m_action_key\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Setup parameters ===\n",
    "edges = [(0, 1), (1,2), (2,3), (3,4)]\n",
    "goal_edges = [(0, 2), (4, 1)]\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 2\n",
    "\n",
    "compare_policies_across_param(\n",
    "    policy_name=\"Q-Learning\",\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    param_name=\"pGen\",\n",
    "    param_values=[0.1 * i for i in range(1, 11)],  # 0.1 to 1.0\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_gen=pGen,  # gets overridden\n",
    "    p_swap=pSwap,\n",
    "    max_age=maxAge,\n",
    "    train_kwargs={\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"num_episodes\": 10,\n",
    "        \"max_steps\": 10000,\n",
    "        \"temperature\": 1.0,\n",
    "        \"temperature_decay\": 0.98,\n",
    "        \"softmax\": False\n",
    "    },\n",
    "    validate_kwargs={},  # optional\n",
    "    plot=True,\n",
    "    num_runs=5,\n",
    "    num_steps=7000,\n",
    "    num_simulations=5\n",
    ")\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
