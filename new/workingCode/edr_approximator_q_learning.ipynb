{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linearApprox import *\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class QuantumNetworkQLearning:\n",
    "    def __init__(\n",
    "        self,\n",
    "        edges,\n",
    "        goalEdges,\n",
    "        pSwap,\n",
    "        pGen,\n",
    "        maxAge,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        epsilon,\n",
    "        softmax,\n",
    "        temperature,\n",
    "        temperature_decay,\n",
    "        reward_mode=\"basic\",\n",
    "        reward_alpha=0.5,\n",
    "        reward_epsilon=0.001\n",
    "    ):\n",
    "        self.edges = edges\n",
    "        self.goalEdges = goalEdges\n",
    "        self.pSwap = pSwap\n",
    "        self.pGen = pGen\n",
    "        self.maxAge = maxAge\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.softmax = softmax\n",
    "        self.temperature = temperature\n",
    "        self.temperature_decay = temperature_decay\n",
    "\n",
    "        self.reward_mode = reward_mode\n",
    "        self.reward_alpha = reward_alpha\n",
    "        self.reward_epsilon = reward_epsilon\n",
    "\n",
    "        feature_size = len(edges) + len(goalEdges)\n",
    "        self.Q = LinearQApproximator(feature_size=feature_size)\n",
    "\n",
    "        self.goal_success_queues = {goal: [] for goal in self.goalEdges}\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        ent_state, _ = state\n",
    "        possible_actions = getPossibleActions(ent_state, self.goalEdges)\n",
    "        features = featurize_state(state, self.goalEdges)\n",
    "        q_scores = [(action, self.Q.get_q_value(features, action)) for action in possible_actions]\n",
    "\n",
    "        if self.softmax:\n",
    "            q_vals = [q for (_, q) in q_scores]\n",
    "            probs = softmax_probs(q_vals, self.temperature)\n",
    "            return random.choices([a for a, _ in q_scores], weights=probs)[0]\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        return max(q_scores, key=lambda x: x[1])[0]\n",
    "\n",
    "    def train(self, num_episodes=10, max_steps=1000, plot=False):\n",
    "        q_value_diffs = []\n",
    "        q_value_diffs_per_goal = {goal: [] for goal in self.goalEdges}\n",
    "\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            ent_state = [(edge, -1) for edge in self.edges]\n",
    "            edrs = {goal: 0.0 for goal in self.goalEdges}\n",
    "            state = get_augmented_state(ent_state, edrs, goal_order=self.goalEdges)\n",
    "\n",
    "            self.goal_success_queues = {goal: [] for goal in self.goalEdges}\n",
    "            total_timesteps = 1\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.choose_action(state)\n",
    "                next_state = performAction(action, state)\n",
    "                next_state = ageEntanglements(next_state, self.maxAge)\n",
    "                next_state = generateEntanglement(next_state, self.pGen)\n",
    "\n",
    "                consumed_edges, goal = action\n",
    "                success = False\n",
    "                if goal is not None and consumed_edges:\n",
    "                    success_prob = self.pSwap ** (len(consumed_edges) - 1)\n",
    "                    success = random.random() < success_prob\n",
    "\n",
    "                reward = compute_reward(\n",
    "                    action=action,\n",
    "                    goal_success_queues=self.goal_success_queues,\n",
    "                    total_timesteps=total_timesteps,\n",
    "                    pSwap=self.pSwap,\n",
    "                    mode=self.reward_mode,\n",
    "                    alpha=self.reward_alpha,\n",
    "                    epsilon=self.reward_epsilon,\n",
    "                    success=success\n",
    "                )\n",
    "\n",
    "                edr_snapshot = {\n",
    "                    g: sum(self.goal_success_queues[g]) / max(1, len(self.goal_success_queues[g]))\n",
    "                    for g in self.goalEdges\n",
    "                }\n",
    "                next_state = get_augmented_state(next_state[0], edr_snapshot, goal_order=self.goalEdges)\n",
    "\n",
    "                features = featurize_state(state, self.goalEdges)\n",
    "                next_features = featurize_state(next_state, self.goalEdges)\n",
    "                possible_next_actions = getPossibleActions(next_state[0], self.goalEdges)\n",
    "\n",
    "                max_next_q = max([self.Q.get_q_value(next_features, a) for a in possible_next_actions], default=0.0)\n",
    "                target = reward + self.gamma * max_next_q\n",
    "                current_q = self.Q.get_q_value(features, action)\n",
    "                q_diff = abs(target - current_q)\n",
    "                q_value_diffs.append(q_diff)\n",
    "                # For each goal, log q_diff if it was relevant; else 0 or np.nan\n",
    "                for g in self.goalEdges:\n",
    "                    if g == goal and consumed_edges:\n",
    "                        q_value_diffs_per_goal[g].append(q_diff)\n",
    "                    else:\n",
    "                        q_value_diffs_per_goal[g].append(np.nan)  # or np.nan if you want to ignore it in avg\n",
    "\n",
    "\n",
    "\n",
    "                self.Q.update(features, action, target, self.alpha)\n",
    "\n",
    "                state = next_state\n",
    "                total_timesteps += 1\n",
    "\n",
    "            if self.softmax and self.temperature_decay:\n",
    "                self.temperature = max(0.01, self.temperature * self.temperature_decay)\n",
    "\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode + 1}\")\n",
    "\n",
    "        return q_value_diffs, q_value_diffs_per_goal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Q-Learning Wrapper for Experiment Framework ===\n",
    "def train_q_learning_policy(edges, goal_edges, p_swap, p_gen, max_age, seed=None, plot_q_convergence=False, **kwargs):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    agent = QuantumNetworkQLearning(\n",
    "        edges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        pSwap=p_swap,\n",
    "        pGen=p_gen,\n",
    "        maxAge=max_age,\n",
    "        alpha=kwargs.get(\"alpha\", 0.1),\n",
    "        gamma=kwargs.get(\"gamma\", 0.99),\n",
    "        epsilon=kwargs.get(\"epsilon\", 0.001),\n",
    "        softmax=kwargs.get(\"softmax\", False),\n",
    "        temperature=kwargs.get(\"temperature\", 1.0),\n",
    "        temperature_decay=kwargs.get(\"temperature_decay\", 0.9),\n",
    "        reward_mode=kwargs.get(\"reward_mode\", \"basic\"),\n",
    "        reward_alpha=kwargs.get(\"reward_alpha\", 0.5),\n",
    "        reward_epsilon=kwargs.get(\"reward_epsilon\", 0.001)\n",
    "    )\n",
    "\n",
    "    q_diffs, q_diffs_per_goal = agent.train( # dont really need q_diffs\n",
    "        num_episodes=kwargs.get(\"num_episodes\", 5),\n",
    "        max_steps=kwargs.get(\"max_steps\", 1000),\n",
    "        plot=True  \n",
    "    )\n",
    "\n",
    "    if plot_q_convergence:\n",
    "        plot_q_value_convergence(q_diffs, q_diffs_per_goal)\n",
    "\n",
    "    return agent.Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2742c",
   "metadata": {},
   "source": [
    "# **Get Plotting**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e36a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming bootstrap_policy_runs() has been defined above,\n",
    "# and you have train_q_learning_policy available\n",
    "\n",
    "bootstrap_policy_runs(\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    policy_name=\"Q-Learning\",\n",
    "    edges=[(0, 1), (1, 3), (2, 3), (3, 4), (4, 5)],\n",
    "    goal_edges=[(0, 5), (2, 4)],\n",
    "    p_swap=0.7,\n",
    "    p_gen=0.7,\n",
    "    max_age=3,\n",
    "    num_runs=10,         # How many seeds to test across\n",
    "    num_steps=200000,    # Length of each simulation (policy validation)\n",
    "    train_kwargs={\n",
    "        \"nLookahead\": 3,       # If your Q-learning doesn't use this, remove it\n",
    "        \"epsilon\": 0.05,\n",
    "        \"gamma\": 0.995,\n",
    "        \"alpha\": 0.1,\n",
    "        \"softmax\": True,\n",
    "        \"temperature\": 3,\n",
    "        \"temperature_decay\": 0.995,\n",
    "        \"reward_mode\": \"basic\",\n",
    "        \"reward_alpha\": 1,\n",
    "        \"reward_epsilon\": 0.001\n",
    "    },\n",
    "    window=1000,          # Averaging window for final Jain/throughput stats\n",
    "    plot=True             # Set to False if you just want the data back\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766701ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_reward_modes_across_param_full_qlearning(\n",
    "    policy_train_fn,\n",
    "    reward_modes,\n",
    "    mode_labels,\n",
    "    param_name,\n",
    "    param_values,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    p_gen,\n",
    "    p_swap,\n",
    "    max_age,\n",
    "    num_runs,\n",
    "    num_steps,\n",
    "    num_simulations,\n",
    "    base_train_kwargs,\n",
    "    validate_kwargs={}\n",
    "):\n",
    "    assert param_name in ['pGen', 'pSwap'], \"param_name must be either 'pGen' or 'pSwap'\"\n",
    "    \n",
    "    color_map = plt.get_cmap(\"tab10\")\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(22, 6))\n",
    "    ax_jain, ax_tp, ax_pareto = axs\n",
    "\n",
    "    for i, mode in enumerate(reward_modes):\n",
    "        avg_jains = []\n",
    "        avg_throughputs = []\n",
    "\n",
    "        for param_val in param_values:\n",
    "            curr_p_gen = param_val if param_name == 'pGen' else p_gen\n",
    "            curr_p_swap = param_val if param_name == 'pSwap' else p_swap\n",
    "\n",
    "            train_kwargs = base_train_kwargs.copy()\n",
    "            train_kwargs['reward_mode'] = mode\n",
    "\n",
    "            print(f\"\\n→ {mode_labels[i]} | {param_name} = {param_val}\")\n",
    "            results = run_policy_experiments(\n",
    "                train_policy_fn=policy_train_fn,\n",
    "                policy_name=f\"{mode_labels[i]} | {param_name}={param_val}\",\n",
    "                edges=edges,\n",
    "                goal_edges=goal_edges,\n",
    "                p_gen=curr_p_gen,\n",
    "                p_swap=curr_p_swap,\n",
    "                max_age=max_age,\n",
    "                num_runs=num_runs,\n",
    "                num_steps=num_steps,\n",
    "                num_simulations=num_simulations,\n",
    "                train_kwargs=train_kwargs,\n",
    "                validate_kwargs=validate_kwargs,\n",
    "                plot=False\n",
    "            )\n",
    "\n",
    "            jains = results['jains']\n",
    "            throughputs = [\n",
    "                sum(results['edrs'][goal][run_i] for goal in goal_edges)\n",
    "                for run_i in range(num_runs)\n",
    "            ]\n",
    "\n",
    "            avg_jains.append(np.mean(jains))\n",
    "            avg_throughputs.append(np.mean(throughputs))\n",
    "\n",
    "        ax_jain.plot(param_values, avg_jains, label=mode_labels[i], marker='o', linestyle='-', color=color_map(i))\n",
    "        ax_tp.plot(param_values, avg_throughputs, label=mode_labels[i], marker='o', linestyle='-', color=color_map(i))\n",
    "        ax_pareto.plot(avg_throughputs, avg_jains, label=mode_labels[i], marker='o', linestyle='-', color=color_map(i))\n",
    "\n",
    "    ax_jain.set_title(f\"Fairness vs {param_name}\")\n",
    "    ax_jain.set_xlabel(param_name)\n",
    "    ax_jain.set_ylabel(\"Jain's Index\")\n",
    "    ax_jain.set_ylim(0.45, 1.05)\n",
    "    ax_jain.grid(True)\n",
    "    ax_jain.legend()\n",
    "\n",
    "    ax_tp.set_title(f\"Throughput vs {param_name}\")\n",
    "    ax_tp.set_xlabel(param_name)\n",
    "    ax_tp.set_ylabel(\"Total Throughput\")\n",
    "    ax_tp.grid(True)\n",
    "    ax_tp.legend()\n",
    "\n",
    "    ax_pareto.set_title(\"Pareto Curve (Throughput vs Fairness)\")\n",
    "    ax_pareto.set_xlabel(\"Total Throughput\")\n",
    "    ax_pareto.set_ylabel(\"Jain's Index\")\n",
    "    ax_pareto.set_xlim(0, 1.05)\n",
    "    ax_pareto.set_ylim(0.45, 1.05)\n",
    "    ax_pareto.grid(True)\n",
    "    ax_pareto.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#####################################################################\n",
    "# Now call the Q-learning version\n",
    "#####################################################################\n",
    "\n",
    "from linearApprox import *\n",
    "mainEdge = [(0, 1), (1, 3), (2, 3), (3, 4), (4, 5)]\n",
    "mainGoals = [(0, 5), (2, 4)]\n",
    "\n",
    "sparseEdge = [(0,1), (1, 2), (2,3), (2,4), (3, 5), (5, 6)]\n",
    "sparseGoals = [(0, 6), (0, 4)]\n",
    "\n",
    "param_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "base_kwargs_q = {\n",
    "    \"alpha\": 0.1,\n",
    "    \"gamma\": 0.995,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"num_episodes\": 20,\n",
    "    \"max_steps\": 5000,\n",
    "    \"temperature\": 5,\n",
    "    \"temperature_decay\": 0.98,\n",
    "    \"softmax\": False,\n",
    "    \"reward_alpha\": 1,\n",
    "    \"reward_epsilon\": 0.001\n",
    "}\n",
    "\n",
    "print(\"SPARSE — Q-LEARNING\")\n",
    "compare_reward_modes_across_param_full_qlearning(\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    reward_modes=[\"basic\", \"partial\", \"logless\", \"without+1\"],\n",
    "    mode_labels=[\"Basic (Log, Full)\", \"Partial Reward\", \"Linear (Logless)\", \"Log w/o + 1\"],\n",
    "    param_name=\"pSwap\",\n",
    "    param_values=param_values,\n",
    "    edges=sparseEdge,\n",
    "    goal_edges=sparseGoals,\n",
    "    p_gen=0.6,\n",
    "    p_swap=0.6,\n",
    "    max_age=3,\n",
    "    num_runs=3,\n",
    "    num_steps=10000,\n",
    "    num_simulations=3,\n",
    "    base_train_kwargs=base_kwargs_q\n",
    ")\n",
    "\n",
    "compare_reward_modes_across_param_full_qlearning(\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    reward_modes=[\"basic\", \"partial\", \"logless\", \"without+1\"],\n",
    "    mode_labels=[\"Basic (Log, Full)\", \"Partial Reward\", \"Linear (Logless)\", \"Log w/o + 1\"],\n",
    "    param_name=\"pGen\",\n",
    "    param_values=param_values,\n",
    "    edges=sparseEdge,\n",
    "    goal_edges=sparseGoals,\n",
    "    p_gen=0.6,\n",
    "    p_swap=0.6,\n",
    "    max_age=3,\n",
    "    num_runs=3,\n",
    "    num_steps=10000,\n",
    "    num_simulations=3,\n",
    "    base_train_kwargs=base_kwargs_q\n",
    ")\n",
    "\n",
    "print(\"MAIN — Q-LEARNING\")\n",
    "compare_reward_modes_across_param_full_qlearning(\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    reward_modes=[\"basic\", \"partial\", \"logless\", \"without+1\"],\n",
    "    mode_labels=[\"Basic (Log, Full)\", \"Partial Reward\", \"Linear (Logless)\", \"Log w/o + 1\"],\n",
    "    param_name=\"pSwap\",\n",
    "    param_values=param_values,\n",
    "    edges=mainEdge,\n",
    "    goal_edges=mainGoals,\n",
    "    p_gen=0.6,\n",
    "    p_swap=0.6,\n",
    "    max_age=3,\n",
    "    num_runs=3,\n",
    "    num_steps=10000,\n",
    "    num_simulations=3,\n",
    "    base_train_kwargs=base_kwargs_q\n",
    ")\n",
    "\n",
    "compare_reward_modes_across_param_full_qlearning(\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    reward_modes=[\"basic\", \"partial\", \"logless\", \"without+1\"],\n",
    "    mode_labels=[\"Basic (Log, Full)\", \"Partial Reward\", \"Linear (Logless)\", \"Log w/o + 1\"],\n",
    "    param_name=\"pGen\",\n",
    "    param_values=param_values,\n",
    "    edges=mainEdge,\n",
    "    goal_edges=mainGoals,\n",
    "    p_gen=0.6,\n",
    "    p_swap=0.6,\n",
    "    max_age=2,\n",
    "    num_runs=4,\n",
    "    num_steps=10000,\n",
    "    num_simulations=3,\n",
    "    base_train_kwargs=base_kwargs_q\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00691d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [(0,1), (1,3),(2,3), (3, 4), (4, 5), (4, 6)]\n",
    "goal_edges = [(0, 5), (2, 4)]\n",
    "pSwap = 0.5\n",
    "pGen = 0.5\n",
    "maxAge = 2\n",
    "max_steps = 10000\n",
    "num_episodes = 100\n",
    "nLookahead = 3\n",
    "epsilon = 0.05\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "edr_window_size=100\n",
    "temperature = 5\n",
    "temperature_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define network and training parameters\n",
    "edges = [(0, 1), (1, 3), (2, 3), (3, 4), (4, 5)]\n",
    "goal_edges = [(0, 5), (2, 4)]\n",
    "max_age_params = [1, 5, 10, 15, 20]\n",
    "\n",
    "p = 0.7\n",
    "num_episodes = 1\n",
    "max_steps = 40000\n",
    "epsilon = 0.05\n",
    "gamma = 0.995\n",
    "alpha = 0.1\n",
    "softmax = False\n",
    "reward_mode = 'basic'\n",
    "reward_alpha = 1\n",
    "reward_epsilon = 0.001\n",
    "temperature = 5\n",
    "temperature_decay = 0.995\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for max_age in max_age_params:\n",
    "    print(max_age)\n",
    "    agent = QuantumNetworkQLearning(\n",
    "        edges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        pSwap=p,\n",
    "        pGen=p,\n",
    "        maxAge=max_age,\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        epsilon=epsilon,\n",
    "        softmax=softmax,\n",
    "        temperature=temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        reward_mode=reward_mode,\n",
    "        reward_alpha=reward_alpha,\n",
    "        reward_epsilon=reward_epsilon\n",
    "    )\n",
    "\n",
    "    q_value_diffs, q_diffs_per_goal = agent.train(\n",
    "        num_episodes=num_episodes,\n",
    "        max_steps=max_steps,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    results[max_age] = (q_value_diffs, q_diffs_per_goal)\n",
    "\n",
    "# --- Helpers ---\n",
    "def interpolate_nans(data):\n",
    "    data = np.array(data, dtype=np.float64)\n",
    "    nans = np.isnan(data)\n",
    "    if np.all(nans):\n",
    "        return data\n",
    "    indices = np.arange(len(data))\n",
    "    data[nans] = np.interp(indices[nans], indices[~nans], data[~nans])\n",
    "    return data\n",
    "\n",
    "def smooth(data, window=1000):\n",
    "    kernel = np.ones(window) / window\n",
    "    return np.convolve(data, kernel, mode='same')\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "window = 2000\n",
    "colors = ['tab:blue', 'tab:purple', 'tab:green', 'tab:red', 'tab:orange']\n",
    "\n",
    "# --- Per-goal Q-value diffs (top plot) ---\n",
    "print('ahhh')\n",
    "for idx, max_age in enumerate(max_age_params):\n",
    "    print(idx)\n",
    "    color = colors[idx]\n",
    "    q_diffs_per_goal = results[max_age][1]\n",
    "\n",
    "    # Short Goal\n",
    "    short_goal_smoothed = smooth(interpolate_nans(q_diffs_per_goal[(2, 4)]), window)\n",
    "    axs[0].plot(short_goal_smoothed, label=f'Short Goal (maxAge={max_age})', linestyle='--', color=color)\n",
    "\n",
    "    # Long Goal\n",
    "    long_goal_smoothed = smooth(interpolate_nans(q_diffs_per_goal[(0, 5)]), window)\n",
    "    axs[0].plot(long_goal_smoothed, label=f'Long Goal (maxAge={max_age})', linestyle='-', color=color)\n",
    "\n",
    "axs[0].set_ylabel('Per-Goal Q-value Update')\n",
    "axs[0].set_title('Smoothed Q-value Updates (Per Goal)')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# --- Global Q-value diffs (bottom plot) ---\n",
    "for idx, max_age in enumerate(max_age_params):\n",
    "    color = colors[idx]\n",
    "    q_value_diffs = results[max_age][0]\n",
    "    global_smoothed = smooth(q_value_diffs, window)\n",
    "    axs[1].plot(global_smoothed, label=f'{max_age}', color=color)\n",
    "\n",
    "axs[1].set_xlabel('Timestep')\n",
    "axs[1].set_ylabel('Global Q-value Update')\n",
    "axs[1].set_title('Smoothed Global Q-value Convergence')\n",
    "axs[1].legend(title='maxAge')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a695e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Run\n",
    "Q = train_q_learning_policy(\n",
    "    seed = 10,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    max_steps=max_steps,\n",
    "    softmax=False,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    reward_mode=\"basic\",     \n",
    "    reward_alpha=1,\n",
    "    plot_q_convergence= True\n",
    ")\n",
    "\n",
    "simulate_policy(\n",
    "    Q_table=Q,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=100000,\n",
    "    edr_window_size=10000,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_values = [0.1 * i for i in range(1, 11)]\n",
    "compare_policies_across_param(\n",
    "    policy_name=\"Q-Learning\",\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    param_name=\"pGen\",\n",
    "    param_values=param_values,  # 0.1 to 1.0\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_gen=pGen,  # gets overridden\n",
    "    p_swap=pSwap,\n",
    "    max_age=maxAge,\n",
    "    train_kwargs={\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"num_episodes\": 10,\n",
    "        \"max_steps\": 10000,\n",
    "        \"temperature\": 1.0,\n",
    "        \"temperature_decay\": 0.98,\n",
    "        \"softmax\": False\n",
    "    },\n",
    "    validate_kwargs={},  # optional\n",
    "    plot=True,\n",
    "    num_runs=10,\n",
    "    num_steps=20000,\n",
    "    num_simulations=10)\n",
    "\n",
    "compare_policies_across_param(\n",
    "    policy_name=\"Q-Learning\",\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    param_name=\"pSwap\",\n",
    "    param_values=param_values,  # 0.1 to 1.0\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_gen=pGen,  # gets overridden\n",
    "    p_swap=pSwap,\n",
    "    max_age=maxAge,\n",
    "    train_kwargs={\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"num_episodes\": 10,\n",
    "        \"max_steps\": 10000,\n",
    "        \"temperature\": 1.0,\n",
    "        \"temperature_decay\": 0.98,\n",
    "        \"softmax\": False\n",
    "    },\n",
    "    validate_kwargs={},  # optional\n",
    "    plot=True,\n",
    "    num_runs=10,\n",
    "    num_steps=20000,\n",
    "    num_simulations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85513bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "\n",
    "pswap_vals = [0.4, 0.6, 0.8]\n",
    "\n",
    "compare_alpha_vs_env_param(\n",
    "    policy_name=\"Q-Learning\",\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    param_name=\"pSwap\",\n",
    "    param_values=pswap_vals,\n",
    "    alpha_values=alpha_vals,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_gen=pGen,\n",
    "    p_swap=pSwap,\n",
    "    max_age=maxAge,\n",
    "    train_kwargs={\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.05,\n",
    "        \"softmax\": False,\n",
    "        \"temperature\": 3,\n",
    "        \"temperature_decay\": 0.99,\n",
    "        \"num_episodes\": 5,\n",
    "        \"max_steps\": 30000\n",
    "    },\n",
    "    num_runs=3,  # Lower for fast debugging\n",
    "    num_steps=5000,\n",
    "    num_simulations=5,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = [0.1, 0.3, 0.5, 0.7, 0.9, 1, 1.1, 1.3, 1.5, 1.7, 1.9 ]\n",
    "# Define network and training parameters\n",
    "edges = [(0,1), (1,3),(2,3), (3, 4), (4, 5)]\n",
    "goal_edges = [(0, 5), (2, 4)]\n",
    "pSwap = 0.7\n",
    "pGen = 0.7\n",
    "maxAge = 2\n",
    "epsilon = 0.05\n",
    "gamma = 0.995\n",
    "alpha = 0.1\n",
    "\n",
    "edr_window_size = 1000\n",
    "temperature = 5\n",
    "temperature_decay = 0.995\n",
    "\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "results = compare_policies_across_alpha(\n",
    "    policy_name=\"Q-Learning\",\n",
    "    policy_train_fn=train_q_learning_policy,\n",
    "    alpha_values=alpha_vals,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_gen=pGen,\n",
    "    p_swap=pSwap,\n",
    "    max_age=maxAge,\n",
    "    train_kwargs={\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.05,\n",
    "        \"softmax\": False,\n",
    "        \"temperature\": 3,\n",
    "        \"temperature_decay\": 0.99,\n",
    "        \"num_episodes\": 5,\n",
    "        \"max_steps\": 30000,\n",
    "    },\n",
    "    num_runs=4,\n",
    "    num_steps=20000,\n",
    "    num_simulations=3,\n",
    "    plot=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Setup ===\n",
    "edges = [(0, 1), (1, 2), (2, 3), (3, 4)]\n",
    "goal_edges = [(0, 2), (4, 1)]\n",
    "maxAge = 2\n",
    "\n",
    "pGen_values = np.linspace(0.1, 1.0, 10)\n",
    "pSwap_values = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "jains_matrix = np.zeros((len(pSwap_values), len(pGen_values)))\n",
    "throughput_matrix = np.zeros((len(pSwap_values), len(pGen_values)))\n",
    "\n",
    "# === Run Experiments Over Grid ===\n",
    "for i, p_swap in enumerate(pSwap_values):\n",
    "    for j, p_gen in enumerate(pGen_values):\n",
    "        print(f\"Running: pSwap={p_swap:.2f}, pGen={p_gen:.2f}\")\n",
    "\n",
    "        result = run_policy_experiments(\n",
    "            train_policy_fn=train_q_learning_policy,\n",
    "            policy_name=f\"Q-Learning (pSwap={p_swap:.2f}, pGen={p_gen:.2f})\",\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            p_gen=p_gen,\n",
    "            p_swap=p_swap,\n",
    "            max_age=maxAge,\n",
    "            num_runs=3,\n",
    "            num_steps=5000,\n",
    "            num_simulations=3,\n",
    "            train_kwargs={\n",
    "                \"alpha\": 0.1,\n",
    "                \"gamma\": 0.99,\n",
    "                \"epsilon\": 0.1,\n",
    "                \"num_episodes\": 5,\n",
    "                \"max_steps\": 8000,\n",
    "                \"temperature\": 1.0,\n",
    "                \"temperature_decay\": 0.98,\n",
    "                \"softmax\": False,\n",
    "            },\n",
    "            plot=False\n",
    "        )\n",
    "\n",
    "        jains_matrix[i, j] = np.mean(result[\"jains\"])\n",
    "        throughput_matrix[i, j] = np.mean([\n",
    "            sum(result[\"edrs\"][goal][run_i] for goal in goal_edges)\n",
    "            for run_i in range(len(result[\"jains\"]))\n",
    "        ])\n",
    "\n",
    "# === Plot Heatmaps ===\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Shared color scale\n",
    "vmin = min(jains_matrix.min(), throughput_matrix.min())\n",
    "vmax = max(jains_matrix.max(), throughput_matrix.max())\n",
    "\n",
    "# Jain's Fairness Heatmap\n",
    "im1 = axs[0].imshow(jains_matrix, cmap='viridis', origin='lower',\n",
    "                    extent=[pGen_values[0], pGen_values[-1], pSwap_values[0], pSwap_values[-1]],\n",
    "                    aspect='auto', vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(\"Jain's Fairness Index\")\n",
    "axs[0].set_xlabel(\"pGen\")\n",
    "axs[0].set_ylabel(\"pSwap\")\n",
    "\n",
    "# Throughput Heatmap\n",
    "im2 = axs[1].imshow(throughput_matrix, cmap='viridis', origin='lower',\n",
    "                    extent=[pGen_values[0], pGen_values[-1], pSwap_values[0], pSwap_values[-1]],\n",
    "                    aspect='auto', vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(\"Total Throughput\")\n",
    "axs[1].set_xlabel(\"pGen\")\n",
    "axs[1].set_ylabel(\"pSwap\")\n",
    "\n",
    "# Shared colorbar at the top\n",
    "cbar_ax = fig.add_axes([0.2, 0.92, 0.6, 0.02])\n",
    "fig.colorbar(im2, cax=cbar_ax, orientation='horizontal')\n",
    "cbar_ax.set_title(\"Shared Value Scale\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf47de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
