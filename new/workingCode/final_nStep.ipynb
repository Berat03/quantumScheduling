{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd38cb70",
   "metadata": {},
   "source": [
    "# **N-STEP SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b19c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque, defaultdict\n",
    "#from linearApprox import *\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0c9b5",
   "metadata": {},
   "source": [
    "#  **UTILITY CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_state(state, edrs, goal_order=None):\n",
    "    if goal_order is None:\n",
    "        goal_order = sorted(edrs.keys())\n",
    "    edr_vector = tuple(edrs[goal] for goal in goal_order)\n",
    "    sorted_state = sorted(state, key=lambda x: (x[0][0], x[0][1]))\n",
    "    return (tuple(sorted_state), edr_vector)\n",
    "\n",
    "def ageEntanglements(augmented_state, maxAge):\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = []\n",
    "    for edge, age in ent_state:\n",
    "        if age >= 0:\n",
    "            new_age = age + 1\n",
    "            if new_age > maxAge:\n",
    "                new_state.append((edge, -1))\n",
    "            else:\n",
    "                new_state.append((edge, new_age))\n",
    "        else:\n",
    "            new_state.append((edge, age))\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "def generateEntanglement(augmented_state, pGen):\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = []\n",
    "    for edge, age in ent_state:\n",
    "        if age < 0:\n",
    "            if random.random() < pGen:\n",
    "                new_state.append((edge, 1))\n",
    "            else:\n",
    "                new_state.append((edge, age))\n",
    "        else:\n",
    "            new_state.append((edge, age))\n",
    "    return (tuple(new_state), edr_bins)\n",
    "def jains_index(edrs):\n",
    "    \"\"\"Compute Jain's Fairness Index.\"\"\"\n",
    "    if all(edr == 0 for edr in edrs.values()):\n",
    "        return 0.0\n",
    "    numerator = sum(edrs.values())**2\n",
    "    denominator = len(edrs) * sum(v**2 for v in edrs.values())\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def featurize_state(state, goal_order):\n",
    "    ent_state, edrs = state\n",
    "    edge_features = [age / 10.0 if age >= 0 else -1.0 for _, age in ent_state]\n",
    "    edr_features = list(edrs)\n",
    "    return np.array(edge_features + edr_features, dtype=np.float32)\n",
    "\n",
    "class LinearQApproximator:\n",
    "    def __init__(self, feature_size):\n",
    "        self.weights = {}  # Dict[action_key] = weight_vector\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def _action_key(self, action):\n",
    "        consumed_paths, goal_list = action\n",
    "\n",
    "        # Handle the no-op case: goal_list is None for no-op action\n",
    "        if goal_list is None:\n",
    "            return ((), None)  # Ensure this is hashable for no-op action\n",
    "\n",
    "        # Sort paths and goals for consistency in hashing\n",
    "        sorted_paths = tuple(sorted(tuple(sorted(path)) for path in consumed_paths))\n",
    "        sorted_goals = tuple(sorted(goal_list))\n",
    "        return (sorted_paths, sorted_goals)\n",
    "\n",
    "    def _init_weights(self, action_key):\n",
    "        if action_key not in self.weights:\n",
    "            self.weights[action_key] = np.zeros(self.feature_size)\n",
    "\n",
    "    def get_q_value(self, features, action):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        return float(np.dot(self.weights[key], features))\n",
    "\n",
    "    def update(self, features, action, target, alpha):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        prediction = np.dot(self.weights[key], features)\n",
    "        error = target - prediction\n",
    "        self.weights[key] += alpha * error * features\n",
    "\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "def get_possible_multi_actions(ent_state, goalEdges, nestedSwaps=False, max_path_length=None):\n",
    "    actions = []\n",
    "    existing_edges = {edge for edge, age in ent_state if age >= 0}\n",
    "\n",
    "    def find_paths(start, visited=None, path=None, depth=0):\n",
    "        \"\"\"DFS to find all paths starting from 'start'.\"\"\"\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if path is None:\n",
    "            path = []\n",
    "\n",
    "        paths = []\n",
    "        for edge in existing_edges:\n",
    "            if edge in visited:\n",
    "                continue\n",
    "            u, v = edge\n",
    "            if u == start:\n",
    "                new_path = path + [edge]\n",
    "                paths.append(new_path)\n",
    "                if max_path_length is None or depth < max_path_length:\n",
    "                    paths.extend(find_paths(v, visited | {edge}, new_path, depth + 1))\n",
    "            elif v == start:\n",
    "                new_path = path + [edge]\n",
    "                paths.append(new_path)\n",
    "                if max_path_length is None or depth < max_path_length:\n",
    "                    paths.extend(find_paths(u, visited | {edge}, new_path, depth + 1))\n",
    "        return paths\n",
    "\n",
    "    single_goal_actions = []\n",
    "    for goal in goalEdges:\n",
    "        start, end = goal\n",
    "        paths = find_paths(start)\n",
    "        for path in paths:\n",
    "            if path:\n",
    "                if not nestedSwaps:\n",
    "                    # Only allow complete paths (start to end)\n",
    "                    if path[-1][0] == end or path[-1][1] == end:\n",
    "                        single_goal_actions.append((path, goal))\n",
    "                        actions.append(([path], [goal]))\n",
    "                else:\n",
    "                    # Allow any partial path\n",
    "                    single_goal_actions.append((path, goal))\n",
    "                    actions.append(([path], [goal]))\n",
    "\n",
    "    # Multi-goal disjoint actions\n",
    "    for k in range(2, len(single_goal_actions) + 1):\n",
    "        for combo in itertools.combinations(single_goal_actions, k):\n",
    "            paths, goals = zip(*combo)\n",
    "            flat_edges = [e for path in paths for e in path]\n",
    "            if len(flat_edges) == len(set(flat_edges)):\n",
    "                actions.append((list(paths), list(goals)))\n",
    "\n",
    "    actions.append(([], None))  # no-op\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(action, goal_success_queues, pSwap, mode=\"basic\", alpha=1.0, epsilon=1e-3, noop_penalty=0.0):\n",
    "    consumed_edges, goals = action\n",
    "    if not goals or not consumed_edges:\n",
    "        return -noop_penalty, False\n",
    "\n",
    "    total_reward = 0.0\n",
    "    any_success = False\n",
    "    used_edges = set()\n",
    "\n",
    "    for goal, path in zip(goals, consumed_edges):\n",
    "        path_edges = set(path)\n",
    "        if not path_edges.isdisjoint(used_edges):\n",
    "            continue\n",
    "        used_edges.update(path_edges)\n",
    "\n",
    "        success_prob = pSwap ** (len(path) - 1)\n",
    "        edr = sum(goal_success_queues[goal]) / len(goal_success_queues[goal]) + epsilon\n",
    "        x = success_prob / edr\n",
    "        success = (random.random() < success_prob)\n",
    "        any_success = any_success or success\n",
    "\n",
    "        if mode == \"partial\":\n",
    "            base = math.log(1 + x)\n",
    "            total_reward += base if success else 0.5 * base\n",
    "        else:\n",
    "            total_reward += math.log(1 + x) if success else 0.0\n",
    "\n",
    "    return total_reward, any_success\n",
    "\n",
    "def performAction(action, augmented_state):\n",
    "    consumed_paths, goal_edges = action\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = list(ent_state)\n",
    "    for path in consumed_paths:\n",
    "        for edge_to_consume in path:\n",
    "            for i, (edge, age) in enumerate(new_state):\n",
    "                if edge == edge_to_consume:\n",
    "                    new_state[i] = (edge, -1)\n",
    "                    break\n",
    "    return (tuple(new_state), edr_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6dc1",
   "metadata": {},
   "source": [
    "# **SIMULATION CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9a4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    p_swap,\n",
    "    p_gen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    plot=True,\n",
    "    nestedSwaps=False\n",
    "):    \n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    raw = [(e, -1) for e in edges]\n",
    "    current = get_augmented_state(raw, {g:0.0 for g in goal_edges}, goal_order=goal_edges)\n",
    "\n",
    "    recent = {g: [] for g in goal_edges}\n",
    "    edr_hist, jain_hist, tp_hist = {g:[] for g in goal_edges}, [], []\n",
    "    valids, acts, qvals = [], [], []\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        ent_state, _ = current\n",
    "        acts_all = get_possible_multi_actions(ent_state, goal_edges, nestedSwaps=nestedSwaps)\n",
    "\n",
    "        if ([], None) not in acts_all:\n",
    "            acts_all.append(([], None))\n",
    "        real = [a for a in acts_all if a != ([], None)]\n",
    "        avail = len(real) > 0\n",
    "        valids.append(1 if avail else 0)\n",
    "\n",
    "        feats = featurize_state(current, goal_edges)\n",
    "        best_a, best_q = max(((a, Q_table.get_q_value(feats, a)) for a in acts_all), key=lambda x: x[1])\n",
    "        qvals.append(best_q)\n",
    "        acts.append(1.0 if (avail and best_a in real) else 0.0)\n",
    "\n",
    "        nxt = performAction(best_a, current)\n",
    "        nxt = ageEntanglements(nxt, max_age)\n",
    "        nxt = generateEntanglement(nxt, p_gen)\n",
    "\n",
    "        consumed_paths, goals = best_a\n",
    "        if goals is not None:\n",
    "            for g, path in zip(goals, consumed_paths):\n",
    "                if path:\n",
    "                    succ = random.random() < (p_swap ** (len(path) - 1))\n",
    "                    recent[g].append(1 if succ else 0)\n",
    "                else:\n",
    "                    recent[g].append(0)\n",
    "            for g in goal_edges:\n",
    "                if g not in goals:\n",
    "                    recent[g].append(0)\n",
    "        else:\n",
    "            for g in goal_edges:\n",
    "                recent[g].append(0)\n",
    "\n",
    "        if len(recent[g]) > edr_window_size:\n",
    "            recent[g].pop(0)\n",
    "\n",
    "        edrs = {g: sum(recent[g]) / len(recent[g]) for g in goal_edges}\n",
    "        for g in goal_edges:\n",
    "            edr_hist[g].append(edrs[g])\n",
    "\n",
    "        total = sum(edrs.values())\n",
    "        tp_hist.append(total)\n",
    "        jain_hist.append(jains_index(edrs))\n",
    "\n",
    "        current = get_augmented_state(nxt[0], edrs, goal_order=goal_edges)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        fig.suptitle(\n",
    "            f\"Policy Sim — pSwap={p_swap}, pGen={p_gen}, maxAge={max_age}, steps={num_steps}, window={edr_window_size}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        # (0) EDR + Jain\n",
    "        ax0 = axs[0]\n",
    "        for g in goal_edges:\n",
    "            ax0.plot(edr_hist[g], label=f\"EDR {g}\")\n",
    "        ax0.plot(jain_hist, '--', label=\"Jain's\", linewidth=2)\n",
    "        ax0.set_title(\"EDR (solid) & Jain (dashed)\")\n",
    "        ax0.set_xlabel(\"Timestep\")\n",
    "        ax0.set_ylabel(\"Value\")\n",
    "        ax0.set_ylim(0, 1.05)\n",
    "        ax0.legend()\n",
    "\n",
    "        # (1) single Pareto point after burn-in\n",
    "        ax1 = axs[1]\n",
    "        avg_tp = np.mean(tp_hist[burn_in:])\n",
    "        avg_jain = np.mean(jain_hist[burn_in:])\n",
    "        ax1.scatter([avg_tp], [avg_jain], s=100, c='crimson')\n",
    "        ax1.set_title(\"Final Pareto Point\")\n",
    "        ax1.set_xlabel(\"Avg Throughput\")\n",
    "        ax1.set_ylabel(\"Avg Jain\")\n",
    "        ax1.set_xlim(0, max(tp_hist) * 1.1)\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.text(avg_tp, avg_jain, f\"  ({avg_tp:.3f}, {avg_jain:.3f})\")\n",
    "\n",
    "        # (2) best Q-value\n",
    "        ax2 = axs[2]\n",
    "        ax2.plot(qvals, color='slateblue')\n",
    "        ax2.set_title(\"Best Q-Value Over Time\")\n",
    "        ax2.set_xlabel(\"Timestep\")\n",
    "        ax2.set_ylabel(\"Q-Value\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    burn_in_idx = 5000\n",
    "    final_edrs = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "    final_tp = sum(final_edrs.values())\n",
    "    final_jain = jains_index(final_edrs)\n",
    "\n",
    "    print(\"\\nMetrics After Burn-in (first 5000 steps ignored):\")\n",
    "    print(\"Mean EDRs:\", {g: f\"{v:.4f}\" for g, v in final_edrs.items()})\n",
    "    print(f\"Total Throughput (sum of EDRs): {final_tp:.4f}\")\n",
    "    print(f\"Jain's Fairness Index: {final_jain:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"edr_history\": edr_hist,\n",
    "        \"jain_history\": jain_hist,\n",
    "        \"throughput_history\": tp_hist,\n",
    "        \"q_values\": qvals\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892b1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    p_swap,\n",
    "    p_gen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    seeds=[10, 20, 30, 40, 50],\n",
    "    plot=True,\n",
    "    nestedSwaps=False\n",
    "):\n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    all_edrs = {g: [] for g in goal_edges}\n",
    "    all_jains = []\n",
    "    all_tp = []\n",
    "\n",
    "    edr_time_series = {g: [] for g in goal_edges}\n",
    "    jain_time_series = []\n",
    "\n",
    "    goal_colors = {\n",
    "        goal_edges[0]: \"tab:blue\",\n",
    "        goal_edges[1]: \"tab:green\"\n",
    "    }\n",
    "\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        result = simulate_policy(\n",
    "            Q_table=Q_table,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            p_swap=p_swap,\n",
    "            p_gen=p_gen,\n",
    "            max_age=max_age,\n",
    "            num_steps=num_steps,\n",
    "            edr_window_size=edr_window_size,\n",
    "            burn_in=burn_in,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        # Final metrics\n",
    "        edr_hist = result[\"edr_history\"]\n",
    "        jain_hist = result[\"jain_history\"]\n",
    "        tp_hist = result[\"throughput_history\"]\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_time_series[g].append(edr_hist[g])\n",
    "        jain_time_series.append(jain_hist)\n",
    "\n",
    "        burn_in_idx = 5000\n",
    "        edrs_final = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "        tp_final = sum(edrs_final.values())\n",
    "        jain_final = jains_index(edrs_final)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            all_edrs[g].append(edrs_final[g])\n",
    "        all_tp.append(tp_final)\n",
    "        all_jains.append(jain_final)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        fig.suptitle(\n",
    "            f\"Multi-Sim — {len(seeds)} Seeds | pSwap={p_swap}, pGen={p_gen}, maxAge={max_age}, steps={num_steps}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        # --- EDR time series ---\n",
    "        ax0 = axs[0]\n",
    "        for g in goal_edges:\n",
    "            for run_edr in edr_time_series[g]:\n",
    "                ax0.plot(run_edr, color=goal_colors[g], alpha=0.3)\n",
    "            ax0.plot(np.mean(edr_time_series[g], axis=0), color=goal_colors[g], linewidth=2, label=f\"EDR {g}\")\n",
    "        ax0.set_title(\"EDRs Over Time\")\n",
    "        ax0.set_xlabel(\"Timestep\")\n",
    "        ax0.set_ylabel(\"EDR\")\n",
    "        ax0.set_ylim(0, 1.05)\n",
    "        ax0.legend()\n",
    "\n",
    "        # --- Jain's Index time series ---\n",
    "        ax1 = axs[1]\n",
    "        for run_jain in jain_time_series:\n",
    "            ax1.plot(run_jain, color=\"gray\", alpha=0.3)\n",
    "        ax1.plot(np.mean(jain_time_series, axis=0), color=\"black\", linewidth=2, label=\"Jain’s Index\")\n",
    "        ax1.set_title(\"Jain's Index Over Time\")\n",
    "        ax1.set_xlabel(\"Timestep\")\n",
    "        ax1.set_ylabel(\"Jain's Fairness\")\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.legend()\n",
    "\n",
    "        # --- Pareto scatter plot ---\n",
    "        ax2 = axs[2]\n",
    "        ax2.scatter(all_tp, all_jains, c='crimson', s=60, label=\"Runs\")\n",
    "        ax2.scatter(np.mean(all_tp), np.mean(all_jains), c='black', s=100, marker='x', label=\"Mean\")\n",
    "        ax2.set_title(\"Pareto Points Across Seeds\")\n",
    "        ax2.set_xlabel(\"Avg Throughput\")\n",
    "        ax2.set_ylabel(\"Avg Jain\")\n",
    "        ax2.set_xlim(0, max(all_tp) * 1.1)\n",
    "        ax2.set_ylim(0, 1.05)\n",
    "        for tp, jn in zip(all_tp, all_jains):\n",
    "            ax2.text(tp + 0.002, jn, f\"({tp:.2f}, {jn:.2f})\", fontsize=8)\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"avg_edrs\": {g: np.mean(all_edrs[g]) for g in goal_edges},\n",
    "        \"avg_tp\": np.mean(all_tp),\n",
    "        \"avg_jain\": np.mean(all_jains),\n",
    "        \"edrs_by_seed\": all_edrs,\n",
    "        \"tp_by_seed\": all_tp,\n",
    "        \"jain_by_seed\": all_jains\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d12b6",
   "metadata": {},
   "source": [
    "#  **Q-LEARNING CODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f82aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_q_learning_linear_policy(\n",
    "    initialEdges, goalEdges, totalSteps,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\", reward_epsilon=1e-3,\n",
    "    noop_penalty=0.01, log_interval=10000,\n",
    "    softmax=True, temperature=1.0, temperature_decay=0.9999,\n",
    "    epsilon=0.01\n",
    "):\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    Q = LinearQApproximator(feature_size=len(initialEdges) + len(goalEdges))\n",
    "    temperature_curr = temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "    def select_action(state, temperature):\n",
    "        feats = featurize_state(state, goalEdges)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges)\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        if softmax:\n",
    "            q_vals = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "            scaled_qs = q_vals / max(temperature, 1e-6)\n",
    "            exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "            probs = exp_qs / np.sum(exp_qs)\n",
    "            chosen = acts[np.random.choice(len(acts), p=probs)]\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                chosen = random.choice(acts)\n",
    "            else:\n",
    "                chosen = max(acts, key=lambda a: Q.get_q_value(feats, a))\n",
    "\n",
    "        return chosen\n",
    "\n",
    "    state = current\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        temperature_curr = max(0.01, temperature_curr * temperature_decay)\n",
    "\n",
    "        action = select_action(state, temperature_curr)\n",
    "        next_state = performAction(action, state)\n",
    "        next_state = ageEntanglements(next_state, maxAge)\n",
    "        next_state = generateEntanglement(next_state, pGen)\n",
    "\n",
    "        r, succ = compute_reward(\n",
    "            action, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, epsilon=reward_epsilon, noop_penalty=noop_penalty\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = action\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        augmented_next_state = (next_state[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "\n",
    "        feats = featurize_state(state, goalEdges)\n",
    "        feats_next = featurize_state(augmented_next_state, goalEdges)\n",
    "        next_actions = get_possible_multi_actions(augmented_next_state[0], goalEdges)\n",
    "        max_q_next = max([Q.get_q_value(feats_next, a) for a in next_actions], default=0.0)\n",
    "\n",
    "        target = r + gamma * max_q_next\n",
    "        current_q = Q.get_q_value(feats, action)\n",
    "        diff = abs(current_q - target)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        for g in goalEdges:\n",
    "            if action[1] is not None and g in action[1]:\n",
    "                q_value_diffs_per_goal[g].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal[g].append(np.nan)\n",
    "\n",
    "        Q.update(feats, action, target, alpha)\n",
    "        state = augmented_next_state\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history\n",
    "\n",
    "def train_q_learning_linear_policy(\n",
    "    edges, goal_edges, p_swap, p_gen, max_age,\n",
    "    seed, totalSteps, alpha, gamma,\n",
    "    edr_window_size, reward_mode, reward_epsilon,\n",
    "    noop_penalty, log_interval,\n",
    "    softmax, temperature, temperature_decay,\n",
    "    epsilon\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    result = run_q_learning_linear_policy(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=p_gen,\n",
    "        pSwap=p_swap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        reward_epsilon=reward_epsilon,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        softmax=softmax,\n",
    "        temperature=temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        epsilon=epsilon\n",
    "    )\n",
    "\n",
    "    if result is None:\n",
    "        print(\"Error: Q-learning returned None.\")\n",
    "        return\n",
    "\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist = result\n",
    "\n",
    "    # Plot Q-value convergence\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(q_diffs)\n",
    "    plt.xlabel(\"Q-learning Updates\")\n",
    "    plt.ylabel(\"Q-value Difference\")\n",
    "    plt.title(\"Q-value Convergence (Global)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot EDR evolution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(edr_steps, edr_hist[g], label=f\"EDR {g}\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"EDR Estimate\")\n",
    "    plt.title(\"EDR Evolution During Training\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7e7f7",
   "metadata": {},
   "source": [
    "#  **SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c42baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_step_sarsa_linear_multi(\n",
    "    initialEdges, goalEdges, totalSteps, nLookahead,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\", reward_epsilon=1e-3,\n",
    "    noop_penalty=0.01, log_interval=10000,\n",
    "    initial_temperature=1.0, temperature_decay=0.9999, nestedSwaps=False\n",
    "):\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    state_buffer = deque([current])\n",
    "    reward_buffer = deque()\n",
    "    Q = LinearQApproximator(feature_size=len(initialEdges) + len(goalEdges))\n",
    "    temperature = initial_temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "    action_counter = {\n",
    "        'noop': 0,\n",
    "        'goal1': 0,\n",
    "        'goal2': 0,\n",
    "        'both': 0\n",
    "    }\n",
    "\n",
    "    def select_action(state, temperature):\n",
    "        feats = featurize_state(state, goalEdges)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "\n",
    "        \n",
    "        # Debug: Print all possible actions and their Q-values\n",
    "        # print(\"Possible actions and Q-values:\")\n",
    "        # for action in acts:\n",
    "        #     action_q_value = Q.get_q_value(feats, action)\n",
    "        #     print(f\"Action: {action}, Q-value: {action_q_value}\")\n",
    "\n",
    "        # Ensure the no-op action is included\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        # Calculate Q-values and probabilities using softmax\n",
    "        q_values = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "        scaled_qs = q_values / max(temperature, 1e-6)\n",
    "        exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "        probs = exp_qs / np.sum(exp_qs)\n",
    "        \n",
    "        # Sample action based on the probabilities\n",
    "        idx = np.random.choice(len(acts), p=probs)\n",
    "        chosen = acts[idx]\n",
    "\n",
    "        # Debug: Print the chosen action\n",
    "        # print(f\"Chosen action: {chosen}\")\n",
    "\n",
    "        # Track the action type for frequency analysis\n",
    "        if chosen == ([], None):\n",
    "            action_counter['noop'] += 1\n",
    "        elif len(chosen[1]) == 1:\n",
    "            if chosen[1][0] == goalEdges[0]:\n",
    "                action_counter['goal1'] += 1\n",
    "            else:\n",
    "                action_counter['goal2'] += 1\n",
    "        elif len(chosen[1]) == 2:\n",
    "            action_counter['both'] += 1\n",
    "\n",
    "        return chosen\n",
    "\n",
    "\n",
    "    action_buffer = deque([select_action(current, temperature)])\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        temperature = max(0.01, temperature * temperature_decay)\n",
    "\n",
    "        S_t = state_buffer[-1]\n",
    "        A_t = action_buffer[-1]\n",
    "\n",
    "        ns = performAction(A_t, S_t)\n",
    "        ns = ageEntanglements(ns, maxAge)\n",
    "        ns = generateEntanglement(ns, pGen)\n",
    "\n",
    "        r, succ = compute_reward(\n",
    "            A_t, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, epsilon=reward_epsilon, noop_penalty=noop_penalty\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = A_t\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        reward_buffer.append(r)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        next_state = (ns[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "\n",
    "        A_next = select_action(next_state, temperature)\n",
    "\n",
    "        state_buffer.append(next_state)\n",
    "        action_buffer.append(A_next)\n",
    "\n",
    "        if len(reward_buffer) >= nLookahead:\n",
    "            G = sum((gamma**i) * reward_buffer[i] for i in range(nLookahead))\n",
    "            s_n = state_buffer[nLookahead]\n",
    "            a_n = action_buffer[nLookahead]\n",
    "            feats_n = featurize_state(s_n, goalEdges)\n",
    "            G += (gamma**nLookahead) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "            s_tau = state_buffer[0]\n",
    "            a_tau = action_buffer[0]\n",
    "            feats_tau = featurize_state(s_tau, goalEdges)\n",
    "            old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "            diff = abs(G - old_q)\n",
    "            q_value_diffs.append(diff)\n",
    "\n",
    "            for gg in goalEdges:\n",
    "                if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                    q_value_diffs_per_goal[gg].append(diff)\n",
    "                else:\n",
    "                    q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "            Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "            state_buffer.popleft()\n",
    "            action_buffer.popleft()\n",
    "            reward_buffer.popleft()\n",
    "\n",
    "    while reward_buffer:\n",
    "        n = len(reward_buffer)\n",
    "        G = sum((gamma**i) * reward_buffer[i] for i in range(n))\n",
    "        if n < len(state_buffer):\n",
    "            s_n = state_buffer[n]\n",
    "            a_n = action_buffer[n]\n",
    "            feats_n = featurize_state(s_n, goalEdges)\n",
    "            G += (gamma**n) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "        s_tau = state_buffer[0]\n",
    "        a_tau = action_buffer[0]\n",
    "        feats_tau = featurize_state(s_tau, goalEdges)\n",
    "        old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "        diff = abs(G - old_q)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        for gg in goalEdges:\n",
    "            if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                q_value_diffs_per_goal[gg].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "        Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "        state_buffer.popleft()\n",
    "        action_buffer.popleft()\n",
    "        reward_buffer.popleft()\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history, action_counter\n",
    "\n",
    "######################################################################################################################################################\n",
    "\n",
    "def train_sarsa_linear_policy(\n",
    "    edges, goal_edges, p_swap, p_gen, max_age,\n",
    "    seed,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode, reward_epsilon,\n",
    "    noop_penalty, log_interval,\n",
    "    initial_temperature, temperature_decay, nestedSwaps\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Run the SARSA multi-step learning\n",
    "    result = run_n_step_sarsa_linear_multi(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        nLookahead=nLookahead,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=p_gen,\n",
    "        pSwap=p_swap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        reward_epsilon=reward_epsilon,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        initial_temperature=initial_temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        nestedSwaps=nestedSwaps\n",
    "    )\n",
    "\n",
    "    # Check if result is valid\n",
    "    if result is None:\n",
    "        print(\"Error: run_n_step_sarsa_linear_multi returned None.\")\n",
    "        return\n",
    "\n",
    "    # Unpack results\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist, action_counter = result\n",
    "\n",
    "    # --- Q-value convergence ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(q_diffs)\n",
    "    plt.xlabel(\"SARSA Updates\")\n",
    "    plt.ylabel(\"Q-value Difference\")\n",
    "    plt.title(\"Q-value Convergence (Global)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- EDR evolution ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(edr_steps, edr_hist[g], label=f\"EDR {g}\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"EDR Estimate\")\n",
    "    plt.title(\"EDR Evolution During Training\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Action frequency plot ---\n",
    "    labels = ['No-op', 'Goal 1', 'Goal 2', 'Both Goals']\n",
    "    frequencies = [\n",
    "        action_counter['noop'],\n",
    "        action_counter['goal1'],\n",
    "        action_counter['goal2'],\n",
    "        action_counter['both']\n",
    "    ]\n",
    "    frequencies = [f / totalSteps for f in frequencies]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, frequencies, color=['gray', 'blue', 'green', 'orange'])\n",
    "    plt.xlabel(\"Action Type\")\n",
    "    plt.ylabel(\"Proportion of Total Actions\")\n",
    "    plt.title(\"Proportion of Each Action Type Chosen During Training\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7d281",
   "metadata": {},
   "source": [
    "# **RUNNING CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae1a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[(0, 1)]], [(0, 3)])\n",
      "([[(0, 1), (1, 2)]], [(0, 3)])\n",
      "([[(0, 1), (1, 2), (2, 3)]], [(0, 3)])\n",
      "([], None)\n"
     ]
    }
   ],
   "source": [
    "edges = [(0,1), (1,2), (2,3)]\n",
    "goals = [(0,3)]\n",
    "\n",
    "# fake a situation\n",
    "raw = [(e, 1) for e in edges]\n",
    "state = (tuple(raw), (0.0,))\n",
    "\n",
    "acts = get_possible_multi_actions(state[0], goals, nestedSwaps=True)\n",
    "for a in acts:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb17739",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges       = [(0,1), (1,3), (2,3), (3,4), (4,5)]\n",
    "goal_edges  = [(0,5), (2,4)]\n",
    "pSwap       = 0.6\n",
    "pGen        = 0.6\n",
    "maxAge      = 2\n",
    "\n",
    "\n",
    "maxAge      = 2\n",
    "totalSteps     = 1000_000\n",
    "nLookahead     = 3\n",
    "gamma          = 0.99\n",
    "alpha          = 0.01\n",
    "windowSize     = 1000\n",
    "reward_mode    = 'basic'\n",
    "reward_epsilon = 1e-3\n",
    "\n",
    "# Softmax temperature parameters\n",
    "initial_temperature = 5.0\n",
    "final_temperature   = 0.1\n",
    "temperature_decay   = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 0.9))\n",
    "\n",
    "# Seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# --- Train N-step SARSA policy (with temperature-based softmax) ---\n",
    "Q1 = train_sarsa_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    reward_epsilon=reward_epsilon,\n",
    "    noop_penalty=0.0,\n",
    "    log_interval=1000,\n",
    "    initial_temperature=initial_temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    nestedSwaps=True\n",
    ")\n",
    "print('done training')\n",
    "# --- Evaluate learned policy ---\n",
    "simulate_policy(\n",
    "    Q_table=Q1,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=50_000,\n",
    "    edr_window_size=windowSize,\n",
    "    plot=True\n",
    ")\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "edges       = [(0,1), (1,3), (2,3), (3,4), (4,5)]\n",
    "goal_edges  = [(0,5), (2,4)]\n",
    "pSwap       = 0.6\n",
    "pGen        = 0.6\n",
    "maxAge      = 2\n",
    "\n",
    "totalSteps     = 10_000_000\n",
    "gamma          = 0.99\n",
    "alpha          = 0.01\n",
    "windowSize     = 1000\n",
    "reward_mode    = 'basic'\n",
    "reward_epsilon = 1e-3\n",
    "\n",
    "initial_temperature = 5.0\n",
    "final_temperature   = 0.1\n",
    "temperature_decay   = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 0.9))\n",
    "\n",
    "epsilon = 0.01  # Only used if softmax=False\n",
    "softmax = True\n",
    "\n",
    "# === Seed setup ===\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# === Train Q-learning policy ===\n",
    "Q_qlearning = train_q_learning_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    reward_epsilon=reward_epsilon,\n",
    "    noop_penalty=0.0,\n",
    "    log_interval=1000,\n",
    "    softmax=softmax,\n",
    "    temperature=initial_temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    epsilon=epsilon\n",
    ")\n",
    "print(\"Done training Q-learning policy\")\n",
    "\n",
    "# === Evaluate learned Q-learning policy ===\n",
    "simulate_policy(\n",
    "    Q_table=Q_qlearning,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=50_000,\n",
    "    edr_window_size=windowSize,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# # === Run multiple simulations with different seeds ===\n",
    "# multi_simulate_policy(\n",
    "#     Q_table=Q_qlearning,\n",
    "#     edges=edges,\n",
    "#     goal_edges=goal_edges,\n",
    "#     p_swap=pSwap,\n",
    "#     p_gen=pGen,\n",
    "#     max_age=maxAge,\n",
    "#     num_steps=20_000,\n",
    "#     seeds=[10, 20, 30, 40, 50]\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
