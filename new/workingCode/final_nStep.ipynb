{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd38cb70",
   "metadata": {},
   "source": [
    "# **N-STEP SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque, defaultdict, Counter\n",
    "#from linearApprox import *\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0c9b5",
   "metadata": {},
   "source": [
    "#  **UTILITY CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_state(state, edrs, goal_order=None):\n",
    "    if goal_order is None:\n",
    "        goal_order = sorted(edrs.keys())\n",
    "    edr_vector = tuple(edrs[goal] for goal in goal_order)\n",
    "    sorted_state = sorted(state, key=lambda x: (x[0][0], x[0][1]))\n",
    "    return (tuple(sorted_state), edr_vector)\n",
    "\n",
    "def ageEntanglements(augmented_state, maxAge):\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = []\n",
    "    for edge, age in ent_state:\n",
    "        if age >= 0:\n",
    "            new_age = age + 1\n",
    "            if new_age > maxAge:\n",
    "                new_state.append((edge, -1))\n",
    "            else:\n",
    "                new_state.append((edge, new_age))\n",
    "        else:\n",
    "            new_state.append((edge, age))\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "def generateEntanglement(augmented_state, pGen, initial_edges):\n",
    "    ent_state, edr_bins = augmented_state\n",
    "\n",
    "    # Set of initial edges (normalized to (min(u,v), max(u,v)) form)\n",
    "    initial_edges_set = { (min(u,v), max(u,v)) for (u,v) in initial_edges }\n",
    "\n",
    "    new_state = []\n",
    "    seen_edges = set()\n",
    "\n",
    "    for edge, age in ent_state:\n",
    "        normalized_edge = (min(edge[0], edge[1]), max(edge[0], edge[1]))\n",
    "        \n",
    "        if normalized_edge in initial_edges_set:\n",
    "            # This is an initial edge, so it can regrow\n",
    "            if age < 0:  # currently dead\n",
    "                if random.random() < pGen:\n",
    "                    new_state.append((normalized_edge, 1))\n",
    "                else:\n",
    "                    new_state.append((normalized_edge, -1))\n",
    "            else:\n",
    "                new_state.append((normalized_edge, age))\n",
    "        else:\n",
    "            # Non-initial edge: only keep if still alive\n",
    "            if age >= 0:\n",
    "                new_state.append((normalized_edge, age))\n",
    "            # Otherwise, if dead and non-initial, don't add back at all (completely delete)\n",
    "        \n",
    "        seen_edges.add(normalized_edge)\n",
    "\n",
    "    # Safety check: Add missing initial edges if completely missing\n",
    "    for edge in initial_edges_set:\n",
    "        if edge not in seen_edges:\n",
    "            if random.random() < pGen:\n",
    "                new_state.append((edge, 1))\n",
    "            else:\n",
    "                new_state.append((edge, -1))\n",
    "\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jains_index(edrs):\n",
    "    \"\"\"Compute Jain's Fairness Index.\"\"\"\n",
    "    if all(edr == 0 for edr in edrs.values()):\n",
    "        return 0.0\n",
    "    numerator = sum(edrs.values())**2\n",
    "    denominator = len(edrs) * sum(v**2 for v in edrs.values())\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def featurize_state(state, goal_order, master_edge_list):\n",
    "    ent_state, edrs = state\n",
    "    edge_age_map = {edge: age for edge, age in ent_state}\n",
    "\n",
    "    edge_features = []\n",
    "    for edge in master_edge_list:\n",
    "        age = edge_age_map.get(edge, -1)\n",
    "        edge_features.append(age / 10.0 if age >= 0 else -1.0)\n",
    "\n",
    "    edr_features = list(edrs)\n",
    "    return np.array(edge_features + edr_features, dtype=np.float32)\n",
    "\n",
    "\n",
    "class LinearQApproximator:\n",
    "    def __init__(self, feature_size):\n",
    "        self.weights = {}  # Dict[action_key] = weight_vector\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def _action_key(self, action):\n",
    "        consumed_paths, goal_list = action\n",
    "\n",
    "        # Handle the no-op case: goal_list is None for no-op action\n",
    "        if goal_list is None:\n",
    "            return ((), None)  # Ensure this is hashable for no-op action\n",
    "\n",
    "        # Sort paths and goals for consistency in hashing\n",
    "        sorted_paths = tuple(sorted(tuple(sorted(path)) for path in consumed_paths))\n",
    "        sorted_goals = tuple(sorted(goal_list))\n",
    "        return (sorted_paths, sorted_goals)\n",
    "\n",
    "    def _init_weights(self, action_key):\n",
    "        if action_key not in self.weights:\n",
    "            self.weights[action_key] = np.zeros(self.feature_size)\n",
    "\n",
    "    def get_q_value(self, features, action):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        return float(np.dot(self.weights[key], features))\n",
    "\n",
    "    def update(self, features, action, target, alpha):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        prediction = np.dot(self.weights[key], features)\n",
    "        error = target - prediction\n",
    "        self.weights[key] += alpha * error * features\n",
    "\n",
    "\n",
    "def get_possible_multi_actionsold(ent_state, goalEdges, nestedSwaps=False, max_path_length=None):\n",
    "    import itertools\n",
    "\n",
    "    actions = []\n",
    "    existing_edges = {tuple(sorted(edge)) for edge, age in ent_state if age >= 0}\n",
    "\n",
    "    def find_paths(start, visited=None, path=None, depth=0):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if path is None:\n",
    "            path = []\n",
    "\n",
    "        paths = []\n",
    "        for edge in existing_edges:\n",
    "            if edge in visited:\n",
    "                continue\n",
    "            u, v = edge\n",
    "            if u == start or v == start:\n",
    "                next_node = v if u == start else u\n",
    "                new_path = path + [edge]\n",
    "                paths.append(new_path)\n",
    "                if max_path_length is None or depth < max_path_length:\n",
    "                    paths.extend(find_paths(next_node, visited | {edge}, new_path, depth + 1))\n",
    "        return paths\n",
    "\n",
    "    single_goal_actions = []\n",
    "    for goal in goalEdges:\n",
    "        start, end = goal\n",
    "        paths = find_paths(start)\n",
    "        for path in paths:\n",
    "            if not path or len(path)<2:\n",
    "                continue\n",
    "            nodes = [n for edge in path for n in edge]\n",
    "            counts = {node: nodes.count(node) for node in nodes}\n",
    "            endpoints = [node for node, count in counts.items() if count == 1]\n",
    "            if len(endpoints) != 2:\n",
    "                continue\n",
    "            if not nestedSwaps:\n",
    "                if set(endpoints) != set(goal):\n",
    "                    continue\n",
    "            normalized_path = [tuple(sorted(e)) for e in path]\n",
    "            single_goal_actions.append((normalized_path, goal))\n",
    "            actions.append(([normalized_path], [goal]))\n",
    "\n",
    "    for k in range(2, len(single_goal_actions) + 1):\n",
    "        for combo in itertools.combinations(single_goal_actions, k):\n",
    "            paths, goals = zip(*combo)\n",
    "            flat_edges = [tuple(sorted(e)) for path in paths for e in path]\n",
    "            if len(flat_edges) == len(set(flat_edges)):\n",
    "                actions.append((list(paths), list(goals)))\n",
    "\n",
    "    actions.append(([], None))\n",
    "    return actions\n",
    "\n",
    "\n",
    "def compute_reward(action, goal_success_queues, pSwap, mode=\"basic\", alphaReward=1.0, noop_penalty=0.0):\n",
    "    epsilon = 1e-7  # smaller epsilon\n",
    "    consumed_edges, goals = action\n",
    "\n",
    "    if not goals or not consumed_edges:\n",
    "        return -noop_penalty, False\n",
    "\n",
    "    total_reward = 0.0\n",
    "    any_success = False\n",
    "    used_edges = set()\n",
    "\n",
    "    for goal, path in zip(goals, consumed_edges):\n",
    "        path_edges = set(path)\n",
    "        if not path_edges.isdisjoint(used_edges):\n",
    "            continue\n",
    "        used_edges.update(path_edges)\n",
    "\n",
    "        success_prob = pSwap ** (len(path) - 1)\n",
    "        edr = sum(goal_success_queues[goal]) / len(goal_success_queues[goal]) + epsilon\n",
    "        x = success_prob / edr\n",
    "        success = (random.random() < success_prob)\n",
    "        any_success = any_success or success\n",
    "\n",
    "        if mode == \"basic\":\n",
    "            if success:\n",
    "                total_reward += math.log(1 + x)\n",
    "        elif mode == \"partial\":\n",
    "            base = math.log(1 + x)\n",
    "            total_reward += base if success else 0.5 * base\n",
    "        elif mode == 'linear':\n",
    "            if success:\n",
    "                total_reward += x\n",
    "        elif mode == 'underserve':\n",
    "            if success:\n",
    "                total_reward += math.log(1/(edr+epsilon))\n",
    "        elif mode == \"alphafair\":\n",
    "            if alphaReward == 1.0:\n",
    "                utility = math.log(1 + x)\n",
    "            else:\n",
    "                utility = ((1 + x) ** (1 - alphaReward) - 1) / (1 - alphaReward)\n",
    "\n",
    "            if success:\n",
    "                total_reward += utility\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown reward mode: {mode}\")\n",
    "\n",
    "    return total_reward, any_success\n",
    "\n",
    "\n",
    "def performAction(action, augmented_state, pSwap, nestedSwaps=False, system_goals=None):\n",
    "    consumed_paths, _ = action\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = list(ent_state)\n",
    "\n",
    "    normalized_goal_edges = set((min(u, v), max(u, v)) for u, v in (system_goals or []))\n",
    "    busy_nodes = set(u for (u, v), age in ent_state if age >= 0 for u, v in [(u, v)])\n",
    "\n",
    "    used_edges = set()  # Tracks entanglements used this timestep (normalized)\n",
    "\n",
    "    for path in consumed_paths:\n",
    "        if not path:\n",
    "            continue\n",
    "\n",
    "        normalized_path = [tuple(sorted(e)) for e in path]\n",
    "\n",
    "        # Prevent reusing entanglements already used this timestep\n",
    "        if any(edge in used_edges for edge in normalized_path):\n",
    "            continue\n",
    "\n",
    "        consumed_ages = []\n",
    "        for edge_to_consume in normalized_path:\n",
    "            for i, (edge, age) in enumerate(new_state):\n",
    "                if tuple(sorted(edge)) == edge_to_consume:\n",
    "                    consumed_ages.append(age)\n",
    "                    busy_nodes.discard(edge[0])\n",
    "                    busy_nodes.discard(edge[1])\n",
    "                    new_state[i] = (edge, -1)\n",
    "                    break\n",
    "\n",
    "        # Mark all edges in this path as used\n",
    "        used_edges.update(normalized_path)\n",
    "\n",
    "        # Attempt swap\n",
    "        success_prob = pSwap ** (len(path) - 1)\n",
    "        swap_success = random.random() < success_prob\n",
    "\n",
    "        if not swap_success:\n",
    "            continue\n",
    "\n",
    "        # Determine new edge from endpoints\n",
    "        nodes = [n for edge in normalized_path for n in edge]\n",
    "        node_counts = {node: nodes.count(node) for node in nodes}\n",
    "        endpoints = [node for node, count in node_counts.items() if count == 1]\n",
    "\n",
    "        if len(endpoints) != 2:\n",
    "            continue\n",
    "\n",
    "        start, end = endpoints\n",
    "        new_edge = (min(start, end), max(start, end))\n",
    "\n",
    "        if new_edge in normalized_path:\n",
    "            continue\n",
    "\n",
    "        if start in busy_nodes or end in busy_nodes:\n",
    "            continue\n",
    "\n",
    "        new_age = max(consumed_ages) if consumed_ages else 0\n",
    "        alive_edges = {edge for edge, age in new_state if age >= 0}\n",
    "\n",
    "        if new_edge not in alive_edges:\n",
    "            if nestedSwaps or new_edge in normalized_goal_edges:\n",
    "                new_state.append((new_edge, new_age))\n",
    "                busy_nodes.update([start, end])\n",
    "\n",
    "    # Remove dead edges\n",
    "    new_state = [pair for pair in new_state if pair[1] != -1]\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "def get_possible_multi_actions(ent_state, goalEdges, nestedSwaps=False, max_path_length=None):\n",
    "    import itertools\n",
    "\n",
    "    actions = []\n",
    "    existing_edges = {tuple(sorted(edge)) for edge, age in ent_state if age >= 0}\n",
    "\n",
    "    def find_paths(start, visited=None, path=None, depth=0):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if path is None:\n",
    "            path = []\n",
    "\n",
    "        paths = []\n",
    "        for edge in existing_edges:\n",
    "            if edge in visited:\n",
    "                continue\n",
    "            u, v = edge\n",
    "            if u == start or v == start:\n",
    "                next_node = v if u == start else u\n",
    "                new_path = path + [edge]\n",
    "                paths.append(new_path)\n",
    "                if max_path_length is None or depth < max_path_length:\n",
    "                    paths.extend(find_paths(next_node, visited | {edge}, new_path, depth + 1))\n",
    "        return paths\n",
    "\n",
    "    # Single-goal actions\n",
    "    single_goal_actions = []\n",
    "    for goal in goalEdges:\n",
    "        start, end = goal\n",
    "        paths = find_paths(start)\n",
    "        for path in paths:\n",
    "            if not path or len(path) < 2:\n",
    "                continue\n",
    "\n",
    "            nodes = [n for edge in path for n in edge]\n",
    "            counts = {node: nodes.count(node) for node in nodes}\n",
    "            endpoints = [node for node, count in counts.items() if count == 1]\n",
    "            if len(endpoints) != 2:\n",
    "                continue\n",
    "\n",
    "            if set(endpoints) != set(goal):\n",
    "                continue\n",
    "\n",
    "            normalized_path = [tuple(sorted(e)) for e in path]\n",
    "            single_goal_actions.append((normalized_path, goal))\n",
    "            actions.append(([normalized_path], [goal]))\n",
    "\n",
    "    # Multi-goal disjoint actions\n",
    "    for k in range(2, len(single_goal_actions) + 1):\n",
    "        for combo in itertools.combinations(single_goal_actions, k):\n",
    "            paths, goals = zip(*combo)\n",
    "\n",
    "            if len(set(goals)) != len(goals):  # ðŸ›¡ï¸ Prevent duplicate goals\n",
    "                continue\n",
    "\n",
    "            flat_edges = [tuple(sorted(e)) for path in paths for e in path]\n",
    "            if len(flat_edges) == len(set(flat_edges)):  # ðŸ›¡ï¸ Ensure disjoint paths\n",
    "                actions.append((list(paths), list(goals)))\n",
    "\n",
    "    # Always allow no-op action\n",
    "    actions.append(([], None))\n",
    "\n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6dc1",
   "metadata": {},
   "source": [
    "# **SIMULATION CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    pSwap,\n",
    "    pGen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    plot=True,\n",
    "    nestedSwaps=False\n",
    "):\n",
    "    nodes = set()\n",
    "    for u, v in edges:\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    nodes = sorted(list(nodes))\n",
    "\n",
    "    master_edge_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    raw = [(e, -1) for e in edges]\n",
    "    current = get_augmented_state(raw, {g:0.0 for g in goal_edges}, goal_order=goal_edges)\n",
    "\n",
    "    recent = {g: [] for g in goal_edges}\n",
    "    edr_hist, jain_hist, tp_hist = {g:[] for g in goal_edges}, [], []\n",
    "    valids, acts, qvals = [], [], []\n",
    "\n",
    "    # New counters\n",
    "    wait_when_options = 0\n",
    "    options_available = 0\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        ent_state, _ = current\n",
    "        acts_all = get_possible_multi_actions(ent_state, goal_edges, nestedSwaps=nestedSwaps)\n",
    "\n",
    "        if ([], None) not in acts_all:\n",
    "            acts_all.append(([], None))\n",
    "        real = [a for a in acts_all if a != ([], None)]\n",
    "        avail = len(real) > 0\n",
    "        valids.append(1 if avail else 0)\n",
    "\n",
    "        feats = featurize_state(current, goal_edges, master_edge_list)\n",
    "        best_a, best_q = max(((a, Q_table.get_q_value(feats, a)) for a in acts_all), key=lambda x: x[1])\n",
    "        qvals.append(best_q)\n",
    "        acts.append(1.0 if (avail and best_a in real) else 0.0)\n",
    "\n",
    "        # --- Track waiting behavior ---\n",
    "        if avail:\n",
    "            options_available += 1\n",
    "            if best_a == ([], None):\n",
    "                wait_when_options += 1\n",
    "\n",
    "        nxt = performAction(best_a, current, pSwap=pSwap, nestedSwaps=nestedSwaps)\n",
    "        nxt = ageEntanglements(nxt, max_age)\n",
    "        nxt = generateEntanglement(nxt, pGen, edges)\n",
    "\n",
    "        consumed_paths, goals = best_a\n",
    "        if goals is not None:\n",
    "            for g, path in zip(goals, consumed_paths):\n",
    "                if path:\n",
    "                    succ = random.random() < (pSwap ** (len(path) - 1))\n",
    "                    recent[g].append(1 if succ else 0)\n",
    "                else:\n",
    "                    recent[g].append(0)\n",
    "            for g in goal_edges:\n",
    "                if g not in goals:\n",
    "                    recent[g].append(0)\n",
    "        else:\n",
    "            for g in goal_edges:\n",
    "                recent[g].append(0)\n",
    "\n",
    "        if len(recent[g]) > edr_window_size:\n",
    "            recent[g].pop(0)\n",
    "\n",
    "        edrs = {g: sum(recent[g]) / len(recent[g]) for g in goal_edges}\n",
    "        for g in goal_edges:\n",
    "            edr_hist[g].append(edrs[g])\n",
    "\n",
    "        total = sum(edrs.values())\n",
    "        tp_hist.append(total)\n",
    "        jain_hist.append(jains_index(edrs))\n",
    "\n",
    "        current = get_augmented_state(nxt[0], edrs, goal_order=goal_edges)\n",
    "\n",
    "    if plot:\n",
    "        # === Define fixed colors ===\n",
    "        color_goal_1 = 'blue'\n",
    "        color_goal_2 = 'orange'\n",
    "        color_jain = 'purple'\n",
    "        goal_colors = [color_goal_1, color_goal_2]\n",
    "\n",
    "        # (1) EDR + Jain\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i, g in enumerate(goal_edges):\n",
    "            plt.plot(edr_hist[g], label=f\"EDR {g}\", color=goal_colors[i % len(goal_colors)], linewidth=2)\n",
    "        plt.plot(jain_hist, '--', label=\"Jain's Index\", linewidth=2, color=color_jain)\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # (2) Pareto Point\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        avg_tp = np.mean(tp_hist[burn_in:])\n",
    "        avg_jain = np.mean(jain_hist[burn_in:])\n",
    "        plt.scatter([avg_tp], [avg_jain], s=100, c='crimson')\n",
    "        plt.xlabel(\"Avg Throughput\")\n",
    "        plt.ylabel(\"Avg Jain's Index\")\n",
    "        plt.xlim(0, max(tp_hist) * 1.1)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True)\n",
    "        plt.text(avg_tp, avg_jain, f\" ({avg_tp:.3f}, {avg_jain:.3f})\", fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # (3) Best Q-Value Evolution\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(qvals, color='slateblue', linewidth=2)\n",
    "        plt.xlabel(\"Timestep\")\n",
    "        plt.ylabel(\"Best Q-Value\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Final metrics\n",
    "    burn_in_idx = 5000\n",
    "    final_edrs = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "    final_tp = sum(final_edrs.values())\n",
    "    final_jain = jains_index(final_edrs)\n",
    "\n",
    "    # Compute waiting ratio\n",
    "    if options_available > 0:\n",
    "        wait_ratio = wait_when_options / options_available\n",
    "    else:\n",
    "        wait_ratio = 0.0\n",
    "\n",
    "    print(\"\\nMetrics After Burn-in (first 5000 steps ignored):\")\n",
    "    print(\"Mean EDRs:\", {g: f\"{v:.4f}\" for g, v in final_edrs.items()})\n",
    "    print(f\"Total Throughput (sum of EDRs): {final_tp:.4f}\")\n",
    "    print(f\"Jain's Fairness Index: {final_jain:.4f}\")\n",
    "    print(f\"Waited when could have acted: {wait_when_options}/{options_available} = {wait_ratio:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"edr_history\": edr_hist,\n",
    "        \"jain_history\": jain_hist,\n",
    "        \"throughput_history\": tp_hist,\n",
    "        \"q_values\": qvals,\n",
    "        \"wait_ratio\": wait_ratio  # <<< NEW\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    pSwap,\n",
    "    pGen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    seeds=[10, 20, 30, 40, 50],\n",
    "    plot=True,\n",
    "    nestedSwaps=False\n",
    "):\n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    all_edrs = {g: [] for g in goal_edges}\n",
    "    all_jains = []\n",
    "    all_tp = []\n",
    "\n",
    "    edr_time_series = {g: [] for g in goal_edges}\n",
    "    jain_time_series = []\n",
    "\n",
    "    goal_colors = {\n",
    "        goal_edges[0]: \"tab:blue\",\n",
    "        goal_edges[1]: \"tab:green\"\n",
    "    }\n",
    "\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        result = simulate_policy(\n",
    "            Q_table=Q_table,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=max_age,\n",
    "            num_steps=num_steps,\n",
    "            edr_window_size=edr_window_size,\n",
    "            burn_in=burn_in,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        # Final metrics\n",
    "        edr_hist = result[\"edr_history\"]\n",
    "        jain_hist = result[\"jain_history\"]\n",
    "        tp_hist = result[\"throughput_history\"]\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_time_series[g].append(edr_hist[g])\n",
    "        jain_time_series.append(jain_hist)\n",
    "\n",
    "        burn_in_idx = 5000\n",
    "        edrs_final = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "        tp_final = sum(edrs_final.values())\n",
    "        jain_final = jains_index(edrs_final)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            all_edrs[g].append(edrs_final[g])\n",
    "        all_tp.append(tp_final)\n",
    "        all_jains.append(jain_final)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        fig.suptitle(\n",
    "            f\"Multi-Sim â€” {len(seeds)} Seeds | pSwap={pSwap}, pGen={pGen}, maxAge={max_age}, steps={num_steps}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        # --- EDR time series ---\n",
    "        ax0 = axs[0]\n",
    "        for g in goal_edges:\n",
    "            for run_edr in edr_time_series[g]:\n",
    "                ax0.plot(run_edr, color=goal_colors[g], alpha=0.3)\n",
    "            ax0.plot(np.mean(edr_time_series[g], axis=0), color=goal_colors[g], linewidth=2, label=f\"EDR {g}\")\n",
    "        ax0.set_title(\"EDRs Over Time\")\n",
    "        ax0.set_xlabel(\"Timestep\")\n",
    "        ax0.set_ylabel(\"EDR\")\n",
    "        ax0.set_ylim(0, 1.05)\n",
    "        ax0.legend()\n",
    "\n",
    "        # --- Jain's Index time series ---\n",
    "        ax1 = axs[1]\n",
    "        for run_jain in jain_time_series:\n",
    "            ax1.plot(run_jain, color=\"gray\", alpha=0.3)\n",
    "        ax1.plot(np.mean(jain_time_series, axis=0), color=\"black\", linewidth=2, label=\"Jainâ€™s Index\")\n",
    "        ax1.set_title(\"Jain's Index Over Time\")\n",
    "        ax1.set_xlabel(\"Timestep\")\n",
    "        ax1.set_ylabel(\"Jain's Fairness\")\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.legend()\n",
    "\n",
    "        # --- Pareto scatter plot ---\n",
    "        ax2 = axs[2]\n",
    "        ax2.scatter(all_tp, all_jains, c='crimson', s=60, label=\"Runs\")\n",
    "        ax2.scatter(np.mean(all_tp), np.mean(all_jains), c='black', s=100, marker='x', label=\"Mean\")\n",
    "        ax2.set_title(\"Pareto Points Across Seeds\")\n",
    "        ax2.set_xlabel(\"Avg Throughput\")\n",
    "        ax2.set_ylabel(\"Avg Jain\")\n",
    "        ax2.set_xlim(0, max(all_tp) * 1.1)\n",
    "        ax2.set_ylim(0, 1.05)\n",
    "        for tp, jn in zip(all_tp, all_jains):\n",
    "            ax2.text(tp + 0.002, jn, f\"({tp:.2f}, {jn:.2f})\", fontsize=8)\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"avg_edrs\": {g: np.mean(all_edrs[g]) for g in goal_edges},\n",
    "        \"avg_tp\": np.mean(all_tp),\n",
    "        \"avg_jain\": np.mean(all_jains),\n",
    "        \"edrs_by_seed\": all_edrs,\n",
    "        \"tp_by_seed\": all_tp,\n",
    "        \"jain_by_seed\": all_jains\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverParam(\n",
    "    param_name, param_values,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    edr_results = {g: [] for g in goal_edges}\n",
    "    jain_results = []\n",
    "    pareto_points = []\n",
    "    full_raw_data = {}\n",
    "\n",
    "    for value in param_values:\n",
    "        print(f\"\\n=== Training with {param_name} = {value} ===\")\n",
    "\n",
    "        if param_name == 'pGen':\n",
    "            pGen = value\n",
    "        elif param_name == 'pSwap':\n",
    "            pSwap = value\n",
    "        else:\n",
    "            raise ValueError(\"param_name must be 'pGen' or 'pSwap'.\")\n",
    "\n",
    "        # --- Build kwargs dynamically ---\n",
    "        train_kwargs = {\n",
    "            'edges': edges,\n",
    "            'goal_edges': goal_edges,\n",
    "            'pSwap': pSwap,\n",
    "            'pGen': pGen,\n",
    "            'max_age': max_age,\n",
    "            'seed': seed,\n",
    "            'totalSteps': totalSteps,\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'edr_window_size': edr_window_size,\n",
    "            'reward_mode': reward_mode,\n",
    "            'nestedSwaps': nestedSwaps,\n",
    "        }\n",
    "\n",
    "        # --- Add nLookahead if needed ---\n",
    "        if training_function.__name__ == 'train_sarsa_linear_policy':\n",
    "            train_kwargs['nLookahead'] = nLookahead\n",
    "            train_kwargs['temperature'] = temperature\n",
    "            train_kwargs['temperature_decay'] = temperature_decay\n",
    "            train_kwargs['log_interval'] = edr_window_size\n",
    "        elif training_function.__name__ == 'train_q_learning_linear_policy':\n",
    "            # rename properly for q_learning\n",
    "            train_kwargs['temperature'] = temperature  # <=== fix here\n",
    "            train_kwargs['temperature_decay'] = temperature_decay\n",
    "\n",
    "        # --- Add extra kwargs ---\n",
    "        train_kwargs.update(extra_kwargs)\n",
    "\n",
    "        # --- Now call training function cleanly ---\n",
    "        Q = training_function(**train_kwargs)\n",
    "\n",
    "        # Simulate\n",
    "        sim_result = simulate_policy(\n",
    "            Q_table=Q,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=max_age,\n",
    "            num_steps=simulate_steps,\n",
    "            edr_window_size=edr_window_size,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        # Post-processing\n",
    "        burn_in_steps = int(simulate_steps * burn_in_ratio)\n",
    "        final_edrs = {g: np.mean(sim_result[\"edr_history\"][g][burn_in_steps:]) for g in goal_edges}\n",
    "        final_tp = sum(final_edrs.values())\n",
    "        final_jain = jains_index(final_edrs)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_results[g].append(final_edrs[g])\n",
    "        jain_results.append(final_jain)\n",
    "        pareto_points.append((final_tp, final_jain))\n",
    "        full_raw_data[value] = sim_result\n",
    "\n",
    "    # --- Plotting (same as before) ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(param_values, edr_results[g], marker='o', label=f\"EDR {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.title(f\"EDR vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(param_values, jain_results, marker='s', label=\"Jain's Fairness\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(f\"Jain's Fairness vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    pareto_tp, pareto_jain = zip(*pareto_points)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pareto_tp, pareto_jain, c='crimson', s=80)\n",
    "    for (tp, jain), val in zip(pareto_points, param_values):\n",
    "        plt.text(tp, jain, f\"{val:.2f}\", fontsize=9)\n",
    "    plt.xlabel(\"Throughput (sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.title(f\"Pareto Plot for different {param_name} values\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, max(pareto_tp) * 1.1)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"param_name\": param_name,\n",
    "        \"param_values\": param_values,\n",
    "        \"edr_results\": edr_results,\n",
    "        \"jain_results\": jain_results,\n",
    "        \"pareto_points\": pareto_points,\n",
    "        \"full_raw_data\": full_raw_data\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c779f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverParamRobust(\n",
    "    param_name, param_values,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    trainCount=3,\n",
    "    simulateCount=3,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    edr_results = {g: [] for g in goal_edges}\n",
    "    jain_results = []\n",
    "    pareto_points = []\n",
    "    full_raw_data = {}\n",
    "\n",
    "    for idx_value, value in enumerate(param_values):\n",
    "        print(f\"\\n=== Training with {param_name} = {value} ===\")\n",
    "\n",
    "        # --- Dynamic parameter assignment ---\n",
    "        if param_name == 'pGen':\n",
    "            pGen = value\n",
    "        elif param_name == 'pSwap':\n",
    "            pSwap = value\n",
    "        else:\n",
    "            raise ValueError(\"param_name must be 'pGen' or 'pSwap'.\")\n",
    "\n",
    "        edrs_all_runs = {g: [] for g in goal_edges}\n",
    "        jains_all_runs = []\n",
    "        tp_all_runs = []\n",
    "\n",
    "        for train_idx in range(trainCount):\n",
    "            train_seed = seed + idx_value * 100 + train_idx * 10  # Safe separation\n",
    "            np.random.seed(train_seed)\n",
    "            random.seed(train_seed)\n",
    "\n",
    "            # --- Build kwargs dynamically ---\n",
    "            train_kwargs = {\n",
    "                'edges': edges,\n",
    "                'goal_edges': goal_edges,\n",
    "                'pSwap': pSwap,\n",
    "                'pGen': pGen,\n",
    "                'max_age': max_age,\n",
    "                'seed': train_seed,\n",
    "                'totalSteps': totalSteps,\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'edr_window_size': edr_window_size,\n",
    "                'reward_mode': reward_mode,\n",
    "                'nestedSwaps': nestedSwaps,\n",
    "            }\n",
    "\n",
    "            if training_function.__name__ == 'train_sarsa_linear_policy':\n",
    "                train_kwargs['nLookahead'] = nLookahead\n",
    "                train_kwargs['temperature'] = temperature\n",
    "                train_kwargs['temperature_decay'] = temperature_decay\n",
    "                train_kwargs['log_interval'] = edr_window_size\n",
    "            elif training_function.__name__ == 'train_q_learning_linear_policy':\n",
    "                train_kwargs['temperature'] = temperature\n",
    "                train_kwargs['temperature_decay'] = temperature_decay\n",
    "\n",
    "            train_kwargs.update(extra_kwargs)\n",
    "\n",
    "            Q = training_function(**train_kwargs)\n",
    "\n",
    "            # --- Simulate multiple times for each trained model ---\n",
    "            sim_seeds = [train_seed + s * 1000 for s in range(simulateCount)]  # Different seeds for simulation\n",
    "            multi_sim_result = multi_simulate_policy(\n",
    "                Q_table=Q,\n",
    "                edges=edges,\n",
    "                goal_edges=goal_edges,\n",
    "                pSwap=pSwap,\n",
    "                pGen=pGen,\n",
    "                max_age=max_age,\n",
    "                num_steps=simulate_steps,\n",
    "                edr_window_size=edr_window_size,\n",
    "                burn_in=int(simulate_steps * burn_in_ratio),\n",
    "                seeds=sim_seeds,\n",
    "                plot=False,\n",
    "                nestedSwaps=nestedSwaps\n",
    "            )\n",
    "\n",
    "            # --- Collect results ---\n",
    "            for g in goal_edges:\n",
    "                edrs_all_runs[g].append(multi_sim_result[\"avg_edrs\"][g])\n",
    "            jains_all_runs.append(multi_sim_result[\"avg_jain\"])\n",
    "            tp_all_runs.append(multi_sim_result[\"avg_tp\"])\n",
    "\n",
    "        # --- After all training runs ---\n",
    "        final_avg_edrs = {g: np.mean(edrs_all_runs[g]) for g in goal_edges}\n",
    "        final_avg_jain = np.mean(jains_all_runs)\n",
    "        final_avg_tp = np.mean(tp_all_runs)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_results[g].append(final_avg_edrs[g])\n",
    "        jain_results.append(final_avg_jain)\n",
    "        pareto_points.append((final_avg_tp, final_avg_jain))\n",
    "        full_raw_data[value] = {\n",
    "            \"edrs_by_run\": edrs_all_runs,\n",
    "            \"jains_by_run\": jains_all_runs,\n",
    "            \"tp_by_run\": tp_all_runs\n",
    "        }\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(param_values, edr_results[g], marker='o', label=f\"EDR {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.title(f\"EDR vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(param_values, jain_results, marker='s', label=\"Jain's Fairness\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(f\"Jain's Fairness vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    pareto_tp, pareto_jain = zip(*pareto_points)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pareto_tp, pareto_jain, c='crimson', s=80)\n",
    "    for (tp, jain), val in zip(pareto_points, param_values):\n",
    "        plt.text(tp, jain, f\"{val:.2f}\", fontsize=9)\n",
    "    plt.xlabel(\"Throughput (sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.title(f\"Pareto Plot for different {param_name} values\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, max(pareto_tp) * 1.1)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"param_name\": param_name,\n",
    "        \"param_values\": param_values,\n",
    "        \"edr_results\": edr_results,\n",
    "        \"jain_results\": jain_results,\n",
    "        \"pareto_points\": pareto_points,\n",
    "        \"full_raw_data\": full_raw_data\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83415a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverParamAndRewardMode(\n",
    "    param_name, param_values, reward_modes,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    all_results = {}\n",
    "\n",
    "    for reward_mode in reward_modes:\n",
    "        print(f\"\\n=== Running for reward mode: {reward_mode} ===\")\n",
    "        \n",
    "        results = compareOverParam(\n",
    "            param_name=param_name,\n",
    "            param_values=param_values,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=max_age,\n",
    "            totalSteps=totalSteps,\n",
    "            nLookahead=nLookahead,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            edr_window_size=edr_window_size,\n",
    "            reward_mode=reward_mode,\n",
    "            temperature=temperature,\n",
    "            temperature_decay=temperature_decay,\n",
    "            seed=seed,\n",
    "            nestedSwaps=nestedSwaps,\n",
    "            training_function=training_function,\n",
    "            simulate_steps=simulate_steps,\n",
    "            burn_in_ratio=burn_in_ratio,\n",
    "            **extra_kwargs\n",
    "        )\n",
    "\n",
    "        all_results[reward_mode] = results\n",
    "\n",
    "    # --- Plot Mean EDR\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for reward_mode, res in all_results.items():\n",
    "        for g in goal_edges:\n",
    "            plt.plot(res['param_values'], res['edr_results'][g], marker='o', label=f\"{reward_mode} - Goal {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Jain's Fairness Index\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for reward_mode, res in all_results.items():\n",
    "        plt.plot(res['param_values'], res['jain_results'], marker='s', label=f\"{reward_mode}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Pareto Frontier (Throughput vs Fairness)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for reward_mode, res in all_results.items():\n",
    "        pareto_tp, pareto_jain = zip(*res['pareto_points'])\n",
    "        plt.plot(pareto_tp, pareto_jain, marker='o', label=f\"{reward_mode}\")  # <-- now connect points!\n",
    "    plt.xlabel(\"Throughput (Sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(0, None)\n",
    "    plt.ylim(0.45, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compareOverParamAndRewardMode_robust(\n",
    "    param_name, param_values, reward_modes,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    n_repeats=3,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    all_results = {}\n",
    "\n",
    "    for reward_mode in reward_modes:\n",
    "        print(f\"\\n=== Running for reward mode: {reward_mode} ===\")\n",
    "        \n",
    "        mode_results = []\n",
    "        for repeat in range(n_repeats):\n",
    "            print(f\"  --- Repeat {repeat+1}/{n_repeats} ---\")\n",
    "            \n",
    "            result = compareOverParam(\n",
    "                param_name=param_name,\n",
    "                param_values=param_values,\n",
    "                edges=edges,\n",
    "                goal_edges=goal_edges,\n",
    "                pSwap=pSwap,\n",
    "                pGen=pGen,\n",
    "                max_age=max_age,\n",
    "                totalSteps=totalSteps,\n",
    "                nLookahead=nLookahead,\n",
    "                alpha=alpha,\n",
    "                gamma=gamma,\n",
    "                edr_window_size=edr_window_size,\n",
    "                reward_mode=reward_mode,\n",
    "                temperature=temperature,\n",
    "                temperature_decay=temperature_decay,\n",
    "                seed=seed + repeat,  # different seed for each repeat\n",
    "                nestedSwaps=nestedSwaps,\n",
    "                training_function=training_function,\n",
    "                simulate_steps=simulate_steps,\n",
    "                burn_in_ratio=burn_in_ratio,\n",
    "                **extra_kwargs\n",
    "            )\n",
    "            mode_results.append(result)\n",
    "        \n",
    "        all_results[reward_mode] = mode_results\n",
    "\n",
    "    # --- Plot all individual runs ---\n",
    "    for reward_mode, runs in all_results.items():\n",
    "        for idx, res in enumerate(runs):\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            for g in goal_edges:\n",
    "                plt.plot(res['param_values'], res['edr_results'][g], marker='o', label=f\"Goal {g} (Run {idx+1})\")\n",
    "            plt.xlabel(param_name)\n",
    "            plt.ylabel(\"Mean EDR\")\n",
    "            plt.title(f\"{reward_mode} - EDR (individual runs)\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    for reward_mode, runs in all_results.items():\n",
    "        for idx, res in enumerate(runs):\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(res['param_values'], res['jain_results'], marker='s', label=f\"Run {idx+1}\")\n",
    "            plt.xlabel(param_name)\n",
    "            plt.ylabel(\"Jain's Index\")\n",
    "            plt.title(f\"{reward_mode} - Jain Index (individual runs)\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    for reward_mode, runs in all_results.items():\n",
    "        for idx, res in enumerate(runs):\n",
    "            pareto_tp, pareto_jain = zip(*res['pareto_points'])\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(pareto_tp, pareto_jain, marker='o', label=f\"Run {idx+1}\")\n",
    "            plt.xlabel(\"Throughput (Sum EDRs)\")\n",
    "            plt.ylabel(\"Jain's Index\")\n",
    "            plt.title(f\"{reward_mode} - Pareto (individual runs)\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.xlim(0, None)\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # --- Now calculate MEANS ---\n",
    "    mean_results = {}\n",
    "\n",
    "    for reward_mode, runs in all_results.items():\n",
    "        param_vals = runs[0]['param_values']\n",
    "        \n",
    "        # Aggregate all EDRs and Jains\n",
    "        edr_means = {g: np.mean([np.array(run['edr_results'][g]) for run in runs], axis=0) for g in goal_edges}\n",
    "        jain_means = np.mean([np.array(run['jain_results']) for run in runs], axis=0)\n",
    "\n",
    "        # Aggregate Pareto Points\n",
    "        all_pareto_tp = []\n",
    "        all_pareto_jain = []\n",
    "        for run in runs:\n",
    "            pareto_tp, pareto_jain = zip(*run['pareto_points'])\n",
    "            all_pareto_tp.append(pareto_tp)\n",
    "            all_pareto_jain.append(pareto_jain)\n",
    "\n",
    "        pareto_tp_mean = np.mean(all_pareto_tp, axis=0)\n",
    "        pareto_jain_mean = np.mean(all_pareto_jain, axis=0)\n",
    "\n",
    "        mean_results[reward_mode] = {\n",
    "            'param_values': param_vals,\n",
    "            'edr_results': edr_means,\n",
    "            'jain_results': jain_means,\n",
    "            'pareto_points': list(zip(pareto_tp_mean, pareto_jain_mean))\n",
    "        }\n",
    "\n",
    "    # --- Plot the MEANS nicely ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for reward_mode, res in mean_results.items():\n",
    "        for g in goal_edges:\n",
    "            plt.plot(res['param_values'], res['edr_results'][g], marker='o', label=f\"{reward_mode} - Goal {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for reward_mode, res in mean_results.items():\n",
    "        plt.plot(res['param_values'], res['jain_results'], marker='s', label=f\"{reward_mode}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for reward_mode, res in mean_results.items():\n",
    "        pareto_tp, pareto_jain = zip(*res['pareto_points'])\n",
    "        plt.plot(pareto_tp, pareto_jain, marker='o', label=f\"{reward_mode}\")\n",
    "    plt.xlabel(\"Throughput (Sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(0, None)\n",
    "    plt.ylim(0.45, 1.05) \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_results, mean_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eda908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverAlpha(\n",
    "    alpha_values,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    edr_results = {g: [] for g in goal_edges}\n",
    "    jain_results = []\n",
    "    pareto_points = []\n",
    "    full_raw_data = {}\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        print(f\"\\n=== Training with alpha = {alpha} ===\")\n",
    "\n",
    "        # --- Build kwargs dynamically ---\n",
    "        train_kwargs = {\n",
    "            'edges': edges,\n",
    "            'goal_edges': goal_edges,\n",
    "            'pSwap': pSwap,\n",
    "            'pGen': pGen,\n",
    "            'max_age': max_age,\n",
    "            'seed': seed,\n",
    "            'totalSteps': totalSteps,\n",
    "            'alpha': alpha,   # <<< FIXED HERE\n",
    "            'gamma': gamma,\n",
    "            'edr_window_size': edr_window_size,\n",
    "            'reward_mode': reward_mode,\n",
    "            'nestedSwaps': nestedSwaps,\n",
    "        }\n",
    "\n",
    "        if training_function.__name__ == 'train_sarsa_linear_policy':\n",
    "            train_kwargs['nLookahead'] = nLookahead\n",
    "            train_kwargs['temperature'] = temperature\n",
    "            train_kwargs['temperature_decay'] = temperature_decay\n",
    "            train_kwargs['log_interval'] = edr_window_size\n",
    "        elif training_function.__name__ == 'train_q_learning_linear_policy':\n",
    "            train_kwargs['temperature'] = temperature\n",
    "            train_kwargs['temperature_decay'] = temperature_decay\n",
    "\n",
    "        train_kwargs.update(extra_kwargs)\n",
    "\n",
    "        Q = training_function(**train_kwargs)\n",
    "\n",
    "        sim_result = simulate_policy(\n",
    "            Q_table=Q,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=max_age,\n",
    "            num_steps=simulate_steps,\n",
    "            edr_window_size=edr_window_size,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        burn_in_steps = int(simulate_steps * burn_in_ratio)\n",
    "        final_edrs = {g: np.mean(sim_result[\"edr_history\"][g][burn_in_steps:]) for g in goal_edges}\n",
    "        final_tp = sum(final_edrs.values())\n",
    "        final_jain = jains_index(final_edrs)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_results[g].append(final_edrs[g])\n",
    "        jain_results.append(final_jain)\n",
    "        pareto_points.append((final_tp, final_jain))\n",
    "        full_raw_data[alpha] = sim_result\n",
    "\n",
    "\n",
    "    # --- Plotting results ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(alpha_values, edr_results[g], marker='o', label=f\"EDR {g}\")\n",
    "    plt.xlabel(\"Alpha (Learning Rate)\")\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.title(f\"EDR vs Alpha\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(alpha_values, jain_results, marker='s', label=\"Jain's Fairness\", color='purple')\n",
    "    plt.xlabel(\"Alpha (Learning Rate)\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(f\"Jain's Fairness vs Alpha\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    pareto_tp, pareto_jain = zip(*pareto_points)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pareto_tp, pareto_jain, c='crimson', s=80)\n",
    "    for (tp, jain), val in zip(pareto_points, alpha_values):\n",
    "        plt.text(tp, jain, f\"{val:.4f}\", fontsize=9)\n",
    "    plt.xlabel(\"Throughput (sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.title(f\"Pareto Plot for different Alpha values\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, max(pareto_tp) * 1.1)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"param_name\": \"alpha\",\n",
    "        \"param_values\": alpha_values,\n",
    "        \"edr_results\": edr_results,\n",
    "        \"jain_results\": jain_results,\n",
    "        \"pareto_points\": pareto_points,\n",
    "        \"full_raw_data\": full_raw_data\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def plot_training_results(\n",
    "    q_diffs, q_diffs_per_goal, edr_steps, edr_hist,\n",
    "    goal_edges, fairness_history, edge_creation_counter=None,\n",
    "    method_name=\"\",\n",
    "    smoothing_window=1000\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # --- Plot Global Q-value Convergence (no smoothing) ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(q_diffs, color='royalblue', alpha=0.8)\n",
    "    plt.xlabel(f\"{method_name} Updates\")\n",
    "    plt.ylabel(\"Global Q-value Difference\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Define fixed colors ---\n",
    "    color_goal_1 = 'blue'\n",
    "    color_goal_2 = 'orange'\n",
    "    color_noop = 'gray'\n",
    "\n",
    "    # --- Plot Per-goal Q-value Convergence + No-Op Together (smoothed) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    goal_colors = [color_goal_1, color_goal_2]\n",
    "    linestyles = ['-', '--', ':', '-.']\n",
    "\n",
    "    for i, g in enumerate(goal_edges):\n",
    "        diffs = np.array(q_diffs_per_goal[g])\n",
    "        steps = np.arange(len(diffs))\n",
    "        mask = ~np.isnan(diffs)\n",
    "\n",
    "        if np.sum(mask) > smoothing_window:\n",
    "            smooth_diffs = moving_average(diffs[mask], smoothing_window)\n",
    "            smooth_steps = steps[mask][:len(smooth_diffs)]\n",
    "            plt.plot(\n",
    "                smooth_steps, smooth_diffs,\n",
    "                label=f\"Goal {g}\",\n",
    "                linestyle=linestyles[i % len(linestyles)],\n",
    "                color=goal_colors[i % len(goal_colors)],\n",
    "                alpha=0.9,\n",
    "                linewidth=2\n",
    "            )\n",
    "\n",
    "    # Plot No-op line if available\n",
    "    if 'noop' in q_diffs_per_goal:\n",
    "        diffs = np.array(q_diffs_per_goal['noop'])\n",
    "        steps = np.arange(len(diffs))\n",
    "        mask = ~np.isnan(diffs)\n",
    "\n",
    "        if np.sum(mask) > smoothing_window:\n",
    "            smooth_diffs = moving_average(diffs[mask], smoothing_window)\n",
    "            smooth_steps = steps[mask][:len(smooth_diffs)]\n",
    "            plt.plot(\n",
    "                smooth_steps, smooth_diffs,\n",
    "                label=\"No-op Action\",\n",
    "                linestyle=':',\n",
    "                color=color_noop,\n",
    "                alpha=0.7,\n",
    "                linewidth=2\n",
    "            )\n",
    "\n",
    "    plt.xlabel(f\"{method_name} Updates\")\n",
    "    plt.ylabel(\"Per-goal Q-value Difference (smoothed)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot EDR Evolution + Jain's Fairness ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for i, g in enumerate(goal_edges):\n",
    "        plt.plot(edr_steps, edr_hist[g], label=f\"EDR {g}\", color=goal_colors[i % len(goal_colors)], linewidth=2)\n",
    "\n",
    "    plt.plot(edr_steps, fairness_history, label=\"Jain's Fairness\", linestyle=\"--\", color='purple', linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"EDR Estimate / Jain's Fairness\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Swapped Edge Creation Frequency ---\n",
    "    if edge_creation_counter is not None:\n",
    "        edges = list(edge_creation_counter.keys())\n",
    "        counts = list(edge_creation_counter.values())\n",
    "        edge_labels = [f\"{u}-{v}\" for u, v in edges]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(edge_labels, counts)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"Number of Swapped Creations\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.title(f\"{method_name}: Edge Creation Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d12b6",
   "metadata": {},
   "source": [
    "#  **Q-LEARNING CODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f82aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_q_learning_linear_policy(\n",
    "    initialEdges, goalEdges, totalSteps,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\", rewardAlpha=1,\n",
    "    noop_penalty=0.00, log_interval=1000,\n",
    "    softmax=True, temperature=1.0, temperature_decay=0.9999,\n",
    "    epsilon=0.01, nestedSwaps=False\n",
    "):\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "    q_value_diffs_per_goal['noop'] = []  # <-- NEW\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    # --- Build Master Edge List ---\n",
    "    nodes = set()\n",
    "    for u, v in initialEdges:\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    nodes = sorted(list(nodes))\n",
    "\n",
    "    master_edge_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "    feature_size = len(master_edge_list) + len(goalEdges)\n",
    "    Q = LinearQApproximator(feature_size=feature_size)\n",
    "    temperature_curr = temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    fairness_history = []\n",
    "    edge_creation_counter = Counter()\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "    def select_action(state, temperature):\n",
    "        feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        if softmax:\n",
    "            q_vals = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "            scaled_qs = q_vals / max(temperature, 1e-6)\n",
    "            exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "            probs = exp_qs / np.sum(exp_qs)\n",
    "            chosen = acts[np.random.choice(len(acts), p=probs)]\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                chosen = random.choice(acts)\n",
    "            else:\n",
    "                chosen = max(acts, key=lambda a: Q.get_q_value(feats, a))\n",
    "\n",
    "        return chosen\n",
    "\n",
    "    state = current\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        temperature_curr = max(0.01, temperature_curr * temperature_decay)\n",
    "\n",
    "        action = select_action(state, temperature_curr)\n",
    "        next_state = performAction(action, state, pSwap=pSwap, nestedSwaps=nestedSwaps)\n",
    "\n",
    "        # Track newly created swapped edges (not initial ones)\n",
    "        new_edges = next_state[0]\n",
    "        for (u, v), age in new_edges:\n",
    "            if (u, v) not in initialEdges and (v, u) not in initialEdges:\n",
    "                edge_creation_counter[(u, v)] += 1\n",
    "\n",
    "        next_state = ageEntanglements(next_state, maxAge)\n",
    "        next_state = generateEntanglement(next_state, pGen, initialEdges)\n",
    "\n",
    "        r, succ = compute_reward(\n",
    "            action, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, noop_penalty=noop_penalty, alphaReward=rewardAlpha\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = action\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        augmented_next_state = (next_state[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "            fairness = jains_index(edr_snap)                \n",
    "            fairness_history.append(fairness)\n",
    "\n",
    "        feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "        feats_next = featurize_state(augmented_next_state, goalEdges, master_edge_list)\n",
    "        next_actions = get_possible_multi_actions(augmented_next_state[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "        if ([], None) not in next_actions:\n",
    "            next_actions.append(([], None))\n",
    "        max_q_next = max([Q.get_q_value(feats_next, a) for a in next_actions], default=0.0)\n",
    "\n",
    "        target = r + gamma * max_q_next\n",
    "        current_q = Q.get_q_value(feats, action)\n",
    "        diff = abs(current_q - target)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        if action[1] is None:\n",
    "            q_value_diffs_per_goal['noop'].append(diff)\n",
    "            for g in goalEdges:\n",
    "                q_value_diffs_per_goal[g].append(np.nan)\n",
    "        else:\n",
    "            for g in goalEdges:\n",
    "                if g in action[1]:\n",
    "                    q_value_diffs_per_goal[g].append(diff)\n",
    "                else:\n",
    "                    q_value_diffs_per_goal[g].append(np.nan)\n",
    "\n",
    "        Q.update(feats, action, target, alpha)\n",
    "        state = augmented_next_state\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history, fairness_history, edge_creation_counter\n",
    "\n",
    "\n",
    "def train_q_learning_linear_policy(\n",
    "    edges, goal_edges, pSwap, pGen, max_age,\n",
    "    seed, totalSteps, alpha, gamma,\n",
    "    edr_window_size, reward_mode, \n",
    "    softmax, temperature, temperature_decay,\n",
    "    nestedSwaps, noop_penalty=0.0, plotTraining=False, rewardAlpha=1\n",
    "\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    log_interval = edr_window_size\n",
    "\n",
    "    result = run_q_learning_linear_policy(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=pGen,\n",
    "        pSwap=pSwap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        softmax=softmax,\n",
    "        temperature=temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        nestedSwaps=nestedSwaps,\n",
    "        rewardAlpha=rewardAlpha\n",
    "    )\n",
    "\n",
    "    if result is None:\n",
    "        print(\"Error: Q-learning returned None.\")\n",
    "        return\n",
    "\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist, fairness_history, edge_creation_counter = result\n",
    "\n",
    "\n",
    "    if plotTraining:\n",
    "        plot_training_results(\n",
    "        q_diffs,\n",
    "        q_diffs_per_goal,\n",
    "        edr_steps,\n",
    "        edr_hist,\n",
    "        goal_edges,\n",
    "        fairness_history,      # <<< ADD THIS\n",
    "        edge_creation_counter,\n",
    "        method_name=\"Q-Learning\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7e7f7",
   "metadata": {},
   "source": [
    "#  **SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c42baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ssa')\n",
    "def run_n_step_sarsa_linear_multi(\n",
    "    initialEdges, goalEdges, totalSteps, nLookahead,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\", rewardAlpha=1,\n",
    "    noop_penalty=0.00, log_interval=1000,\n",
    "    temperature=1.0, temperature_decay=0.9999, nestedSwaps=False, Q_Existing=None\n",
    "):\n",
    "    # --- Build Master Edge List ---\n",
    "    nodes = set()\n",
    "    for u, v in initialEdges:\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    nodes = sorted(list(nodes))\n",
    "\n",
    "    master_edge_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "    feature_size = len(master_edge_list) + len(goalEdges)\n",
    "    if Q_Existing is None:\n",
    "        Q = LinearQApproximator(feature_size=feature_size)\n",
    "    else:\n",
    "        Q = Q_Existing\n",
    "\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "    q_value_diffs_per_goal['noop'] = []  # Add No-op as a \"goal\" for tracking\n",
    "\n",
    "    edge_creation_counter = Counter()\n",
    "\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    state_buffer = deque([current])\n",
    "    reward_buffer = deque()\n",
    "    temperature = temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    fairness_history = []\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "\n",
    "    # --- Updated select_action ---\n",
    "    # def select_action(state, temperature):\n",
    "    #     feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "    #     acts = get_possible_multi_actions(state[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "\n",
    "    #     if ([], None) not in acts:\n",
    "    #         acts.append(([], None))\n",
    "\n",
    "    #     q_values = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "    #     scaled_qs = q_values / max(temperature, 1e-6)\n",
    "    #     exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "    #     probs = exp_qs / np.sum(exp_qs)\n",
    "\n",
    "    #     idx = np.random.choice(len(acts), p=probs)\n",
    "    #     chosen = acts[idx]\n",
    "        \n",
    "    #     return chosen\n",
    "    \n",
    "    def select_action(state, temperature):\n",
    "        softmax=True\n",
    "        feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        if softmax:\n",
    "            q_vals = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "            \n",
    "            if np.any(np.isnan(q_vals)) or np.any(np.isinf(q_vals)):\n",
    "                # fallback if Q-values are broken\n",
    "                probs = np.ones(len(acts)) / len(acts)\n",
    "            else:\n",
    "                scaled_qs = q_vals / max(temperature, 1e-6)\n",
    "                exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "                sum_exp = np.sum(exp_qs)\n",
    "\n",
    "                if sum_exp < 1e-8 or np.isnan(sum_exp):\n",
    "                    # fallback if exp_qs are messed up\n",
    "                    probs = np.ones(len(acts)) / len(acts)\n",
    "                else:\n",
    "                    probs = exp_qs / sum_exp\n",
    "            \n",
    "            chosen = acts[np.random.choice(len(acts), p=probs)]\n",
    "        else:\n",
    "            print('no epsilon')\n",
    "            epsilon = 0.001 #??\n",
    "            if random.random() < epsilon:\n",
    "                chosen = random.choice(acts)\n",
    "            else:\n",
    "                chosen = max(acts, key=lambda a: Q.get_q_value(feats, a))\n",
    "\n",
    "        return chosen\n",
    "\n",
    "\n",
    "    # --- Start Training Loop ---\n",
    "    action_buffer = deque([select_action(current, temperature)])\n",
    "    initial_alpha = alpha  # Save initial value\n",
    "    final_decay_factor = 0.05\n",
    "    \n",
    "    for t in range(totalSteps):\n",
    "        alpha = initial_alpha * (final_decay_factor ** (t / totalSteps))\n",
    "        alpha = max(alpha, 1e-5)  # Optional floor\n",
    "        if (t+1) % 100_000 == 0:\n",
    "            print(f\"Step {t+1}\")\n",
    "        if (t+1) % 1_000_000 == 0:\n",
    "            print(f\"Step {t+1}: alpha = {alpha:.6f}\")\n",
    "\n",
    "        temperature = max(0.01, temperature * temperature_decay)\n",
    "        S_t = state_buffer[-1]\n",
    "        A_t = action_buffer[-1]\n",
    "        \n",
    "        # --- Debug print when exactly 3 actions are available ---\n",
    "        \n",
    "        acts = get_possible_multi_actions(S_t[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        ns = performAction(A_t, S_t, pSwap=pSwap, nestedSwaps=nestedSwaps, system_goals=goalEdges)\n",
    "        # Track newly created swapped edges (not original edges)\n",
    "        new_edges = ns[0]  # list of (edge, age)\n",
    "        for (u, v), age in new_edges:\n",
    "            if (u, v) not in initialEdges and (v, u) not in initialEdges:\n",
    "                edge_creation_counter[(u, v)] += 1\n",
    "\n",
    "\n",
    "\n",
    "        ns = ageEntanglements(ns, maxAge)\n",
    "        ns = generateEntanglement(ns, pGen, initial_edges=initialEdges)\n",
    "        r, succ = compute_reward(\n",
    "            A_t, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, noop_penalty=noop_penalty, alphaReward=rewardAlpha\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = A_t\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        \n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        reward_buffer.append(r)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        next_state = (ns[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "                # Compute Jain's Index at each log point\n",
    "            fairness = jains_index(edr_snap)\n",
    "            fairness_history.append(fairness)\n",
    "\n",
    "\n",
    "        A_next = select_action(next_state, temperature)\n",
    "\n",
    "        state_buffer.append(next_state)\n",
    "        action_buffer.append(A_next)\n",
    "\n",
    "        if len(reward_buffer) >= nLookahead:\n",
    "            G = sum((gamma**i) * reward_buffer[i] for i in range(nLookahead))\n",
    "            s_n = state_buffer[nLookahead]\n",
    "            a_n = action_buffer[nLookahead]\n",
    "            feats_n = featurize_state(s_n, goalEdges, master_edge_list)\n",
    "            G += (gamma**nLookahead) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "            s_tau = state_buffer[0]\n",
    "            a_tau = action_buffer[0]\n",
    "            feats_tau = featurize_state(s_tau, goalEdges, master_edge_list)\n",
    "            old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "            diff = abs(G - old_q)\n",
    "            q_value_diffs.append(diff)\n",
    "            \n",
    "            \n",
    "            for gg in goalEdges:\n",
    "                if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                    q_value_diffs_per_goal[gg].append(diff)\n",
    "                else:\n",
    "                    q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "            # Special handling for no-op\n",
    "            if a_tau[1] is None:  # If it's no-op action\n",
    "                q_value_diffs_per_goal['noop'].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal['noop'].append(float('nan'))\n",
    "\n",
    "\n",
    "            Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "            state_buffer.popleft()\n",
    "            action_buffer.popleft()\n",
    "            reward_buffer.popleft()\n",
    "\n",
    "    while reward_buffer:\n",
    "        n = len(reward_buffer)\n",
    "        G = sum((gamma**i) * reward_buffer[i] for i in range(n))\n",
    "        if n < len(state_buffer):\n",
    "            s_n = state_buffer[n]\n",
    "            a_n = action_buffer[n]\n",
    "            feats_n = featurize_state(s_n, goalEdges, master_edge_list)\n",
    "            G += (gamma**n) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "        s_tau = state_buffer[0]\n",
    "        a_tau = action_buffer[0]\n",
    "        feats_tau = featurize_state(s_tau, goalEdges, master_edge_list)\n",
    "        old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "        diff = abs(G - old_q)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        for gg in goalEdges:\n",
    "            if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                q_value_diffs_per_goal[gg].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "        Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "        state_buffer.popleft()\n",
    "        action_buffer.popleft()\n",
    "        reward_buffer.popleft()\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history, fairness_history, edge_creation_counter\n",
    "\n",
    "\n",
    "def train_sarsa_linear_policy(\n",
    "    edges, goal_edges, pSwap, pGen, max_age,\n",
    "    seed, totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    log_interval,\n",
    "    temperature, temperature_decay, nestedSwaps,\n",
    "    noop_penalty=0.0, plotTraining=False, Q_Existing=None, rewardAlpha=1, softmax=None\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    log_interval = edr_window_size\n",
    "\n",
    "    result = run_n_step_sarsa_linear_multi(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        nLookahead=nLookahead,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=pGen,\n",
    "        pSwap=pSwap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        temperature=temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        nestedSwaps=nestedSwaps,\n",
    "        Q_Existing=Q_Existing,\n",
    "        rewardAlpha=rewardAlpha,\n",
    "    )\n",
    "\n",
    "    if result is None:\n",
    "        print(\"Error: run_n_step_sarsa_linear_multi returned None.\")\n",
    "        return\n",
    "\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist, fairness_history, edge_creation_counter = result\n",
    "\n",
    "    if plotTraining:\n",
    "        plot_training_results(\n",
    "            q_diffs, q_diffs_per_goal, edr_steps, edr_hist,\n",
    "            goal_edges, fairness_history, edge_creation_counter,\n",
    "            method_name=\"SARSA\"\n",
    "        )\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverParamAndModel(\n",
    "    param_name, param_values, models,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    reward_mode=\"basic\",\n",
    "    **extra_kwargs\n",
    "):\n",
    "    all_results = {}\n",
    "\n",
    "    # --- Training kwargs (only what training functions expect) ---\n",
    "    training_keys = [\n",
    "        'edges', 'goal_edges', 'pSwap', 'pGen', 'max_age', 'seed', 'totalSteps', 'nLookahead',\n",
    "        'alpha', 'gamma', 'edr_window_size', 'reward_mode',\n",
    "        'temperature', 'temperature_decay', 'nestedSwaps', 'softmax', 'log_interval'\n",
    "    ]\n",
    "\n",
    "    for model_name, train_func in models.items():\n",
    "        print(f\"\\n=== Running for model: {model_name} ===\")\n",
    "\n",
    "        results = {\n",
    "            'param_values': [],\n",
    "            'edr_results': {g: [] for g in goal_edges},\n",
    "            'jain_results': [],\n",
    "            'pareto_points': []\n",
    "        }\n",
    "\n",
    "        for param_val in param_values:\n",
    "            print(f\"  --- Param {param_name} = {param_val} ---\")\n",
    "                        \n",
    "            # Assemble kwargs for training\n",
    "            kwargs = {\n",
    "                'edges': edges,\n",
    "                'goal_edges': goal_edges,\n",
    "                'pSwap': pSwap,\n",
    "                'pGen': pGen,\n",
    "                'max_age': max_age,\n",
    "                'seed': seed,\n",
    "                'totalSteps': totalSteps,\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'edr_window_size': edr_window_size,\n",
    "                'reward_mode': reward_mode,\n",
    "                'nestedSwaps': nestedSwaps,\n",
    "                'temperature': temperature,\n",
    "                'temperature_decay': temperature_decay,\n",
    "            }\n",
    "            kwargs.update(extra_kwargs)\n",
    "\n",
    "            # Add model-specific kwargs\n",
    "            if 'sarsa' in model_name.lower():\n",
    "                kwargs['nLookahead'] = nLookahead\n",
    "                kwargs['log_interval'] = edr_window_size\n",
    "            else:\n",
    "                # Q-learning doesn't need nLookahead or log_interval\n",
    "                kwargs.pop('nLookahead', None)\n",
    "                kwargs.pop('log_interval', None)\n",
    "\n",
    "\n",
    "            # Overwrite pSwap or pGen for the sweep\n",
    "            if param_name == 'pSwap':\n",
    "                kwargs['pSwap'] = param_val\n",
    "            elif param_name == 'pGen':\n",
    "                kwargs['pGen'] = param_val\n",
    "\n",
    "            # Filter only training-related kwargs\n",
    "            train_kwargs = {k: v for k, v in kwargs.items() if k in training_keys}\n",
    "\n",
    "            # Train the model\n",
    "            model = train_func(**train_kwargs)\n",
    "\n",
    "\n",
    "            # Simulate the trained model\n",
    "            sim_results = simulate_policy(\n",
    "                Q_table=model, \n",
    "                edges=edges, \n",
    "                goal_edges=goal_edges,\n",
    "                pSwap=kwargs['pSwap'],\n",
    "                pGen=kwargs['pGen'],\n",
    "                max_age=max_age,\n",
    "                num_steps=simulate_steps,\n",
    "                nestedSwaps=nestedSwaps,\n",
    "                plot=False  # no per-run plots while sweeping\n",
    "            )\n",
    "\n",
    "            # Aggregate simulation results\n",
    "            burn_in_idx = 5000  # hardcoded burn-in same as simulate_policy\n",
    "            final_edrs = {g: np.mean(sim_results['edr_history'][g][burn_in_idx:]) for g in goal_edges}\n",
    "            final_tp = sum(final_edrs.values())\n",
    "            final_jain = np.mean(sim_results['jain_history'][burn_in_idx:])\n",
    "\n",
    "            # Save results\n",
    "            results['param_values'].append(param_val)\n",
    "            for g in goal_edges:\n",
    "                results['edr_results'][g].append(final_edrs[g])\n",
    "            results['jain_results'].append(final_jain)\n",
    "            results['pareto_points'].append((final_tp, final_jain))\n",
    "\n",
    "        all_results[model_name] = results\n",
    "\n",
    "    # --- Plot Mean EDR\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for model_name, res in all_results.items():\n",
    "        for g in goal_edges:\n",
    "            plt.plot(res['param_values'], res['edr_results'][g], marker='o', label=f\"{model_name} - Goal {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.title(\"Effect of Model Choice on EDR\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Jain's Fairness Index\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for model_name, res in all_results.items():\n",
    "        plt.plot(res['param_values'], res['jain_results'], marker='s', label=f\"{model_name}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(\"Effect of Model Choice on Fairness\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Pareto Frontier (Throughput vs Fairness)\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for model_name, res in all_results.items():\n",
    "        pareto_tp, pareto_jain = zip(*res['pareto_points'])\n",
    "        plt.plot(pareto_tp, pareto_jain, marker='o', label=f\"{model_name}\")\n",
    "    plt.xlabel(\"Throughput (Sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(\"Throughput vs Fairness Trade-off\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(0, None)\n",
    "    plt.ylim(0.45, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4debf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def run_sarsa_qlearning_over_maxage(\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen,\n",
    "    seed,\n",
    "    totalSteps,\n",
    "    alpha, gamma,\n",
    "    windowSize,\n",
    "    reward_mode,\n",
    "    temperature, temperature_decay,\n",
    "    nestedSwaps,\n",
    "    maxAge_values,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_steps=5000\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Prepare arrays to record results\n",
    "    sarsa_jain_scores = []\n",
    "    sarsa_throughput_scores = []\n",
    "    qlearn_jain_scores = []\n",
    "    qlearn_throughput_scores = []\n",
    "\n",
    "    for maxAge_val in maxAge_values:\n",
    "        print(f\"\\n=== Training for maxAge={maxAge_val} ===\")\n",
    "\n",
    "        # --- SARSA ---\n",
    "        Q_sarsa = train_sarsa_linear_policy(\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=maxAge_val,\n",
    "            seed=seed,\n",
    "            totalSteps=totalSteps,\n",
    "            nLookahead=5,  # Fixed for SARSA\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            edr_window_size=windowSize,\n",
    "            reward_mode=reward_mode,\n",
    "            log_interval=windowSize,\n",
    "            temperature=temperature,\n",
    "            temperature_decay=temperature_decay,\n",
    "            nestedSwaps=nestedSwaps,\n",
    "            noop_penalty=0.0,\n",
    "            plotTraining=False,\n",
    "            rewardAlpha=0.5\n",
    "        )\n",
    "\n",
    "        sim_sarsa = simulate_policy(\n",
    "            Q_table=Q_sarsa,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=maxAge_val,\n",
    "            num_steps=simulate_steps,\n",
    "            edr_window_size=windowSize,\n",
    "            burn_in=burn_in_steps,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        final_jain_sarsa = np.mean(sim_sarsa['jain_history'][burn_in_steps:])\n",
    "        final_tp_sarsa = np.mean(sim_sarsa['throughput_history'][burn_in_steps:])\n",
    "        sarsa_jain_scores.append(final_jain_sarsa)\n",
    "        sarsa_throughput_scores.append(final_tp_sarsa)\n",
    "\n",
    "        # --- Q-Learning ---\n",
    "        Q_qlearn = train_q_learning_linear_policy(\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=maxAge_val,\n",
    "            seed=seed,\n",
    "            totalSteps=totalSteps,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            edr_window_size=windowSize,\n",
    "            reward_mode=reward_mode,\n",
    "            temperature=temperature,\n",
    "            temperature_decay=temperature_decay,\n",
    "            nestedSwaps=nestedSwaps,\n",
    "            noop_penalty=0.0,\n",
    "            plotTraining=False,\n",
    "            rewardAlpha=0.5\n",
    "        )\n",
    "\n",
    "        sim_qlearn = simulate_policy(\n",
    "            Q_table=Q_qlearn,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=maxAge_val,\n",
    "            num_steps=simulate_steps,\n",
    "            edr_window_size=windowSize,\n",
    "            burn_in=burn_in_steps,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        final_jain_qlearn = np.mean(sim_qlearn['jain_history'][burn_in_steps:])\n",
    "        final_tp_qlearn = np.mean(sim_qlearn['throughput_history'][burn_in_steps:])\n",
    "        qlearn_jain_scores.append(final_jain_qlearn)\n",
    "        qlearn_throughput_scores.append(final_tp_qlearn)\n",
    "\n",
    "    # --- Plot results ---\n",
    "\n",
    "    # (1) Jain's Index vs maxAge\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(maxAge_values, sarsa_jain_scores, marker='o', linestyle='-', label=\"SARSA\")\n",
    "    plt.plot(maxAge_values, qlearn_jain_scores, marker='s', linestyle='--', label=\"Q-Learning\")\n",
    "    plt.xlabel(\"maxAge\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(\"Jain's Fairness vs maxAge (SARSA vs Q-Learning)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # (2) Throughput vs maxAge\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(maxAge_values, sarsa_throughput_scores, marker='o', linestyle='-', label=\"SARSA\")\n",
    "    plt.plot(maxAge_values, qlearn_throughput_scores, marker='s', linestyle='--', label=\"Q-Learning\")\n",
    "    plt.xlabel(\"maxAge\")\n",
    "    plt.ylabel(\"Throughput (Sum of EDRs)\")\n",
    "    plt.title(\"Throughput vs maxAge (SARSA vs Q-Learning)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Return all results ---\n",
    "    return {\n",
    "        \"maxAge_values\": maxAge_values,\n",
    "        \"sarsa_jain\": sarsa_jain_scores,\n",
    "        \"sarsa_throughput\": sarsa_throughput_scores,\n",
    "        \"qlearn_jain\": qlearn_jain_scores,\n",
    "        \"qlearn_throughput\": qlearn_throughput_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d68a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareNestedVsNonNested(\n",
    "    param_name, param_values,\n",
    "    sarsa_func,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    simulate_steps=50_000,\n",
    "    reward_mode=\"basic\",\n",
    "    **extra_kwargs\n",
    "):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    all_results = {'nested': {}, 'non_nested': {}}\n",
    "\n",
    "    training_keys = [\n",
    "        'edges', 'goal_edges', 'pSwap', 'pGen', 'max_age', 'seed', 'totalSteps', 'nLookahead',\n",
    "        'alpha', 'gamma', 'edr_window_size', 'reward_mode',\n",
    "        'temperature', 'temperature_decay', 'nestedSwaps', 'softmax', 'log_interval'\n",
    "    ]\n",
    "\n",
    "    for nested_flag in [True, False]:\n",
    "        mode = 'nested' if nested_flag else 'non_nested'\n",
    "        print(f\"\\n=== Running for {'Nested Swaps' if nested_flag else 'Non-Nested Swaps'} ===\")\n",
    "\n",
    "        results = {\n",
    "            'param_values': [],\n",
    "            'throughputs': [],\n",
    "            'jain_results': [],\n",
    "            'wait_rates': [],\n",
    "            'pareto_points': []\n",
    "        }\n",
    "\n",
    "        for param_val in param_values:\n",
    "            print(f\"  --- Param {param_name} = {param_val} ---\")\n",
    "\n",
    "            kwargs = {\n",
    "                'edges': edges,\n",
    "                'goal_edges': goal_edges,\n",
    "                'pSwap': pSwap,\n",
    "                'pGen': pGen,\n",
    "                'max_age': max_age,\n",
    "                'seed': seed,\n",
    "                'totalSteps': totalSteps,\n",
    "                'nLookahead': nLookahead,\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'edr_window_size': edr_window_size,\n",
    "                'reward_mode': reward_mode,\n",
    "                'temperature': temperature,\n",
    "                'temperature_decay': temperature_decay,\n",
    "                'nestedSwaps': nested_flag,\n",
    "                'log_interval': edr_window_size,\n",
    "            }\n",
    "            kwargs.update(extra_kwargs)\n",
    "\n",
    "            if param_name == 'pSwap':\n",
    "                kwargs['pSwap'] = param_val\n",
    "            elif param_name == 'pGen':\n",
    "                kwargs['pGen'] = param_val\n",
    "\n",
    "            train_kwargs = {k: v for k, v in kwargs.items() if k in training_keys}\n",
    "\n",
    "            model = sarsa_func(**train_kwargs)\n",
    "\n",
    "            sim_results = simulate_policy(\n",
    "                Q_table=model,\n",
    "                edges=edges,\n",
    "                goal_edges=goal_edges,\n",
    "                pSwap=kwargs['pSwap'],\n",
    "                pGen=kwargs['pGen'],\n",
    "                max_age=max_age,\n",
    "                num_steps=simulate_steps,\n",
    "                nestedSwaps=nested_flag,\n",
    "                plot=False\n",
    "            )\n",
    "\n",
    "            burn_in_idx = 5000\n",
    "            final_edrs = {g: np.mean(sim_results['edr_history'][g][burn_in_idx:]) for g in goal_edges}\n",
    "            final_tp = sum(final_edrs.values())\n",
    "            final_jain = np.mean(sim_results['jain_history'][burn_in_idx:])\n",
    "            wait_rate = sim_results['wait_ratio']\n",
    "\n",
    "            results['param_values'].append(param_val)\n",
    "            results['throughputs'].append(final_tp)\n",
    "            results['jain_results'].append(final_jain)\n",
    "            results['wait_rates'].append(wait_rate)\n",
    "            results['pareto_points'].append((final_tp, final_jain))\n",
    "\n",
    "        all_results[mode] = results\n",
    "\n",
    "    # === Now the simple plots ===\n",
    "\n",
    "    param_values = np.array(param_values)\n",
    "    param_labels = {\n",
    "    'pSwap': r'Probability of Successful Swapping ($p_\\mathrm{swap}$)',\n",
    "    'pGen': r'Probability of Link Generation ($p_\\mathrm{gen}$)',\n",
    "    }\n",
    "    param_label = param_labels.get(param_name, param_name)\n",
    "\n",
    "\n",
    "    # Plot 1: Throughput vs param\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(param_values, all_results['nested']['throughputs'], marker='o', label='Nested Swaps')\n",
    "    plt.plot(param_values, all_results['non_nested']['throughputs'], marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(param_label)\n",
    "    plt.ylabel(\"Total Throughput\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Jain's Fairness Index vs param\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(param_values, all_results['nested']['jain_results'], marker='o', label='Nested Swaps')\n",
    "    plt.plot(param_values, all_results['non_nested']['jain_results'], marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(param_label)\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Pareto Frontier (Throughput vs Jain)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    nested_tp, nested_jain = zip(*all_results['nested']['pareto_points'])\n",
    "    non_nested_tp, non_nested_jain = zip(*all_results['non_nested']['pareto_points'])\n",
    "    plt.plot(nested_tp, nested_jain, marker='o', label='Nested Swaps')\n",
    "    plt.plot(non_nested_tp, non_nested_jain, marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(\"Total Throughput\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(0, None)\n",
    "    plt.ylim(0.45, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 4: Wait Rate vs param\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(param_values, all_results['nested']['wait_rates'], marker='o', label='Nested Swaps')\n",
    "    plt.plot(param_values, all_results['non_nested']['wait_rates'], marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(param_label)\n",
    "    plt.ylabel(\"Wait Rate\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6184d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareTrainingMethods(\n",
    "    param_name, param_values,\n",
    "    training_func,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps,\n",
    "    alpha, gamma,\n",
    "    edr_window_size,\n",
    "    temperature, temperature_decay,\n",
    "    seed,\n",
    "    simulate_steps=50_000,\n",
    "    reward_mode=\"basic\",\n",
    "    nLookahead=None,\n",
    "    method_type=\"sarsa\",  # \"sarsa\", \"qlearning\", or \"greedy\"\n",
    "    **extra_kwargs\n",
    "):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    all_results = {'nested': {}, 'non_nested': {}}\n",
    "\n",
    "    # Define training keys based on method\n",
    "    if method_type == \"sarsa\":\n",
    "        training_keys = [\n",
    "            'edges', 'goal_edges', 'pSwap', 'pGen', 'max_age', 'seed', 'totalSteps', 'nLookahead',\n",
    "            'alpha', 'gamma', 'edr_window_size', 'reward_mode',\n",
    "            'temperature', 'temperature_decay', 'nestedSwaps', 'softmax', 'log_interval'\n",
    "        ]\n",
    "    elif method_type == \"qlearning\":\n",
    "        training_keys = [\n",
    "            'edges', 'goal_edges', 'pSwap', 'pGen', 'max_age', 'seed', 'totalSteps',\n",
    "            'alpha', 'gamma', 'edr_window_size', 'reward_mode',\n",
    "            'temperature', 'temperature_decay', 'nestedSwaps', 'softmax', 'log_interval'\n",
    "        ]\n",
    "    elif method_type == \"greedy\":\n",
    "        training_keys = []  # Greedy doesn't \"train\" a model\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method_type: {method_type}\")\n",
    "\n",
    "    # If Greedy, no nested swaps, no training\n",
    "    nested_flags = [False] if method_type == \"greedy\" else [True, False]\n",
    "\n",
    "    for nested_flag in nested_flags:\n",
    "        mode = 'nested' if nested_flag else 'non_nested'\n",
    "        print(f\"\\n=== Running for {'Nested Swaps' if nested_flag else 'Non-Nested Swaps'} ===\")\n",
    "\n",
    "        results = {\n",
    "            'param_values': [],\n",
    "            'throughputs': [],\n",
    "            'jain_results': [],\n",
    "            'wait_rates': [],\n",
    "            'pareto_points': []\n",
    "        }\n",
    "\n",
    "        for param_val in param_values:\n",
    "            print(f\"  --- Param {param_name} = {param_val} ---\")\n",
    "\n",
    "            # Set base kwargs\n",
    "            kwargs = {\n",
    "                'edges': edges,\n",
    "                'goal_edges': goal_edges,\n",
    "                'pSwap': pSwap,\n",
    "                'pGen': pGen,\n",
    "                'max_age': max_age,\n",
    "                'seed': seed,\n",
    "                'totalSteps': totalSteps,\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'edr_window_size': edr_window_size,\n",
    "                'reward_mode': reward_mode,\n",
    "                'temperature': temperature,\n",
    "                'temperature_decay': temperature_decay,\n",
    "                'nestedSwaps': nested_flag,\n",
    "                'log_interval': edr_window_size,\n",
    "            }\n",
    "            if nLookahead is not None:\n",
    "                kwargs['nLookahead'] = nLookahead\n",
    "\n",
    "            kwargs.update(extra_kwargs)\n",
    "\n",
    "            # Override sweeped param\n",
    "            if param_name == 'pSwap':\n",
    "                kwargs['pSwap'] = param_val\n",
    "            elif param_name == 'pGen':\n",
    "                kwargs['pGen'] = param_val\n",
    "\n",
    "            # --- Now train or simulate ---\n",
    "            if method_type in [\"sarsa\", \"qlearning\"]:\n",
    "                train_kwargs = {k: v for k, v in kwargs.items() if k in training_keys}\n",
    "                model = training_func(**train_kwargs)\n",
    "\n",
    "                # Simulate learned policy\n",
    "                sim_results = simulate_policy(\n",
    "                    Q_table=model,\n",
    "                    edges=edges,\n",
    "                    goal_edges=goal_edges,\n",
    "                    pSwap=kwargs['pSwap'],\n",
    "                    pGen=kwargs['pGen'],\n",
    "                    max_age=max_age,\n",
    "                    num_steps=simulate_steps,\n",
    "                    nestedSwaps=nested_flag,\n",
    "                    plot=False\n",
    "                )\n",
    "            elif method_type == \"greedy\":\n",
    "                # Directly run greedy simulator\n",
    "                edr_history, fairness_history, goals_achieved, goal_attempts, total_timesteps, aged, actions = training_func(\n",
    "                    edges, goal_edges,\n",
    "                    pSwap=kwargs['pSwap'],\n",
    "                    pGen=kwargs['pGen'],\n",
    "                    maxAge=kwargs['max_age'],\n",
    "                    policy=extra_kwargs.get('policy', 'utility'),\n",
    "                    num_steps=simulate_steps\n",
    "                )\n",
    "                sim_results = {\n",
    "                    'edr_history': edr_history,\n",
    "                    'jain_history': fairness_history['jains_index'],\n",
    "                    'wait_ratio': np.mean(aged[-1000:])  # approximate\n",
    "                }\n",
    "\n",
    "            # --- Analyze results ---\n",
    "            burn_in_idx = 5000\n",
    "            final_edrs = {g: np.mean(sim_results['edr_history'][g][burn_in_idx:]) for g in goal_edges}\n",
    "            final_tp = sum(final_edrs.values())\n",
    "            final_jain = np.mean(sim_results['jain_history'][burn_in_idx:])\n",
    "            wait_rate = sim_results['wait_ratio']\n",
    "\n",
    "            results['param_values'].append(param_val)\n",
    "            results['throughputs'].append(final_tp)\n",
    "            results['jain_results'].append(final_jain)\n",
    "            results['wait_rates'].append(wait_rate)\n",
    "            results['pareto_points'].append((final_tp, final_jain))\n",
    "\n",
    "        all_results[mode] = results\n",
    "\n",
    "    # === Plotting ===\n",
    "\n",
    "    param_values = np.array(param_values)\n",
    "    param_labels = {\n",
    "        'pSwap': r'Probability of Successful Swapping ($p_\\mathrm{swap}$)',\n",
    "        'pGen': r'Probability of Link Generation ($p_\\mathrm{gen}$)',\n",
    "    }\n",
    "    param_label = param_labels.get(param_name, param_name)\n",
    "\n",
    "    # Plot 1: Throughput vs param\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if method_type == \"greedy\":\n",
    "        plt.plot(param_values, all_results['non_nested']['throughputs'], marker='s', label='Greedy')\n",
    "    else:\n",
    "        plt.plot(param_values, all_results['nested']['throughputs'], marker='o', label='Nested Swaps')\n",
    "        plt.plot(param_values, all_results['non_nested']['throughputs'], marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(param_label)\n",
    "    plt.ylabel(\"Total Throughput\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Jain's Fairness Index vs param\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if method_type == \"greedy\":\n",
    "        plt.plot(param_values, all_results['non_nested']['jain_results'], marker='s', label='Greedy')\n",
    "    else:\n",
    "        plt.plot(param_values, all_results['nested']['jain_results'], marker='o', label='Nested Swaps')\n",
    "        plt.plot(param_values, all_results['non_nested']['jain_results'], marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(param_label)\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Pareto Frontier (Throughput vs Jain)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if method_type == \"greedy\":\n",
    "        non_nested_tp, non_nested_jain = zip(*all_results['non_nested']['pareto_points'])\n",
    "        plt.plot(non_nested_tp, non_nested_jain, marker='s', label='Greedy')\n",
    "    else:\n",
    "        nested_tp, nested_jain = zip(*all_results['nested']['pareto_points'])\n",
    "        non_nested_tp, non_nested_jain = zip(*all_results['non_nested']['pareto_points'])\n",
    "        plt.plot(nested_tp, nested_jain, marker='o', label='Nested Swaps')\n",
    "        plt.plot(non_nested_tp, non_nested_jain, marker='s', label='Non-Nested Swaps')\n",
    "    plt.xlabel(\"Total Throughput\")\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.xlim(0, None)\n",
    "    plt.ylim(0.45, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1acf6",
   "metadata": {},
   "source": [
    "jain matrix and edr matrix\n",
    "\n",
    "[[0.5        0.5        0.50229503 0.50467854 0.50954801 0.51233041\n",
    "  0.52201917 0.53838227 0.5        0.5       ]\n",
    " [0.5        0.5        0.5        0.52988175 0.53982937 0.56099546\n",
    "  0.57768545 0.59370976 0.5        0.5       ]\n",
    " [0.50071118 0.5        0.5        0.53292938 0.55030856 0.57309289\n",
    "  0.60052613 0.69488192 0.5        0.5       ]\n",
    " [0.50334301 0.5        0.51947727 0.53870008 0.55861251 0.58154082\n",
    "  0.62414957 0.72707747 0.5        0.5       ]\n",
    " [0.5        0.50603841 0.51255878 0.52779306 0.53871887 0.58209765\n",
    "  0.68473935 0.71945041 0.5        0.9462098 ]\n",
    " [0.50227394 0.5013926  0.51969908 0.53628335 0.55789785 0.57971529\n",
    "  0.64647994 0.69588931 0.90363203 0.95166383]\n",
    " [0.50169699 0.5        0.51163531 0.52658791 0.55013583 0.5706594\n",
    "  0.59552357 0.69832202 0.82943808 0.99005154]\n",
    " [0.50114731 0.50307657 0.50713109 0.52508739 0.53946558 0.55480011\n",
    "  0.60263676 0.77936055 0.94333359 0.99980178]\n",
    " [0.50052377 0.50077387 0.5        0.51552506 0.52293985 0.53320577\n",
    "  0.56028313 0.80059782 0.951061   0.99927277]\n",
    " [0.5        0.5        0.5        0.5        0.5        0.5\n",
    "  0.59978133 0.69951495 0.88162794 1.        ]]\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "[[0.00316243 0.00686    0.0100903  0.01339076 0.01650929 0.02023828\n",
    "  0.02313859 0.02755494 0.03026867 0.0335976 ]\n",
    " [0.0100775  0.02016653 0.0302049  0.04017264 0.05086861 0.06205667\n",
    "  0.07426959 0.08787841 0.0894448  0.099519  ]\n",
    " [0.01785382 0.0348183  0.05180517 0.07112581 0.09143479 0.11104224\n",
    "  0.1336142  0.15296906 0.16082853 0.17847687]\n",
    " [0.02436973 0.0521463  0.07935118 0.1075979  0.1365404  0.16656432\n",
    "  0.19792452 0.2277803  0.2407397  0.2677936 ]\n",
    " [0.0346023  0.07079648 0.10444902 0.14243317 0.18244908 0.22355622\n",
    "  0.26241463 0.31216899 0.31955547 0.4341626 ]\n",
    " [0.04509889 0.08984521 0.13310239 0.18237074 0.23303386 0.28396194\n",
    "  0.33336851 0.39711657 0.46052857 0.54928729]\n",
    " [0.0545215  0.1143096  0.16366197 0.22150666 0.28571463 0.34957529\n",
    "  0.41774527 0.4842695  0.57083754 0.67081742]\n",
    " [0.06859111 0.13523556 0.20539268 0.27371264 0.34600758 0.42597713\n",
    "  0.49489732 0.5541944  0.65165273 0.78565699]\n",
    " [0.081887   0.16615752 0.24788433 0.33396325 0.42158335 0.50858295\n",
    "  0.59096953 0.63105812 0.74202484 0.89546676]\n",
    " [0.10151673 0.19945233 0.29835907 0.4007114  0.49902087 0.5987393\n",
    "  0.63895587 0.7301051  0.83842497 0.99999422]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jain_matrix)\n",
    "print('---' * 50)\n",
    "print(edr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7d281",
   "metadata": {},
   "source": [
    "# **RUNNING CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totalSteps = 100_000\n",
    "# First, train the SARSA policy\n",
    "Q = train_sarsa_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=0.2,        # example learning rate (choose based on experiment)\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    log_interval=windowSize,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    noop_penalty=0.0,\n",
    "    plotTraining=True,\n",
    "    rewardAlpha=0.5,  # smoothing reward\n",
    ")\n",
    "\n",
    "# Then, simulate the trained policy\n",
    "simulation_results = simulate_policy(\n",
    "    Q_table=Q,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=50_000,  # how long to simulate\n",
    "    edr_window_size=1000,\n",
    "    burn_in=5000,      # optional, ignore early unstable steps\n",
    "    plot=True,\n",
    "    nestedSwaps=nestedSwaps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e6fa723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training for maxAge=1 ===\n",
      "Step 100000\n",
      "Step 200000\n",
      "Step 300000\n",
      "Step 400000\n",
      "Step 500000\n",
      "\n",
      "Metrics After Burn-in (first 5000 steps ignored):\n",
      "Mean EDRs: {(0, 4): '0.0109', (3, 5): '0.2137'}\n",
      "Total Throughput (sum of EDRs): 0.2247\n",
      "Jain's Fairness Index: 0.5510\n",
      "Waited when could have acted: 0/20325 = 0.0000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_q_learning_linear_policy() missing 1 required positional argument: 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     35\u001b[39m np.random.seed(seed)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Now run both SARSA and Q-Learning sweep\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m results = \u001b[43mrun_sarsa_qlearning_over_maxage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindowSize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindowSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnestedSwaps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnestedSwaps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxAge_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxAge_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43msimulate_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimulate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mburn_in_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mburn_in_steps\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mrun_sarsa_qlearning_over_maxage\u001b[39m\u001b[34m(edges, goal_edges, pSwap, pGen, seed, totalSteps, alpha, gamma, windowSize, reward_mode, temperature, temperature_decay, nestedSwaps, maxAge_values, simulate_steps, burn_in_steps)\u001b[39m\n\u001b[32m     71\u001b[39m sarsa_throughput_scores.append(final_tp_sarsa)\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# --- Q-Learning ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m Q_qlearn = \u001b[43mtrain_q_learning_linear_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_age\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxAge_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43medr_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindowSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnestedSwaps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnestedSwaps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoop_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplotTraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrewardAlpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m sim_qlearn = simulate_policy(\n\u001b[32m     95\u001b[39m     Q_table=Q_qlearn,\n\u001b[32m     96\u001b[39m     edges=edges,\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m     nestedSwaps=nestedSwaps\n\u001b[32m    106\u001b[39m )\n\u001b[32m    108\u001b[39m final_jain_qlearn = np.mean(sim_qlearn[\u001b[33m'\u001b[39m\u001b[33mjain_history\u001b[39m\u001b[33m'\u001b[39m][burn_in_steps:])\n",
      "\u001b[31mTypeError\u001b[39m: train_q_learning_linear_policy() missing 1 required positional argument: 'softmax'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Fixed parameters\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "totalSteps = 500_000\n",
    "nLookahead = 5\n",
    "alpha = 0.005\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "nestedSwaps = False\n",
    "\n",
    "\n",
    "\n",
    "# Sweep values\n",
    "maxAge_values = [1, 2, 3, 4, 5, 6]\n",
    "simulate_steps = 50_000\n",
    "burn_in_steps = 5000\n",
    "\n",
    "# Set seeds\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# -------------------------------\n",
    "# Now run both SARSA and Q-Learning sweep\n",
    "# -------------------------------\n",
    "\n",
    "results = run_sarsa_qlearning_over_maxage(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    windowSize=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    maxAge_values=maxAge_values,\n",
    "    simulate_steps=simulate_steps,\n",
    "    burn_in_steps=burn_in_steps\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d034db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up parameters\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 500_000\n",
    "\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic' \n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "nestedSwaps = False\n",
    "\n",
    "# --- Define your sweeps ---\n",
    "pSwap_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "pGen_values =[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# Prepare matrices\n",
    "jain_matrix = np.zeros((len(pGen_values), len(pSwap_values)))\n",
    "edr_matrix = np.zeros((len(pGen_values), len(pSwap_values)))\n",
    "\n",
    "# --- Loop over (pGen, pSwap) grid ---\n",
    "for i, pGen_val in enumerate(pGen_values):\n",
    "    for j, pSwap_val in enumerate(pSwap_values):\n",
    "        print(f\"\\n=== Running for pSwap={pSwap_val}, pGen={pGen_val} ===\")\n",
    "        \n",
    "        result = compareOverParam(\n",
    "            param_name='pSwap',  # We will manually vary pSwap and pGen\n",
    "            param_values=[pSwap_val],  # just single value\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap_val,\n",
    "            pGen=pGen_val,\n",
    "            max_age=maxAge,\n",
    "            totalSteps=totalSteps,\n",
    "            nLookahead=nLookahead,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            edr_window_size=windowSize,\n",
    "            reward_mode='basic',  # or linear, partial\n",
    "            temperature=temperature,\n",
    "            temperature_decay=temperature_decay,\n",
    "            seed=seed,\n",
    "            nestedSwaps=nestedSwaps,\n",
    "            training_function=train_sarsa_linear_policy,\n",
    "            simulate_steps=50_000,\n",
    "            burn_in_ratio=0.5,\n",
    "        )\n",
    "\n",
    "        # Extract Jain and EDR (throughput = sum EDRs)\n",
    "        jain = result['jain_results'][0]\n",
    "        edr_sum = sum(result['edr_results'][g][0] for g in goal_edges)\n",
    "\n",
    "        jain_matrix[i, j] = jain\n",
    "        edr_matrix[i, j] = edr_sum\n",
    "\n",
    "# --- Now plot heatmaps ---\n",
    "\n",
    "# (1) Jain's Index Heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(jain_matrix, origin='lower', cmap='viridis', extent=[min(pSwap_values), max(pSwap_values), min(pGen_values), max(pGen_values)], aspect='auto')\n",
    "plt.colorbar(label=\"Jain's Index\")\n",
    "plt.xlabel('pSwap')\n",
    "plt.ylabel('pGen')\n",
    "plt.title(\"Jain's Index over (pSwap, pGen)\")\n",
    "plt.xticks(pSwap_values)\n",
    "plt.yticks(pGen_values)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (2) EDR Sum (Throughput) Heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(edr_matrix, origin='lower', cmap='plasma', extent=[min(pSwap_values), max(pSwap_values), min(pGen_values), max(pGen_values)], aspect='auto')\n",
    "plt.colorbar(label=\"Sum EDR (Throughput)\")\n",
    "plt.xlabel('pSwap')\n",
    "plt.ylabel('pGen')\n",
    "plt.title(\"Throughput over (pSwap, pGen)\")\n",
    "plt.xticks(pSwap_values)\n",
    "plt.yticks(pGen_values)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 500_000\n",
    "\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'alphafair' \n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "nestedSwaps = False\n",
    "\n",
    "# Alpha values you want to sweep (example)\n",
    "alpha_values = [0.2, 0.4, 0.6, 0.9, 1, 1.5, 2]\n",
    "\n",
    "# Call compareOverAlpha\n",
    "results = compareOverAlpha(\n",
    "    alpha_values=alpha_values,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    training_function=train_sarsa_linear_policy,  # or train_q_learning_linear_policy\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7758922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define your network and parameters\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 2_000_000\n",
    "\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "alpha = 0.005\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "\n",
    "nestedSwaps = False   # or True, depending what you want\n",
    "\n",
    "# Parameter sweep\n",
    "param_name = 'pSwap'   # <-- what parameter you want to vary\n",
    "param_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Now call your compareOverParam function\n",
    "resultspSwap = compareOverParam(\n",
    "    param_name=param_name,\n",
    "    param_values=param_values,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    training_function=train_sarsa_linear_policy,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.1,   # Optional: change to 0.1 for faster burn-in\n",
    ")\n",
    "\n",
    "param_name = 'pGen'   # <-- what parameter you want to vary\n",
    "\n",
    "resultspGen = compareOverParam(\n",
    "    param_name=param_name,\n",
    "    param_values=param_values,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    training_function=train_sarsa_linear_policy,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.1,   # Optional: change to 0.1 for faster burn-in\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define your network and parameters\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 200_000\n",
    "\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "alpha = 0.005\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "\n",
    "nestedSwaps = False   # or True, depending on what you want\n",
    "\n",
    "# Parameter sweep setup\n",
    "param_name = 'pGen'\n",
    "param_values = [0.1, 0.3, 0.5, 0.7, 0.8, 1]\n",
    "\n",
    "# Reward modes to sweep\n",
    "reward_modes = ['basic', 'partial']\n",
    "\n",
    "# Finally call compareOverParamAndRewardMode\n",
    "all_results_normalreward= compareOverParamAndRewardMode(\n",
    "    param_name=param_name,\n",
    "    param_values=param_values,\n",
    "    reward_modes=reward_modes,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    training_function=train_sarsa_linear_policy,   # <=== you have to define which training function here\n",
    "    simulate_steps=60_000,\n",
    "    burn_in_ratio=0.5\n",
    ")\n",
    "\n",
    "param_name = 'pSwap'\n",
    "param_values = [0.1, 0.3, 0.5, 0.7, 0.8, 1]\n",
    "\n",
    "# Reward modes to sweep\n",
    "reward_modes = ['basic', 'partial']\n",
    "\n",
    "# Finally call compareOverParamAndRewardMode\n",
    "all_results_normalreward= compareOverParamAndRewardMode(\n",
    "    param_name=param_name,\n",
    "    param_values=param_values,\n",
    "    reward_modes=reward_modes,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwaps,\n",
    "    training_function=train_sarsa_linear_policy,   # <=== you have to define which training function here\n",
    "    simulate_steps=60_000,\n",
    "    burn_in_ratio=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44978622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Network topology and goal edges\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "# Swap and generation probabilities\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 500_000\n",
    "\n",
    "# SARSA hyperparameters\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "alpha = 0.005\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "paramValues= [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# === Now call compareNestedVsNonNested ===\n",
    "compareNestedpSwap = compareNestedVsNonNested(\n",
    "    param_name='pSwap',                  # Choose which parameter to sweep (could also use 'pGen')\n",
    "    param_values=paramValues,   # Example sweep over pSwap values\n",
    "    sarsa_func=train_sarsa_linear_policy,    # Your SARSA training function\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,                           # Base value (only overwritten for sweep)\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    simulate_steps=30_000,                  # How long to simulate after training\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\" sarsa 500k, 0.6, 0.6, main, gives \n",
    "\n",
    "{'nested': {'param_values': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'throughputs': [np.float64(0.04479173409113084), np.float64(0.089807141270978), np.float64(0.13394902587228277), np.float64(0.17990761796143231), np.float64(0.22991024117051917), np.float64(0.2818934627136345), np.float64(0.33523304789475794), np.float64(0.39519306049669667), np.float64(0.46298218184218565), np.float64(0.5502106493187333)], 'jain_results': [np.float64(0.5041105079674845), np.float64(0.5011325449574769), np.float64(0.5148825890270682), np.float64(0.5307178572177074), np.float64(0.5516622707981663), np.float64(0.571438681337192), np.float64(0.6329514110428551), np.float64(0.686835914269177), np.float64(0.901849353702561), np.float64(0.9598492713921325)], 'wait_rates': [0.03147214290054074, 0.21367134449977365, 0.08167096925604318, 0.07704476212795876, 0.07846556233653008, 0.0916, 0.07310344827586207, 0.07935421301842661, 0.07264523406655386, 0.07982583454281568], 'pareto_points': [(np.float64(0.04479173409113084), np.float64(0.5041105079674845)), (np.float64(0.089807141270978), np.float64(0.5011325449574769)), (np.float64(0.13394902587228277), np.float64(0.5148825890270682)), (np.float64(0.17990761796143231), np.float64(0.5307178572177074)), (np.float64(0.22991024117051917), np.float64(0.5516622707981663)), (np.float64(0.2818934627136345), np.float64(0.571438681337192)), (np.float64(0.33523304789475794), np.float64(0.6329514110428551)), (np.float64(0.39519306049669667), np.float64(0.686835914269177)), (np.float64(0.46298218184218565), np.float64(0.901849353702561)), (np.float64(0.5502106493187333), np.float64(0.9598492713921325))]}, 'non_nested': {'param_values': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], 'throughputs': [np.float64(0.04531722023121352), np.float64(0.08984169486916369), np.float64(0.13484894172821396), np.float64(0.1803932722680079), np.float64(0.22931611256865778), np.float64(0.28128791866412656), np.float64(0.3315077131712439), np.float64(0.39384526071305986), np.float64(0.465622672557804), np.float64(0.5517184953504484)], 'jain_results': [np.float64(0.5042371023310197), np.float64(0.5011548145368954), np.float64(0.5176766081875562), np.float64(0.5351256080225916), np.float64(0.5546053362051213), np.float64(0.580218656547907), np.float64(0.6490807776780783), np.float64(0.6983053470564536), np.float64(0.9051326257014519), np.float64(0.9507401094555775)], 'wait_rates': [0.0, 0.19471028362623977, 0.01493550577053632, 0.0001867297398232292, 0.0, 0.011403562842877397, 0.0, 0.0, 0.0, 0.0], 'pareto_points': [(np.float64(0.04531722023121352), np.float64(0.5042371023310197)), (np.float64(0.08984169486916369), np.float64(0.5011548145368954)), (np.float64(0.13484894172821396), np.float64(0.5176766081875562)), (np.float64(0.1803932722680079), np.float64(0.5351256080225916)), (np.float64(0.22931611256865778), np.float64(0.5546053362051213)), (np.float64(0.28128791866412656), np.float64(0.580218656547907)), (np.float64(0.3315077131712439), np.float64(0.6490807776780783)), (np.float64(0.39384526071305986), np.float64(0.6983053470564536)), (np.float64(0.465622672557804), np.float64(0.9051326257014519)), (np.float64(0.5517184953504484), np.float64(0.9507401094555775))]}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareTrainingMethods(\n",
    "    param_name='pSwap',\n",
    "    param_values=paramValues,\n",
    "    training_func=train_q_learning_linear_policy,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    method_type=\"qlearning\",   # <<< Important\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compareNestedpSwap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f76a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Network topology and goal edges\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "# Swap and generation probabilities\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 10_000\n",
    "\n",
    "# Q-Learning hyperparameters\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "alpha = 0.005\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "paramValues = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# === Now call compareNestedVsNonNested with Q-learning ===\n",
    "compareNestedpSwap_Q = compareNestedVsNonNested(\n",
    "    param_name='pSwap',                         # Parameter to sweep\n",
    "    param_values=paramValues,                   # Values to sweep over\n",
    "    sarsa_func=train_q_learning_linear_policy,  # <<<< Q-Learning training function\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,                                # Base value\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    simulate_steps=30_000,                      # Simulate after training\n",
    "\n",
    "    # --- Q-Learning-specific parameters ---\n",
    "    softmax=True,                               # Softmax exploration\n",
    "    nestedSwaps=False                           # Toggle nested swaps if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Network topology and goal edges\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "# Swap and generation probabilities\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 500_000\n",
    "\n",
    "# SARSA hyperparameters\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "alpha = 0.005\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "paramValues= [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# === Now call compareNestedVsNonNested ===\n",
    "compareNestedpSwap = compareNestedVsNonNested(\n",
    "    param_name='pSwap',                  # Choose which parameter to sweep (could also use 'pGen')\n",
    "    param_values=paramValues,   # Example sweep over pSwap values\n",
    "    sarsa_func=train_sarsa_linear_policy,    # Your SARSA training function\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,                           # Base value (only overwritten for sweep)\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    simulate_steps=30_000,                  # How long to simulate after training\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3974a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure seed is set\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 20\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Your environment and parameters\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "pSwap = 0.6\n",
    "pGen = 0.6\n",
    "maxAge = 3\n",
    "totalSteps = 2_000_000\n",
    "nestedSwap = False\n",
    "\n",
    "nLookahead = 5\n",
    "gamma = 0.995\n",
    "windowSize = 1000\n",
    "reward_mode = 'basic'\n",
    "alpha = 0.005\n",
    "\n",
    "temperature = 6.0\n",
    "final_temperature = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "\n",
    "# Sweep settings\n",
    "param_name = 'pSwap'\n",
    "param_values = [0.2, 0.5, 0.6, 0.8]\n",
    "\n",
    "# Models dictionary\n",
    "models = {\n",
    "    \"SARSA\": train_sarsa_linear_policy,\n",
    "    \"Q-Learning\": train_q_learning_linear_policy\n",
    "}\n",
    "\n",
    "# Now finally call\n",
    "all_results_models = compareOverParamAndModel(\n",
    "    param_name=param_name,\n",
    "    param_values=param_values,\n",
    "    models=models,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwap,\n",
    "    simulate_steps=60_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    reward_mode=reward_mode,\n",
    "    softmax=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 20\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "#####\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "pSwap       = 0.6\n",
    "pGen        = 0.6\n",
    "maxAge      =  3\n",
    "totalSteps     = 800_000\n",
    "nestedSwap = False\n",
    "#####\n",
    "nLookahead     = 5\n",
    "gamma          = 0.995\n",
    "windowSize     = 1000\n",
    "reward_mode    = 'basic'\n",
    "alpha          = 0.005\n",
    "temperature = 6.0\n",
    "final_temperature   = 0.1\n",
    "temperature_decay = (final_temperature / temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "\n",
    "\n",
    "# Define your sweep\n",
    "param_name = 'pSwap'  # or 'pGen'\n",
    "param_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "reward_modes = ['basic', 'linear', 'partial']\n",
    "# Call the comparison function\n",
    "all_results2 = compareOverParamAndRewardMode(\n",
    "    param_name=param_name,\n",
    "    param_values=param_values,\n",
    "    reward_modes=reward_modes,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    temperature=temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=seed,\n",
    "    nestedSwaps=nestedSwap,\n",
    "    training_function=train_sarsa_linear_policy,\n",
    "    simulate_steps=60_000,\n",
    "    burn_in_ratio=0.5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
