{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd38cb70",
   "metadata": {},
   "source": [
    "# **N-STEP SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b19c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque, defaultdict\n",
    "from linearApprox import *\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0c9b5",
   "metadata": {},
   "source": [
    "#  **UTILITY CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jains_index(edrs):\n",
    "    \"\"\"Compute Jain's Fairness Index.\"\"\"\n",
    "    if all(edr == 0 for edr in edrs.values()):\n",
    "        return 0.0\n",
    "    numerator = sum(edrs.values())**2\n",
    "    denominator = len(edrs) * sum(v**2 for v in edrs.values())\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def featurize_state(state, goal_order):\n",
    "    ent_state, edrs = state\n",
    "    edge_features = [age / 10.0 if age >= 0 else -1.0 for _, age in ent_state]\n",
    "    edr_features = list(edrs)\n",
    "    return np.array(edge_features + edr_features, dtype=np.float32)\n",
    "\n",
    "class LinearQApproximator:\n",
    "    def __init__(self, feature_size):\n",
    "        self.weights = {}  # Dict[action_key] = weight_vector\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def _action_key(self, action):\n",
    "        consumed_paths, goal_list = action\n",
    "\n",
    "        # Handle the no-op case: goal_list is None for no-op action\n",
    "        if goal_list is None:\n",
    "            return ((), None)  # Ensure this is hashable for no-op action\n",
    "\n",
    "        # Sort paths and goals for consistency in hashing\n",
    "        sorted_paths = tuple(sorted(tuple(sorted(path)) for path in consumed_paths))\n",
    "        sorted_goals = tuple(sorted(goal_list))\n",
    "        return (sorted_paths, sorted_goals)\n",
    "\n",
    "    def _init_weights(self, action_key):\n",
    "        if action_key not in self.weights:\n",
    "            self.weights[action_key] = np.zeros(self.feature_size)\n",
    "\n",
    "    def get_q_value(self, features, action):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        return float(np.dot(self.weights[key], features))\n",
    "\n",
    "    def update(self, features, action, target, alpha):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        prediction = np.dot(self.weights[key], features)\n",
    "        error = target - prediction\n",
    "        self.weights[key] += alpha * error * features\n",
    "\n",
    "\n",
    "def get_possible_multi_actions(state, goalEdges):\n",
    "    actions = []\n",
    "    existing_edges = {edge for edge, age in state if age >= 0}\n",
    "\n",
    "    def find_path(start, end, path=None):\n",
    "        if path is None:\n",
    "            path = []\n",
    "        if start == end:\n",
    "            return [path]\n",
    "        paths = []\n",
    "        for edge in existing_edges:\n",
    "            if edge not in path:\n",
    "                if edge[0] == start:\n",
    "                    new_paths = find_path(edge[1], end, path + [edge])\n",
    "                    paths.extend(new_paths)\n",
    "                elif edge[1] == start:\n",
    "                    new_paths = find_path(edge[0], end, path + [edge])\n",
    "                    paths.extend(new_paths)\n",
    "        return paths\n",
    "\n",
    "    # Single-goal actions\n",
    "    single_goal_actions = []\n",
    "    for goal in goalEdges:\n",
    "        start, end = goal\n",
    "        paths = find_path(start, end)\n",
    "        for path in paths:\n",
    "            if len(path) > 0:\n",
    "                single_goal_actions.append((path, goal))\n",
    "                actions.append(([path], [goal]))  # wrapped in list for multi-path format\n",
    "\n",
    "    # Multi-goal actions (disjoint paths only)\n",
    "    for k in range(2, len(single_goal_actions) + 1):\n",
    "        for combo in itertools.combinations(single_goal_actions, k):\n",
    "            paths, goals = zip(*combo)\n",
    "            flat_edges = [e for path in paths for e in path]\n",
    "            if len(flat_edges) == len(set(flat_edges)):\n",
    "                actions.append((list(paths), list(goals)))\n",
    "\n",
    "    # Add no-op action explicitly\n",
    "    actions.append(([], None))  # no-op action\n",
    "\n",
    "    return actions\n",
    "\n",
    "def compute_reward(action, goal_success_queues, pSwap, mode=\"basic\", alpha=1.0, epsilon=1e-3, noop_penalty=0.0):\n",
    "    consumed_edges, goals = action\n",
    "    if not goals or not consumed_edges:\n",
    "        return -noop_penalty, False\n",
    "\n",
    "    total_reward = 0.0\n",
    "    any_success = False\n",
    "    used_edges = set()\n",
    "\n",
    "    for goal, path in zip(goals, consumed_edges):\n",
    "        path_edges = set(path)\n",
    "        if not path_edges.isdisjoint(used_edges):\n",
    "            continue\n",
    "        used_edges.update(path_edges)\n",
    "\n",
    "        success_prob = pSwap ** (len(path) - 1)\n",
    "        edr = sum(goal_success_queues[goal]) / len(goal_success_queues[goal]) + epsilon\n",
    "        x = success_prob / edr\n",
    "        success = (random.random() < success_prob)\n",
    "        any_success = any_success or success\n",
    "\n",
    "        if mode == \"partial\":\n",
    "            base = math.log(1 + x)\n",
    "            total_reward += base if success else 0.5 * base\n",
    "        else:\n",
    "            total_reward += math.log(1 + x) if success else 0.0\n",
    "\n",
    "    return total_reward, any_success\n",
    "\n",
    "def performAction(action, aaugmented_state):\n",
    "    consumed_paths, goal_edges = action\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = list(ent_state)\n",
    "    for path in consumed_paths:\n",
    "        for edge_to_consume in path:\n",
    "            for i, (edge, age) in enumerate(new_state):\n",
    "                if edge == edge_to_consume:\n",
    "                    new_state[i] = (edge, -1)\n",
    "                    break\n",
    "    return (tuple(new_state), edr_bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6dc1",
   "metadata": {},
   "source": [
    "# **SIMULATION CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    p_swap,\n",
    "    p_gen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    plot=True\n",
    "):\n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    raw = [(e, -1) for e in edges]\n",
    "    current = get_augmented_state(raw, {g:0.0 for g in goal_edges}, goal_order=goal_edges)\n",
    "\n",
    "    recent = {g: [] for g in goal_edges}\n",
    "    edr_hist, jain_hist, tp_hist = {g:[] for g in goal_edges}, [], []\n",
    "    valids, acts, qvals = [], [], []\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        ent_state, _ = current\n",
    "        acts_all = get_possible_multi_actions(ent_state, goal_edges)\n",
    "        if ([], None) not in acts_all:\n",
    "            acts_all.append(([], None))\n",
    "        real = [a for a in acts_all if a != ([], None)]\n",
    "        avail = len(real) > 0\n",
    "        valids.append(1 if avail else 0)\n",
    "\n",
    "        feats = featurize_state(current, goal_edges)\n",
    "        best_a, best_q = max(((a, Q_table.get_q_value(feats, a)) for a in acts_all), key=lambda x: x[1])\n",
    "        qvals.append(best_q)\n",
    "        acts.append(1.0 if (avail and best_a in real) else 0.0)\n",
    "\n",
    "        nxt = performAction(best_a, current)\n",
    "        nxt = ageEntanglements(nxt, max_age)\n",
    "        nxt = generateEntanglement(nxt, p_gen)\n",
    "\n",
    "        consumed_paths, goals = best_a\n",
    "        if goals is not None:\n",
    "            for g, path in zip(goals, consumed_paths):\n",
    "                if path:\n",
    "                    succ = random.random() < (p_swap ** (len(path) - 1))\n",
    "                    recent[g].append(1 if succ else 0)\n",
    "                else:\n",
    "                    recent[g].append(0)\n",
    "            for g in goal_edges:\n",
    "                if g not in goals:\n",
    "                    recent[g].append(0)\n",
    "        else:\n",
    "            for g in goal_edges:\n",
    "                recent[g].append(0)\n",
    "\n",
    "        if len(recent[g]) > edr_window_size:\n",
    "            recent[g].pop(0)\n",
    "\n",
    "        edrs = {g: sum(recent[g]) / len(recent[g]) for g in goal_edges}\n",
    "        for g in goal_edges:\n",
    "            edr_hist[g].append(edrs[g])\n",
    "\n",
    "        total = sum(edrs.values())\n",
    "        tp_hist.append(total)\n",
    "        jain_hist.append(jains_index(edrs))\n",
    "\n",
    "        current = get_augmented_state(nxt[0], edrs, goal_order=goal_edges)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        fig.suptitle(\n",
    "            f\"Policy Sim â€” pSwap={p_swap}, pGen={p_gen}, maxAge={max_age}, steps={num_steps}, window={edr_window_size}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        # (0) EDR + Jain\n",
    "        ax0 = axs[0]\n",
    "        for g in goal_edges:\n",
    "            ax0.plot(edr_hist[g], label=f\"EDR {g}\")\n",
    "        ax0.plot(jain_hist, '--', label=\"Jain's\", linewidth=2)\n",
    "        ax0.set_title(\"EDR (solid) & Jain (dashed)\")\n",
    "        ax0.set_xlabel(\"Timestep\")\n",
    "        ax0.set_ylabel(\"Value\")\n",
    "        ax0.set_ylim(0, 1.05)\n",
    "        ax0.legend()\n",
    "\n",
    "        # (1) single Pareto point after burn-in\n",
    "        ax1 = axs[1]\n",
    "        avg_tp = np.mean(tp_hist[burn_in:])\n",
    "        avg_jain = np.mean(jain_hist[burn_in:])\n",
    "        ax1.scatter([avg_tp], [avg_jain], s=100, c='crimson')\n",
    "        ax1.set_title(\"Final Pareto Point\")\n",
    "        ax1.set_xlabel(\"Avg Throughput\")\n",
    "        ax1.set_ylabel(\"Avg Jain\")\n",
    "        ax1.set_xlim(0, max(tp_hist) * 1.1)\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.text(avg_tp, avg_jain, f\"  ({avg_tp:.3f}, {avg_jain:.3f})\")\n",
    "\n",
    "        # (2) best Q-value\n",
    "        ax2 = axs[2]\n",
    "        ax2.plot(qvals, color='slateblue')\n",
    "        ax2.set_title(\"Best Q-Value Over Time\")\n",
    "        ax2.set_xlabel(\"Timestep\")\n",
    "        ax2.set_ylabel(\"Q-Value\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    burn_in_idx = 5000\n",
    "    final_edrs = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "    final_tp = sum(final_edrs.values())\n",
    "    final_jain = jains_index(final_edrs)\n",
    "\n",
    "    print(\"\\nMetrics After Burn-in (first 5000 steps ignored):\")\n",
    "    print(\"Mean EDRs:\", {g: f\"{v:.4f}\" for g, v in final_edrs.items()})\n",
    "    print(f\"Total Throughput (sum of EDRs): {final_tp:.4f}\")\n",
    "    print(f\"Jain's Fairness Index: {final_jain:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"edr_history\": edr_hist,\n",
    "        \"jain_history\": jain_hist,\n",
    "        \"throughput_history\": tp_hist,\n",
    "        \"q_values\": qvals\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d12b6",
   "metadata": {},
   "source": [
    "#  **Q-LEARNING CODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f82aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e7e7f7",
   "metadata": {},
   "source": [
    "#  **SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c42baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_step_sarsa_linear_multi(\n",
    "    initialEdges, goalEdges, totalSteps, nLookahead,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\", reward_epsilon=1e-3,\n",
    "    noop_penalty=0.01, log_interval=10000,\n",
    "    initial_temperature=1.0, temperature_decay=0.9999\n",
    "):\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    state_buffer = deque([current])\n",
    "    reward_buffer = deque()\n",
    "    Q = LinearQApproximator(feature_size=len(initialEdges) + len(goalEdges))\n",
    "    temperature = initial_temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "    action_counter = {\n",
    "        'noop': 0,\n",
    "        'goal1': 0,\n",
    "        'goal2': 0,\n",
    "        'both': 0\n",
    "    }\n",
    "\n",
    "    def select_action(state, temperature):\n",
    "        feats = featurize_state(state, goalEdges)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges)\n",
    "        \n",
    "        # Debug: Print all possible actions and their Q-values\n",
    "        # print(\"Possible actions and Q-values:\")\n",
    "        # for action in acts:\n",
    "        #     action_q_value = Q.get_q_value(feats, action)\n",
    "        #     print(f\"Action: {action}, Q-value: {action_q_value}\")\n",
    "\n",
    "        # Ensure the no-op action is included\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        # Calculate Q-values and probabilities using softmax\n",
    "        q_values = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "        scaled_qs = q_values / max(temperature, 1e-6)\n",
    "        exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "        probs = exp_qs / np.sum(exp_qs)\n",
    "        \n",
    "        # Sample action based on the probabilities\n",
    "        idx = np.random.choice(len(acts), p=probs)\n",
    "        chosen = acts[idx]\n",
    "\n",
    "        # Debug: Print the chosen action\n",
    "        # print(f\"Chosen action: {chosen}\")\n",
    "\n",
    "        # Track the action type for frequency analysis\n",
    "        if chosen == ([], None):\n",
    "            action_counter['noop'] += 1\n",
    "        elif len(chosen[1]) == 1:\n",
    "            if chosen[1][0] == goalEdges[0]:\n",
    "                action_counter['goal1'] += 1\n",
    "            else:\n",
    "                action_counter['goal2'] += 1\n",
    "        elif len(chosen[1]) == 2:\n",
    "            action_counter['both'] += 1\n",
    "\n",
    "        return chosen\n",
    "\n",
    "\n",
    "    action_buffer = deque([select_action(current, temperature)])\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        temperature = max(0.01, temperature * temperature_decay)\n",
    "\n",
    "        S_t = state_buffer[-1]\n",
    "        A_t = action_buffer[-1]\n",
    "\n",
    "        ns = performAction(A_t, S_t)\n",
    "        ns = ageEntanglements(ns, maxAge)\n",
    "        ns = generateEntanglement(ns, pGen)\n",
    "\n",
    "        r, succ = compute_reward(\n",
    "            A_t, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, epsilon=reward_epsilon, noop_penalty=noop_penalty\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = A_t\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        reward_buffer.append(r)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        next_state = (ns[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "\n",
    "        A_next = select_action(next_state, temperature)\n",
    "\n",
    "        state_buffer.append(next_state)\n",
    "        action_buffer.append(A_next)\n",
    "\n",
    "        if len(reward_buffer) >= nLookahead:\n",
    "            G = sum((gamma**i) * reward_buffer[i] for i in range(nLookahead))\n",
    "            s_n = state_buffer[nLookahead]\n",
    "            a_n = action_buffer[nLookahead]\n",
    "            feats_n = featurize_state(s_n, goalEdges)\n",
    "            G += (gamma**nLookahead) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "            s_tau = state_buffer[0]\n",
    "            a_tau = action_buffer[0]\n",
    "            feats_tau = featurize_state(s_tau, goalEdges)\n",
    "            old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "            diff = abs(G - old_q)\n",
    "            q_value_diffs.append(diff)\n",
    "\n",
    "            for gg in goalEdges:\n",
    "                if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                    q_value_diffs_per_goal[gg].append(diff)\n",
    "                else:\n",
    "                    q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "            Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "            state_buffer.popleft()\n",
    "            action_buffer.popleft()\n",
    "            reward_buffer.popleft()\n",
    "\n",
    "    while reward_buffer:\n",
    "        n = len(reward_buffer)\n",
    "        G = sum((gamma**i) * reward_buffer[i] for i in range(n))\n",
    "        if n < len(state_buffer):\n",
    "            s_n = state_buffer[n]\n",
    "            a_n = action_buffer[n]\n",
    "            feats_n = featurize_state(s_n, goalEdges)\n",
    "            G += (gamma**n) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "        s_tau = state_buffer[0]\n",
    "        a_tau = action_buffer[0]\n",
    "        feats_tau = featurize_state(s_tau, goalEdges)\n",
    "        old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "        diff = abs(G - old_q)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        for gg in goalEdges:\n",
    "            if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                q_value_diffs_per_goal[gg].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "        Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "        state_buffer.popleft()\n",
    "        action_buffer.popleft()\n",
    "        reward_buffer.popleft()\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history, action_counter\n",
    "\n",
    "######################################################################################################################################################\n",
    "\n",
    "def train_sarsa_linear_policy(\n",
    "    edges, goal_edges, p_swap, p_gen, max_age,\n",
    "    seed,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode, reward_epsilon,\n",
    "    noop_penalty, log_interval,\n",
    "    initial_temperature, temperature_decay\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Run the SARSA multi-step learning\n",
    "    result = run_n_step_sarsa_linear_multi(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        nLookahead=nLookahead,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=p_gen,\n",
    "        pSwap=p_swap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        reward_epsilon=reward_epsilon,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        initial_temperature=initial_temperature,\n",
    "        temperature_decay=temperature_decay\n",
    "    )\n",
    "\n",
    "    # Check if result is valid\n",
    "    if result is None:\n",
    "        print(\"Error: run_n_step_sarsa_linear_multi returned None.\")\n",
    "        return\n",
    "\n",
    "    # Unpack results\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist, action_counter = result\n",
    "\n",
    "    # --- Q-value convergence ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(q_diffs)\n",
    "    plt.xlabel(\"SARSA Updates\")\n",
    "    plt.ylabel(\"Q-value Difference\")\n",
    "    plt.title(\"Q-value Convergence (Global)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- EDR evolution ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(edr_steps, edr_hist[g], label=f\"EDR {g}\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"EDR Estimate\")\n",
    "    plt.title(\"EDR Evolution During Training\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Action frequency plot ---\n",
    "    labels = ['No-op', 'Goal 1', 'Goal 2', 'Both Goals']\n",
    "    frequencies = [\n",
    "        action_counter['noop'],\n",
    "        action_counter['goal1'],\n",
    "        action_counter['goal2'],\n",
    "        action_counter['both']\n",
    "    ]\n",
    "    frequencies = [f / totalSteps for f in frequencies]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, frequencies, color=['gray', 'blue', 'green', 'orange'])\n",
    "    plt.xlabel(\"Action Type\")\n",
    "    plt.ylabel(\"Proportion of Total Actions\")\n",
    "    plt.title(\"Proportion of Each Action Type Chosen During Training\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb17739",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges       = [(1,2), (2,3), (3,4), (4,5)]\n",
    "goal_edges  = [(1,4), (3,5)]\n",
    "\n",
    "pSwap       = 0.7\n",
    "pGen        = 0.7\n",
    "maxAge      = 2\n",
    "\n",
    "totalSteps     = 40_000_000\n",
    "nLookahead     = 3\n",
    "gamma          = 0.99\n",
    "alpha          = 0.01\n",
    "windowSize     = 1000\n",
    "reward_mode    = 'basic'\n",
    "reward_epsilon = 1e-3\n",
    "\n",
    "# Softmax temperature parameters\n",
    "initial_temperature = 5.0\n",
    "final_temperature   = 0.1\n",
    "temperature_decay   = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 0.9))\n",
    "\n",
    "# Seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# --- Train N-step SARSA policy (with temperature-based softmax) ---\n",
    "Q = train_sarsa_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    reward_epsilon=reward_epsilon,\n",
    "    noop_penalty=0.0,\n",
    "    log_interval=1000,\n",
    "    initial_temperature=initial_temperature,\n",
    "    temperature_decay=temperature_decay\n",
    ")\n",
    "print('done training')\n",
    "# --- Evaluate learned policy ---\n",
    "simulate_policy(\n",
    "    Q_table=Q,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    p_swap=pSwap,\n",
    "    p_gen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=50_000,\n",
    "    edr_window_size=windowSize,\n",
    "    plot=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
