{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd38cb70",
   "metadata": {},
   "source": [
    "# **N-STEP SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b19c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque, defaultdict, Counter\n",
    "#from linearApprox import *\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0c9b5",
   "metadata": {},
   "source": [
    "#  **UTILITY CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4ab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_state(state, edrs, goal_order=None):\n",
    "    if goal_order is None:\n",
    "        goal_order = sorted(edrs.keys())\n",
    "    edr_vector = tuple(edrs[goal] for goal in goal_order)\n",
    "    sorted_state = sorted(state, key=lambda x: (x[0][0], x[0][1]))\n",
    "    return (tuple(sorted_state), edr_vector)\n",
    "\n",
    "def ageEntanglements(augmented_state, maxAge):\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = []\n",
    "    for edge, age in ent_state:\n",
    "        if age >= 0:\n",
    "            new_age = age + 1\n",
    "            if new_age > maxAge:\n",
    "                new_state.append((edge, -1))\n",
    "            else:\n",
    "                new_state.append((edge, new_age))\n",
    "        else:\n",
    "            new_state.append((edge, age))\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "def generateEntanglement(augmented_state, pGen, initial_edges):\n",
    "    ent_state, edr_bins = augmented_state\n",
    "\n",
    "    # Set of initial edges (normalized to (min(u,v), max(u,v)) form)\n",
    "    initial_edges_set = { (min(u,v), max(u,v)) for (u,v) in initial_edges }\n",
    "\n",
    "    new_state = []\n",
    "    seen_edges = set()\n",
    "\n",
    "    for edge, age in ent_state:\n",
    "        normalized_edge = (min(edge[0], edge[1]), max(edge[0], edge[1]))\n",
    "        \n",
    "        if normalized_edge in initial_edges_set:\n",
    "            # This is an initial edge, so it can regrow\n",
    "            if age < 0:  # currently dead\n",
    "                if random.random() < pGen:\n",
    "                    new_state.append((normalized_edge, 1))\n",
    "                else:\n",
    "                    new_state.append((normalized_edge, -1))\n",
    "            else:\n",
    "                new_state.append((normalized_edge, age))\n",
    "        else:\n",
    "            # Non-initial edge: only keep if still alive\n",
    "            if age >= 0:\n",
    "                new_state.append((normalized_edge, age))\n",
    "            # Otherwise, if dead and non-initial, don't add back at all (completely delete)\n",
    "        \n",
    "        seen_edges.add(normalized_edge)\n",
    "\n",
    "    # Safety check: Add missing initial edges if completely missing\n",
    "    for edge in initial_edges_set:\n",
    "        if edge not in seen_edges:\n",
    "            if random.random() < pGen:\n",
    "                new_state.append((edge, 1))\n",
    "            else:\n",
    "                new_state.append((edge, -1))\n",
    "\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jains_index(edrs):\n",
    "    \"\"\"Compute Jain's Fairness Index.\"\"\"\n",
    "    if all(edr == 0 for edr in edrs.values()):\n",
    "        return 0.0\n",
    "    numerator = sum(edrs.values())**2\n",
    "    denominator = len(edrs) * sum(v**2 for v in edrs.values())\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def featurize_state(state, goal_order, master_edge_list):\n",
    "    ent_state, edrs = state\n",
    "    edge_age_map = {edge: age for edge, age in ent_state}\n",
    "\n",
    "    edge_features = []\n",
    "    for edge in master_edge_list:\n",
    "        age = edge_age_map.get(edge, -1)\n",
    "        edge_features.append(age / 10.0 if age >= 0 else -1.0)\n",
    "\n",
    "    edr_features = list(edrs)\n",
    "    return np.array(edge_features + edr_features, dtype=np.float32)\n",
    "\n",
    "\n",
    "class LinearQApproximator:\n",
    "    def __init__(self, feature_size):\n",
    "        self.weights = {}  # Dict[action_key] = weight_vector\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def _action_key(self, action):\n",
    "        consumed_paths, goal_list = action\n",
    "\n",
    "        # Handle the no-op case: goal_list is None for no-op action\n",
    "        if goal_list is None:\n",
    "            return ((), None)  # Ensure this is hashable for no-op action\n",
    "\n",
    "        # Sort paths and goals for consistency in hashing\n",
    "        sorted_paths = tuple(sorted(tuple(sorted(path)) for path in consumed_paths))\n",
    "        sorted_goals = tuple(sorted(goal_list))\n",
    "        return (sorted_paths, sorted_goals)\n",
    "\n",
    "    def _init_weights(self, action_key):\n",
    "        if action_key not in self.weights:\n",
    "            self.weights[action_key] = np.zeros(self.feature_size)\n",
    "\n",
    "    def get_q_value(self, features, action):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        return float(np.dot(self.weights[key], features))\n",
    "\n",
    "    def update(self, features, action, target, alpha):\n",
    "        key = self._action_key(action)\n",
    "        self._init_weights(key)\n",
    "        prediction = np.dot(self.weights[key], features)\n",
    "        error = target - prediction\n",
    "        self.weights[key] += alpha * error * features\n",
    "\n",
    "\n",
    "def get_possible_multi_actionsold(ent_state, goalEdges, nestedSwaps=False, max_path_length=None):\n",
    "    import itertools\n",
    "\n",
    "    actions = []\n",
    "    existing_edges = {tuple(sorted(edge)) for edge, age in ent_state if age >= 0}\n",
    "\n",
    "    def find_paths(start, visited=None, path=None, depth=0):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if path is None:\n",
    "            path = []\n",
    "\n",
    "        paths = []\n",
    "        for edge in existing_edges:\n",
    "            if edge in visited:\n",
    "                continue\n",
    "            u, v = edge\n",
    "            if u == start or v == start:\n",
    "                next_node = v if u == start else u\n",
    "                new_path = path + [edge]\n",
    "                paths.append(new_path)\n",
    "                if max_path_length is None or depth < max_path_length:\n",
    "                    paths.extend(find_paths(next_node, visited | {edge}, new_path, depth + 1))\n",
    "        return paths\n",
    "\n",
    "    single_goal_actions = []\n",
    "    for goal in goalEdges:\n",
    "        start, end = goal\n",
    "        paths = find_paths(start)\n",
    "        for path in paths:\n",
    "            if not path or len(path)<2:\n",
    "                continue\n",
    "            nodes = [n for edge in path for n in edge]\n",
    "            counts = {node: nodes.count(node) for node in nodes}\n",
    "            endpoints = [node for node, count in counts.items() if count == 1]\n",
    "            if len(endpoints) != 2:\n",
    "                continue\n",
    "            if not nestedSwaps:\n",
    "                if set(endpoints) != set(goal):\n",
    "                    continue\n",
    "            normalized_path = [tuple(sorted(e)) for e in path]\n",
    "            single_goal_actions.append((normalized_path, goal))\n",
    "            actions.append(([normalized_path], [goal]))\n",
    "\n",
    "    for k in range(2, len(single_goal_actions) + 1):\n",
    "        for combo in itertools.combinations(single_goal_actions, k):\n",
    "            paths, goals = zip(*combo)\n",
    "            flat_edges = [tuple(sorted(e)) for path in paths for e in path]\n",
    "            if len(flat_edges) == len(set(flat_edges)):\n",
    "                actions.append((list(paths), list(goals)))\n",
    "\n",
    "    actions.append(([], None))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(action, goal_success_queues, pSwap, mode=\"basic\", alpha=1.0, noop_penalty=0.0):\n",
    "    epsilon = 0.0001\n",
    "    consumed_edges, goals = action\n",
    "    if not goals or not consumed_edges:\n",
    "        return -noop_penalty, False\n",
    "\n",
    "    total_reward = 0.0\n",
    "    any_success = False\n",
    "    used_edges = set()\n",
    "\n",
    "    for goal, path in zip(goals, consumed_edges):\n",
    "        path_edges = set(path)\n",
    "        if not path_edges.isdisjoint(used_edges):\n",
    "            continue\n",
    "        used_edges.update(path_edges)\n",
    "\n",
    "        success_prob = pSwap ** (len(path) - 1)\n",
    "        edr = sum(goal_success_queues[goal]) / len(goal_success_queues[goal]) + epsilon\n",
    "        x = success_prob / edr\n",
    "        success = (random.random() < success_prob)\n",
    "        any_success = any_success or success\n",
    "\n",
    "        if mode == \"partial\":\n",
    "            base = math.log(1 + x)\n",
    "            total_reward += base if success else 0.5 * base\n",
    "        else:\n",
    "            total_reward += math.log(1 + x) if success else 0.0\n",
    "\n",
    "    return total_reward, any_success\n",
    "\n",
    "def performAction(action, augmented_state, pSwap, nestedSwaps=False, system_goals=None):\n",
    "    consumed_paths, _ = action\n",
    "    ent_state, edr_bins = augmented_state\n",
    "    new_state = list(ent_state)\n",
    "\n",
    "    normalized_goal_edges = set((min(u, v), max(u, v)) for u, v in (system_goals or []))\n",
    "    busy_nodes = set(u for (u, v), age in ent_state if age >= 0 for u, v in [(u, v)])\n",
    "\n",
    "    used_edges = set()  # Tracks entanglements used this timestep (normalized)\n",
    "\n",
    "    for path in consumed_paths:\n",
    "        if not path:\n",
    "            continue\n",
    "\n",
    "        normalized_path = [tuple(sorted(e)) for e in path]\n",
    "\n",
    "        # Prevent reusing entanglements already used this timestep\n",
    "        if any(edge in used_edges for edge in normalized_path):\n",
    "            continue\n",
    "\n",
    "        consumed_ages = []\n",
    "        for edge_to_consume in normalized_path:\n",
    "            for i, (edge, age) in enumerate(new_state):\n",
    "                if tuple(sorted(edge)) == edge_to_consume:\n",
    "                    consumed_ages.append(age)\n",
    "                    busy_nodes.discard(edge[0])\n",
    "                    busy_nodes.discard(edge[1])\n",
    "                    new_state[i] = (edge, -1)\n",
    "                    break\n",
    "\n",
    "        # Mark all edges in this path as used\n",
    "        used_edges.update(normalized_path)\n",
    "\n",
    "        # Attempt swap\n",
    "        success_prob = pSwap ** (len(path) - 1)\n",
    "        swap_success = random.random() < success_prob\n",
    "\n",
    "        if not swap_success:\n",
    "            continue\n",
    "\n",
    "        # Determine new edge from endpoints\n",
    "        nodes = [n for edge in normalized_path for n in edge]\n",
    "        node_counts = {node: nodes.count(node) for node in nodes}\n",
    "        endpoints = [node for node, count in node_counts.items() if count == 1]\n",
    "\n",
    "        if len(endpoints) != 2:\n",
    "            continue\n",
    "\n",
    "        start, end = endpoints\n",
    "        new_edge = (min(start, end), max(start, end))\n",
    "\n",
    "        if new_edge in normalized_path:\n",
    "            continue\n",
    "\n",
    "        if start in busy_nodes or end in busy_nodes:\n",
    "            continue\n",
    "\n",
    "        new_age = max(consumed_ages) if consumed_ages else 0\n",
    "        alive_edges = {edge for edge, age in new_state if age >= 0}\n",
    "\n",
    "        if new_edge not in alive_edges:\n",
    "            if nestedSwaps or new_edge in normalized_goal_edges:\n",
    "                new_state.append((new_edge, new_age))\n",
    "                busy_nodes.update([start, end])\n",
    "\n",
    "    # Remove dead edges\n",
    "    new_state = [pair for pair in new_state if pair[1] != -1]\n",
    "    return (tuple(new_state), edr_bins)\n",
    "\n",
    "def get_possible_multi_actions(ent_state, goalEdges, nestedSwaps=False, max_path_length=None):\n",
    "    import itertools\n",
    "\n",
    "    actions = []\n",
    "    existing_edges = {tuple(sorted(edge)) for edge, age in ent_state if age >= 0}\n",
    "\n",
    "    def find_paths(start, visited=None, path=None, depth=0):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if path is None:\n",
    "            path = []\n",
    "\n",
    "        paths = []\n",
    "        for edge in existing_edges:\n",
    "            if edge in visited:\n",
    "                continue\n",
    "            u, v = edge\n",
    "            if u == start or v == start:\n",
    "                next_node = v if u == start else u\n",
    "                new_path = path + [edge]\n",
    "                paths.append(new_path)\n",
    "                if max_path_length is None or depth < max_path_length:\n",
    "                    paths.extend(find_paths(next_node, visited | {edge}, new_path, depth + 1))\n",
    "        return paths\n",
    "\n",
    "    # Single-goal actions\n",
    "    single_goal_actions = []\n",
    "    for goal in goalEdges:\n",
    "        start, end = goal\n",
    "        paths = find_paths(start)\n",
    "        for path in paths:\n",
    "            if not path or len(path) < 2:  # 🛡️ Skip trivial 1-hop paths\n",
    "                continue\n",
    "\n",
    "            nodes = [n for edge in path for n in edge]\n",
    "            counts = {node: nodes.count(node) for node in nodes}\n",
    "            endpoints = [node for node, count in counts.items() if count == 1]\n",
    "            if len(endpoints) != 2:\n",
    "                continue\n",
    "\n",
    "            if set(endpoints) != set(goal):\n",
    "                continue\n",
    "\n",
    "            normalized_path = [tuple(sorted(e)) for e in path]\n",
    "            single_goal_actions.append((normalized_path, goal))\n",
    "            actions.append(([normalized_path], [goal]))\n",
    "\n",
    "    # Multi-goal disjoint actions\n",
    "    for k in range(2, len(single_goal_actions) + 1):\n",
    "        for combo in itertools.combinations(single_goal_actions, k):\n",
    "            paths, goals = zip(*combo)\n",
    "\n",
    "            if len(set(goals)) != len(goals):  # 🛡️ Prevent duplicate goals\n",
    "                continue\n",
    "\n",
    "            flat_edges = [tuple(sorted(e)) for path in paths for e in path]\n",
    "            if len(flat_edges) == len(set(flat_edges)):  # 🛡️ Ensure disjoint paths\n",
    "                actions.append((list(paths), list(goals)))\n",
    "\n",
    "    # Always allow no-op action\n",
    "    actions.append(([], None))\n",
    "\n",
    "    return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6dc1",
   "metadata": {},
   "source": [
    "# **SIMULATION CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9a4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    pSwap,\n",
    "    pGen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    plot=True,\n",
    "    nestedSwaps=False\n",
    "):    \n",
    "    \n",
    "    \n",
    "    nodes = set()\n",
    "    for u, v in edges:\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    nodes = sorted(list(nodes))\n",
    "\n",
    "    master_edge_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    raw = [(e, -1) for e in edges]\n",
    "    current = get_augmented_state(raw, {g:0.0 for g in goal_edges}, goal_order=goal_edges)\n",
    "\n",
    "    recent = {g: [] for g in goal_edges}\n",
    "    edr_hist, jain_hist, tp_hist = {g:[] for g in goal_edges}, [], []\n",
    "    valids, acts, qvals = [], [], []\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        ent_state, _ = current\n",
    "        acts_all = get_possible_multi_actions(ent_state, goal_edges, nestedSwaps=nestedSwaps)\n",
    "\n",
    "        if ([], None) not in acts_all:\n",
    "            acts_all.append(([], None))\n",
    "        real = [a for a in acts_all if a != ([], None)]\n",
    "        avail = len(real) > 0\n",
    "        valids.append(1 if avail else 0)\n",
    "\n",
    "        feats = featurize_state(current, goal_edges, master_edge_list)\n",
    "        best_a, best_q = max(((a, Q_table.get_q_value(feats, a)) for a in acts_all), key=lambda x: x[1])\n",
    "        qvals.append(best_q)\n",
    "        acts.append(1.0 if (avail and best_a in real) else 0.0)\n",
    "\n",
    "        nxt = performAction(best_a, current, pSwap=pSwap, nestedSwaps=nestedSwaps)\n",
    "        nxt = ageEntanglements(nxt, max_age)\n",
    "        nxt = generateEntanglement(nxt, pGen, edges)\n",
    "\n",
    "        consumed_paths, goals = best_a\n",
    "        if goals is not None:\n",
    "            for g, path in zip(goals, consumed_paths):\n",
    "                if path:\n",
    "                    succ = random.random() < (pSwap ** (len(path) - 1))\n",
    "                    recent[g].append(1 if succ else 0)\n",
    "                else:\n",
    "                    recent[g].append(0)\n",
    "            for g in goal_edges:\n",
    "                if g not in goals:\n",
    "                    recent[g].append(0)\n",
    "        else:\n",
    "            for g in goal_edges:\n",
    "                recent[g].append(0)\n",
    "\n",
    "        if len(recent[g]) > edr_window_size:\n",
    "            recent[g].pop(0)\n",
    "\n",
    "        edrs = {g: sum(recent[g]) / len(recent[g]) for g in goal_edges}\n",
    "        for g in goal_edges:\n",
    "            edr_hist[g].append(edrs[g])\n",
    "\n",
    "        total = sum(edrs.values())\n",
    "        tp_hist.append(total)\n",
    "        jain_hist.append(jains_index(edrs))\n",
    "\n",
    "        current = get_augmented_state(nxt[0], edrs, goal_order=goal_edges)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        fig.suptitle(\n",
    "            f\"Policy Sim — pSwap={pSwap}, pGen={pGen}, maxAge={max_age}, steps={num_steps}, window={edr_window_size}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        # (0) EDR + Jain\n",
    "        ax0 = axs[0]\n",
    "        for g in goal_edges:\n",
    "            ax0.plot(edr_hist[g], label=f\"EDR {g}\")\n",
    "        ax0.plot(jain_hist, '--', label=\"Jain's\", linewidth=2)\n",
    "        ax0.set_title(\"EDR (solid) & Jain (dashed)\")\n",
    "        ax0.set_xlabel(\"Timestep\")\n",
    "        ax0.set_ylabel(\"Value\")\n",
    "        ax0.set_ylim(0, 1.05)\n",
    "        ax0.legend()\n",
    "\n",
    "        # (1) single Pareto point after burn-in\n",
    "        ax1 = axs[1]\n",
    "        avg_tp = np.mean(tp_hist[burn_in:])\n",
    "        avg_jain = np.mean(jain_hist[burn_in:])\n",
    "        ax1.scatter([avg_tp], [avg_jain], s=100, c='crimson')\n",
    "        ax1.set_title(\"Final Pareto Point\")\n",
    "        ax1.set_xlabel(\"Avg Throughput\")\n",
    "        ax1.set_ylabel(\"Avg Jain\")\n",
    "        ax1.set_xlim(0, max(tp_hist) * 1.1)\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.text(avg_tp, avg_jain, f\"  ({avg_tp:.3f}, {avg_jain:.3f})\")\n",
    "\n",
    "        # (2) best Q-value\n",
    "        ax2 = axs[2]\n",
    "        ax2.plot(qvals, color='slateblue')\n",
    "        ax2.set_title(\"Best Q-Value Over Time\")\n",
    "        ax2.set_xlabel(\"Timestep\")\n",
    "        ax2.set_ylabel(\"Q-Value\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    burn_in_idx = 5000\n",
    "    final_edrs = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "    final_tp = sum(final_edrs.values())\n",
    "    final_jain = jains_index(final_edrs)\n",
    "\n",
    "    print(\"\\nMetrics After Burn-in (first 5000 steps ignored):\")\n",
    "    print(\"Mean EDRs:\", {g: f\"{v:.4f}\" for g, v in final_edrs.items()})\n",
    "    print(f\"Total Throughput (sum of EDRs): {final_tp:.4f}\")\n",
    "    print(f\"Jain's Fairness Index: {final_jain:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"edr_history\": edr_hist,\n",
    "        \"jain_history\": jain_hist,\n",
    "        \"throughput_history\": tp_hist,\n",
    "        \"q_values\": qvals\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892b1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_simulate_policy(\n",
    "    Q_table,\n",
    "    edges,\n",
    "    goal_edges,\n",
    "    pSwap,\n",
    "    pGen,\n",
    "    max_age,\n",
    "    num_steps,\n",
    "    edr_window_size=100,\n",
    "    burn_in=None,\n",
    "    seeds=[10, 20, 30, 40, 50],\n",
    "    plot=True,\n",
    "    nestedSwaps=False\n",
    "):\n",
    "    if burn_in is None:\n",
    "        burn_in = num_steps // 2\n",
    "\n",
    "    all_edrs = {g: [] for g in goal_edges}\n",
    "    all_jains = []\n",
    "    all_tp = []\n",
    "\n",
    "    edr_time_series = {g: [] for g in goal_edges}\n",
    "    jain_time_series = []\n",
    "\n",
    "    goal_colors = {\n",
    "        goal_edges[0]: \"tab:blue\",\n",
    "        goal_edges[1]: \"tab:green\"\n",
    "    }\n",
    "\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        result = simulate_policy(\n",
    "            Q_table=Q_table,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=max_age,\n",
    "            num_steps=num_steps,\n",
    "            edr_window_size=edr_window_size,\n",
    "            burn_in=burn_in,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        # Final metrics\n",
    "        edr_hist = result[\"edr_history\"]\n",
    "        jain_hist = result[\"jain_history\"]\n",
    "        tp_hist = result[\"throughput_history\"]\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_time_series[g].append(edr_hist[g])\n",
    "        jain_time_series.append(jain_hist)\n",
    "\n",
    "        burn_in_idx = 5000\n",
    "        edrs_final = {g: np.mean(edr_hist[g][burn_in_idx:]) for g in goal_edges}\n",
    "        tp_final = sum(edrs_final.values())\n",
    "        jain_final = jains_index(edrs_final)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            all_edrs[g].append(edrs_final[g])\n",
    "        all_tp.append(tp_final)\n",
    "        all_jains.append(jain_final)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        fig.suptitle(\n",
    "            f\"Multi-Sim — {len(seeds)} Seeds | pSwap={pSwap}, pGen={pGen}, maxAge={max_age}, steps={num_steps}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        # --- EDR time series ---\n",
    "        ax0 = axs[0]\n",
    "        for g in goal_edges:\n",
    "            for run_edr in edr_time_series[g]:\n",
    "                ax0.plot(run_edr, color=goal_colors[g], alpha=0.3)\n",
    "            ax0.plot(np.mean(edr_time_series[g], axis=0), color=goal_colors[g], linewidth=2, label=f\"EDR {g}\")\n",
    "        ax0.set_title(\"EDRs Over Time\")\n",
    "        ax0.set_xlabel(\"Timestep\")\n",
    "        ax0.set_ylabel(\"EDR\")\n",
    "        ax0.set_ylim(0, 1.05)\n",
    "        ax0.legend()\n",
    "\n",
    "        # --- Jain's Index time series ---\n",
    "        ax1 = axs[1]\n",
    "        for run_jain in jain_time_series:\n",
    "            ax1.plot(run_jain, color=\"gray\", alpha=0.3)\n",
    "        ax1.plot(np.mean(jain_time_series, axis=0), color=\"black\", linewidth=2, label=\"Jain’s Index\")\n",
    "        ax1.set_title(\"Jain's Index Over Time\")\n",
    "        ax1.set_xlabel(\"Timestep\")\n",
    "        ax1.set_ylabel(\"Jain's Fairness\")\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.legend()\n",
    "\n",
    "        # --- Pareto scatter plot ---\n",
    "        ax2 = axs[2]\n",
    "        ax2.scatter(all_tp, all_jains, c='crimson', s=60, label=\"Runs\")\n",
    "        ax2.scatter(np.mean(all_tp), np.mean(all_jains), c='black', s=100, marker='x', label=\"Mean\")\n",
    "        ax2.set_title(\"Pareto Points Across Seeds\")\n",
    "        ax2.set_xlabel(\"Avg Throughput\")\n",
    "        ax2.set_ylabel(\"Avg Jain\")\n",
    "        ax2.set_xlim(0, max(all_tp) * 1.1)\n",
    "        ax2.set_ylim(0, 1.05)\n",
    "        for tp, jn in zip(all_tp, all_jains):\n",
    "            ax2.text(tp + 0.002, jn, f\"({tp:.2f}, {jn:.2f})\", fontsize=8)\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"avg_edrs\": {g: np.mean(all_edrs[g]) for g in goal_edges},\n",
    "        \"avg_tp\": np.mean(all_tp),\n",
    "        \"avg_jain\": np.mean(all_jains),\n",
    "        \"edrs_by_seed\": all_edrs,\n",
    "        \"tp_by_seed\": all_tp,\n",
    "        \"jain_by_seed\": all_jains\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221bba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverParam(\n",
    "    param_name, param_values,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    initial_temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    edr_results = {g: [] for g in goal_edges}\n",
    "    jain_results = []\n",
    "    pareto_points = []\n",
    "    full_raw_data = {}\n",
    "\n",
    "    for value in param_values:\n",
    "        print(f\"\\n=== Training with {param_name} = {value} ===\")\n",
    "\n",
    "        if param_name == 'pGen':\n",
    "            pGen = value\n",
    "        elif param_name == 'pSwap':\n",
    "            pSwap = value\n",
    "        else:\n",
    "            raise ValueError(\"param_name must be 'pGen' or 'pSwap'.\")\n",
    "\n",
    "        # --- Build kwargs dynamically ---\n",
    "        train_kwargs = {\n",
    "            'edges': edges,\n",
    "            'goal_edges': goal_edges,\n",
    "            'pSwap': pSwap,\n",
    "            'pGen': pGen,\n",
    "            'max_age': max_age,\n",
    "            'seed': seed,\n",
    "            'totalSteps': totalSteps,\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'edr_window_size': edr_window_size,\n",
    "            'reward_mode': reward_mode,\n",
    "            'nestedSwaps': nestedSwaps,\n",
    "        }\n",
    "\n",
    "        # --- Add nLookahead if needed ---\n",
    "        if training_function.__name__ == 'train_sarsa_linear_policy':\n",
    "            train_kwargs['nLookahead'] = nLookahead\n",
    "            train_kwargs['initial_temperature'] = initial_temperature\n",
    "            train_kwargs['temperature_decay'] = temperature_decay\n",
    "            train_kwargs['log_interval'] = edr_window_size\n",
    "        elif training_function.__name__ == 'train_q_learning_linear_policy':\n",
    "            # rename properly for q_learning\n",
    "            train_kwargs['temperature'] = initial_temperature  # <=== fix here\n",
    "            train_kwargs['temperature_decay'] = temperature_decay\n",
    "\n",
    "        # --- Add extra kwargs ---\n",
    "        train_kwargs.update(extra_kwargs)\n",
    "\n",
    "        # --- Now call training function cleanly ---\n",
    "        Q = training_function(**train_kwargs)\n",
    "\n",
    "        # Simulate\n",
    "        sim_result = simulate_policy(\n",
    "            Q_table=Q,\n",
    "            edges=edges,\n",
    "            goal_edges=goal_edges,\n",
    "            pSwap=pSwap,\n",
    "            pGen=pGen,\n",
    "            max_age=max_age,\n",
    "            num_steps=simulate_steps,\n",
    "            edr_window_size=edr_window_size,\n",
    "            plot=False,\n",
    "            nestedSwaps=nestedSwaps\n",
    "        )\n",
    "\n",
    "        # Post-processing\n",
    "        burn_in_steps = int(simulate_steps * burn_in_ratio)\n",
    "        final_edrs = {g: np.mean(sim_result[\"edr_history\"][g][burn_in_steps:]) for g in goal_edges}\n",
    "        final_tp = sum(final_edrs.values())\n",
    "        final_jain = jains_index(final_edrs)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_results[g].append(final_edrs[g])\n",
    "        jain_results.append(final_jain)\n",
    "        pareto_points.append((final_tp, final_jain))\n",
    "        full_raw_data[value] = sim_result\n",
    "\n",
    "    # --- Plotting (same as before) ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(param_values, edr_results[g], marker='o', label=f\"EDR {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.title(f\"EDR vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(param_values, jain_results, marker='s', label=\"Jain's Fairness\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(f\"Jain's Fairness vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    pareto_tp, pareto_jain = zip(*pareto_points)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pareto_tp, pareto_jain, c='crimson', s=80)\n",
    "    for (tp, jain), val in zip(pareto_points, param_values):\n",
    "        plt.text(tp, jain, f\"{val:.2f}\", fontsize=9)\n",
    "    plt.xlabel(\"Throughput (sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.title(f\"Pareto Plot for different {param_name} values\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, max(pareto_tp) * 1.1)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"param_name\": param_name,\n",
    "        \"param_values\": param_values,\n",
    "        \"edr_results\": edr_results,\n",
    "        \"jain_results\": jain_results,\n",
    "        \"pareto_points\": pareto_points,\n",
    "        \"full_raw_data\": full_raw_data\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c779f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareOverParamRobust(\n",
    "    param_name, param_values,\n",
    "    edges, goal_edges,\n",
    "    pSwap, pGen, max_age,\n",
    "    totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    initial_temperature, temperature_decay,\n",
    "    seed,\n",
    "    nestedSwaps,\n",
    "    training_function,\n",
    "    simulate_steps=50_000,\n",
    "    burn_in_ratio=0.5,\n",
    "    trainCount=3,\n",
    "    simulateCount=3,\n",
    "    **extra_kwargs\n",
    "):\n",
    "    edr_results = {g: [] for g in goal_edges}\n",
    "    jain_results = []\n",
    "    pareto_points = []\n",
    "    full_raw_data = {}\n",
    "\n",
    "    for idx_value, value in enumerate(param_values):\n",
    "        print(f\"\\n=== Training with {param_name} = {value} ===\")\n",
    "\n",
    "        # --- Dynamic parameter assignment ---\n",
    "        if param_name == 'pGen':\n",
    "            pGen = value\n",
    "        elif param_name == 'pSwap':\n",
    "            pSwap = value\n",
    "        else:\n",
    "            raise ValueError(\"param_name must be 'pGen' or 'pSwap'.\")\n",
    "\n",
    "        edrs_all_runs = {g: [] for g in goal_edges}\n",
    "        jains_all_runs = []\n",
    "        tp_all_runs = []\n",
    "\n",
    "        for train_idx in range(trainCount):\n",
    "            train_seed = seed + idx_value * 100 + train_idx * 10  # Safe separation\n",
    "            np.random.seed(train_seed)\n",
    "            random.seed(train_seed)\n",
    "\n",
    "            # --- Build kwargs dynamically ---\n",
    "            train_kwargs = {\n",
    "                'edges': edges,\n",
    "                'goal_edges': goal_edges,\n",
    "                'pSwap': pSwap,\n",
    "                'pGen': pGen,\n",
    "                'max_age': max_age,\n",
    "                'seed': train_seed,\n",
    "                'totalSteps': totalSteps,\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'edr_window_size': edr_window_size,\n",
    "                'reward_mode': reward_mode,\n",
    "                'nestedSwaps': nestedSwaps,\n",
    "            }\n",
    "\n",
    "            if training_function.__name__ == 'train_sarsa_linear_policy':\n",
    "                train_kwargs['nLookahead'] = nLookahead\n",
    "                train_kwargs['initial_temperature'] = initial_temperature\n",
    "                train_kwargs['temperature_decay'] = temperature_decay\n",
    "                train_kwargs['log_interval'] = edr_window_size\n",
    "            elif training_function.__name__ == 'train_q_learning_linear_policy':\n",
    "                train_kwargs['temperature'] = initial_temperature\n",
    "                train_kwargs['temperature_decay'] = temperature_decay\n",
    "\n",
    "            train_kwargs.update(extra_kwargs)\n",
    "\n",
    "            Q = training_function(**train_kwargs)\n",
    "\n",
    "            # --- Simulate multiple times for each trained model ---\n",
    "            sim_seeds = [train_seed + s * 1000 for s in range(simulateCount)]  # Different seeds for simulation\n",
    "            multi_sim_result = multi_simulate_policy(\n",
    "                Q_table=Q,\n",
    "                edges=edges,\n",
    "                goal_edges=goal_edges,\n",
    "                pSwap=pSwap,\n",
    "                pGen=pGen,\n",
    "                max_age=max_age,\n",
    "                num_steps=simulate_steps,\n",
    "                edr_window_size=edr_window_size,\n",
    "                burn_in=int(simulate_steps * burn_in_ratio),\n",
    "                seeds=sim_seeds,\n",
    "                plot=False,\n",
    "                nestedSwaps=nestedSwaps\n",
    "            )\n",
    "\n",
    "            # --- Collect results ---\n",
    "            for g in goal_edges:\n",
    "                edrs_all_runs[g].append(multi_sim_result[\"avg_edrs\"][g])\n",
    "            jains_all_runs.append(multi_sim_result[\"avg_jain\"])\n",
    "            tp_all_runs.append(multi_sim_result[\"avg_tp\"])\n",
    "\n",
    "        # --- After all training runs ---\n",
    "        final_avg_edrs = {g: np.mean(edrs_all_runs[g]) for g in goal_edges}\n",
    "        final_avg_jain = np.mean(jains_all_runs)\n",
    "        final_avg_tp = np.mean(tp_all_runs)\n",
    "\n",
    "        for g in goal_edges:\n",
    "            edr_results[g].append(final_avg_edrs[g])\n",
    "        jain_results.append(final_avg_jain)\n",
    "        pareto_points.append((final_avg_tp, final_avg_jain))\n",
    "        full_raw_data[value] = {\n",
    "            \"edrs_by_run\": edrs_all_runs,\n",
    "            \"jains_by_run\": jains_all_runs,\n",
    "            \"tp_by_run\": tp_all_runs\n",
    "        }\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(param_values, edr_results[g], marker='o', label=f\"EDR {g}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean EDR\")\n",
    "    plt.title(f\"EDR vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(param_values, jain_results, marker='s', label=\"Jain's Fairness\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Jain's Index\")\n",
    "    plt.title(f\"Jain's Fairness vs {param_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    pareto_tp, pareto_jain = zip(*pareto_points)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(pareto_tp, pareto_jain, c='crimson', s=80)\n",
    "    for (tp, jain), val in zip(pareto_points, param_values):\n",
    "        plt.text(tp, jain, f\"{val:.2f}\", fontsize=9)\n",
    "    plt.xlabel(\"Throughput (sum EDRs)\")\n",
    "    plt.ylabel(\"Jain's Fairness Index\")\n",
    "    plt.title(f\"Pareto Plot for different {param_name} values\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0, max(pareto_tp) * 1.1)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"param_name\": param_name,\n",
    "        \"param_values\": param_values,\n",
    "        \"edr_results\": edr_results,\n",
    "        \"jain_results\": jain_results,\n",
    "        \"pareto_points\": pareto_points,\n",
    "        \"full_raw_data\": full_raw_data\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628e7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def plot_training_results(\n",
    "    q_diffs, q_diffs_per_goal, edr_steps, edr_hist,\n",
    "    goal_edges, fairness_history, edge_creation_counter=None,\n",
    "    method_name=\"\",\n",
    "    smoothing_window=1000  # <--- added\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # --- Plot Global Q-value Convergence (no smoothing) ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(q_diffs, color='royalblue', alpha=0.8)\n",
    "    plt.xlabel(f\"{method_name} Updates\")\n",
    "    plt.ylabel(\"Global Q-value Difference\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"{method_name}: Global Q-value Convergence\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Per-goal Q-value Convergence + No-Op Together (smoothed) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    linestyles = ['-', '--', ':', '-.']\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(goal_edges) + 1))\n",
    "\n",
    "    for i, g in enumerate(goal_edges):\n",
    "        diffs = np.array(q_diffs_per_goal[g])\n",
    "        steps = np.arange(len(diffs))\n",
    "        mask = ~np.isnan(diffs)\n",
    "\n",
    "        if np.sum(mask) > smoothing_window:  # only smooth if enough points\n",
    "            smooth_diffs = moving_average(diffs[mask], smoothing_window)\n",
    "            smooth_steps = steps[mask][:len(smooth_diffs)]\n",
    "            plt.plot(\n",
    "                smooth_steps, smooth_diffs,\n",
    "                label=f\"Goal {g}\",\n",
    "                linestyle=linestyles[i % len(linestyles)],\n",
    "                color=colors[i],\n",
    "                alpha=0.9,\n",
    "                linewidth=2\n",
    "            )\n",
    "\n",
    "    # Plot No-op line if available\n",
    "    if 'noop' in q_diffs_per_goal:\n",
    "        diffs = np.array(q_diffs_per_goal['noop'])\n",
    "        steps = np.arange(len(diffs))\n",
    "        mask = ~np.isnan(diffs)\n",
    "\n",
    "        if np.sum(mask) > smoothing_window:\n",
    "            smooth_diffs = moving_average(diffs[mask], smoothing_window)\n",
    "            smooth_steps = steps[mask][:len(smooth_diffs)]\n",
    "            plt.plot(\n",
    "                smooth_steps, smooth_diffs,\n",
    "                label=\"No-op Action\",\n",
    "                linestyle=':',\n",
    "                color='black',\n",
    "                alpha=0.7,\n",
    "                linewidth=2\n",
    "            )\n",
    "\n",
    "    plt.xlabel(f\"{method_name} Updates\")\n",
    "    plt.ylabel(\"Per-goal Q-value Difference (smoothed)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"{method_name}: Per-Goal Q-value Convergence + No-Op (smoothed)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot EDR Evolution + Jain's Fairness ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for g in goal_edges:\n",
    "        plt.plot(edr_steps, edr_hist[g], label=f\"EDR {g}\")\n",
    "    plt.plot(edr_steps, fairness_history, label=\"Jain's Fairness\", linestyle=\"--\", color=\"black\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"EDR Estimate / Jain's Fairness\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"{method_name}: EDR + Fairness Over Training\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Swapped Edge Creation Frequency ---\n",
    "    if edge_creation_counter is not None:\n",
    "        edges = list(edge_creation_counter.keys())\n",
    "        counts = list(edge_creation_counter.values())\n",
    "        edge_labels = [f\"{u}-{v}\" for u, v in edges]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(edge_labels, counts)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"Number of Swapped Creations\")\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.title(f\"{method_name}: Edge Creation Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d12b6",
   "metadata": {},
   "source": [
    "#  **Q-LEARNING CODE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f82aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_q_learning_linear_policy(\n",
    "    initialEdges, goalEdges, totalSteps,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\",\n",
    "    noop_penalty=0.00, log_interval=1000,\n",
    "    softmax=True, temperature=1.0, temperature_decay=0.9999,\n",
    "    epsilon=0.01, nestedSwaps=False\n",
    "):\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    # --- Build Master Edge List ---\n",
    "    nodes = set()\n",
    "    for u, v in initialEdges:\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    nodes = sorted(list(nodes))\n",
    "\n",
    "    master_edge_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "    feature_size = len(master_edge_list) + len(goalEdges)\n",
    "    Q = LinearQApproximator(feature_size=feature_size)\n",
    "    temperature_curr = temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    fairness_history = []\n",
    "    edge_creation_counter = Counter()\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "    def select_action(state, temperature):\n",
    "        feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges)\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        if softmax:\n",
    "            q_vals = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "            scaled_qs = q_vals / max(temperature, 1e-6)\n",
    "            exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "            probs = exp_qs / np.sum(exp_qs)\n",
    "            chosen = acts[np.random.choice(len(acts), p=probs)]\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                chosen = random.choice(acts)\n",
    "            else:\n",
    "                chosen = max(acts, key=lambda a: Q.get_q_value(feats, a))\n",
    "\n",
    "        return chosen\n",
    "\n",
    "    state = current\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        temperature_curr = max(0.01, temperature_curr * temperature_decay)\n",
    "\n",
    "        action = select_action(state, temperature_curr)\n",
    "        next_state = performAction(action, state, pSwap=pSwap, nestedSwaps=nestedSwaps)\n",
    "        # Track newly created swapped edges (not initial ones)\n",
    "        new_edges = next_state[0]  # list of (edge, age)\n",
    "        for (u, v), age in new_edges:\n",
    "            if (u, v) not in initialEdges and (v, u) not in initialEdges:\n",
    "                edge_creation_counter[(u, v)] += 1\n",
    "\n",
    "        \n",
    "        next_state = ageEntanglements(next_state, maxAge)\n",
    "        next_state = generateEntanglement(next_state, pGen, initialEdges)\n",
    "\n",
    "        r, succ = compute_reward(\n",
    "            action, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, noop_penalty=noop_penalty\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = action\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        augmented_next_state = (next_state[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "            fairness = jains_index(edr_snap)                \n",
    "            fairness_history.append(fairness)\n",
    "\n",
    "        feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "        feats_next = featurize_state(augmented_next_state, goalEdges, master_edge_list)\n",
    "        next_actions = get_possible_multi_actions(augmented_next_state[0], goalEdges)\n",
    "        max_q_next = max([Q.get_q_value(feats_next, a) for a in next_actions], default=0.0)\n",
    "\n",
    "        target = r + gamma * max_q_next\n",
    "        current_q = Q.get_q_value(feats, action)\n",
    "        diff = abs(current_q - target)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        for g in goalEdges:\n",
    "            if action[1] is not None and g in action[1]:\n",
    "                q_value_diffs_per_goal[g].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal[g].append(np.nan)\n",
    "\n",
    "        Q.update(feats, action, target, alpha)\n",
    "        state = augmented_next_state\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history, fairness_history, edge_creation_counter\n",
    "\n",
    "def train_q_learning_linear_policy(\n",
    "    edges, goal_edges, pSwap, pGen, max_age,\n",
    "    seed, totalSteps, alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    softmax, temperature, temperature_decay,\n",
    "    nestedSwaps, noop_penalty=0.0, plotTraining=False\n",
    "\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    log_interval = edr_window_size\n",
    "\n",
    "    result = run_q_learning_linear_policy(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=pGen,\n",
    "        pSwap=pSwap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        softmax=softmax,\n",
    "        temperature=temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        nestedSwaps=nestedSwaps\n",
    "    )\n",
    "\n",
    "    if result is None:\n",
    "        print(\"Error: Q-learning returned None.\")\n",
    "        return\n",
    "\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist, fairness_history, edge_creation_counter = result\n",
    "\n",
    "\n",
    "    if plotTraining:\n",
    "        plot_training_results(\n",
    "        q_diffs,\n",
    "        q_diffs_per_goal,\n",
    "        edr_steps,\n",
    "        edr_hist,\n",
    "        goal_edges,\n",
    "        fairness_history,      # <<< ADD THIS\n",
    "        edge_creation_counter,\n",
    "        method_name=\"Q-Learning\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7e7f7",
   "metadata": {},
   "source": [
    "#  **SARSA CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c42baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_step_sarsa_linear_multi(\n",
    "    initialEdges, goalEdges, totalSteps, nLookahead,\n",
    "    gamma, alpha, pGen, pSwap, maxAge,\n",
    "    edr_window_size=100, reward_mode=\"basic\",\n",
    "    noop_penalty=0.00, log_interval=1000,\n",
    "    initial_temperature=1.0, temperature_decay=0.9999, nestedSwaps=False\n",
    "):\n",
    "    # --- Build Master Edge List ---\n",
    "    nodes = set()\n",
    "    for u, v in initialEdges:\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    nodes = sorted(list(nodes))\n",
    "\n",
    "    master_edge_list = []\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "    feature_size = len(master_edge_list) + len(goalEdges)\n",
    "    Q = LinearQApproximator(feature_size=feature_size)\n",
    "\n",
    "    q_value_diffs = []\n",
    "    q_value_diffs_per_goal = {g: [] for g in goalEdges}\n",
    "    q_value_diffs_per_goal['noop'] = []  # Add No-op as a \"goal\" for tracking\n",
    "\n",
    "    edge_creation_counter = Counter()\n",
    "\n",
    "\n",
    "    goal_success_queues = {\n",
    "        g: deque([1] * (edr_window_size // 2) + [0] * (edr_window_size // 2), maxlen=edr_window_size)\n",
    "        for g in goalEdges\n",
    "    }\n",
    "\n",
    "    raw = [(e, -1) for e in initialEdges]\n",
    "    edr_snap = {g: 0.0 for g in goalEdges}\n",
    "    current = (tuple(raw), tuple(edr_snap[g] for g in goalEdges))\n",
    "\n",
    "    state_buffer = deque([current])\n",
    "    reward_buffer = deque()\n",
    "    temperature = initial_temperature\n",
    "\n",
    "    edr_tracking_steps = []\n",
    "    fairness_history = []\n",
    "    edr_tracking_history = {g: [] for g in goalEdges}\n",
    "\n",
    "\n",
    "    # --- Updated select_action ---\n",
    "    def select_action(state, temperature):\n",
    "        feats = featurize_state(state, goalEdges, master_edge_list)\n",
    "        acts = get_possible_multi_actions(state[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        q_values = np.array([Q.get_q_value(feats, a) for a in acts], dtype=np.float64)\n",
    "        scaled_qs = q_values / max(temperature, 1e-6)\n",
    "        exp_qs = np.exp(scaled_qs - np.max(scaled_qs))\n",
    "        probs = exp_qs / np.sum(exp_qs)\n",
    "\n",
    "        idx = np.random.choice(len(acts), p=probs)\n",
    "        chosen = acts[idx]\n",
    "        \n",
    "        return chosen\n",
    "\n",
    "    # --- Start Training Loop ---\n",
    "    action_buffer = deque([select_action(current, temperature)])\n",
    "\n",
    "    for t in range(totalSteps):\n",
    "        if (t+1) % 100_000 == 0:\n",
    "            print(f\"Step {t+1}\")\n",
    "\n",
    "        temperature = max(0.01, temperature * temperature_decay)\n",
    "        S_t = state_buffer[-1]\n",
    "        A_t = action_buffer[-1]\n",
    "        \n",
    "        # --- Debug print when exactly 3 actions are available ---\n",
    "        \n",
    "        acts = get_possible_multi_actions(S_t[0], goalEdges, nestedSwaps=nestedSwaps)\n",
    "        if ([], None) not in acts:\n",
    "            acts.append(([], None))\n",
    "\n",
    "        # if len(acts) == 3:\n",
    "        #     feats = featurize_state(S_t, goalEdges, master_edge_list)\n",
    "            \n",
    "        #     print(f\"\\n[DEBUG] Step {t} - 3 Actions available:\")\n",
    "\n",
    "        #     all_qs = []\n",
    "        #     for a in acts:\n",
    "        #         q_val = Q.get_q_value(feats, a)\n",
    "        #         all_qs.append((a, q_val))\n",
    "            \n",
    "        #     # Sort actions by Q descending for easier reading\n",
    "        #     all_qs = sorted(all_qs, key=lambda x: -x[1])\n",
    "\n",
    "        #     for i, (a, q) in enumerate(all_qs):\n",
    "        #         print(f\"  Action {i+1}: {a}, Q={q:.4f}\")\n",
    "\n",
    "        #     chosen_action = A_t\n",
    "        #     chosen_q = Q.get_q_value(feats, chosen_action)\n",
    "        #     print(f\"--> CHOSEN action: {chosen_action} | Q={chosen_q:.4f}\")\n",
    "\n",
    "\n",
    "        ns = performAction(A_t, S_t, pSwap=pSwap, nestedSwaps=nestedSwaps, system_goals=goalEdges)\n",
    "        # Track newly created swapped edges (not original edges)\n",
    "        new_edges = ns[0]  # list of (edge, age)\n",
    "        for (u, v), age in new_edges:\n",
    "            if (u, v) not in initialEdges and (v, u) not in initialEdges:\n",
    "                edge_creation_counter[(u, v)] += 1\n",
    "\n",
    "\n",
    "\n",
    "        ns = ageEntanglements(ns, maxAge)\n",
    "        ns = generateEntanglement(ns, pGen, initial_edges=initialEdges)\n",
    "        r, succ = compute_reward(\n",
    "            A_t, goal_success_queues, pSwap,\n",
    "            mode=reward_mode, noop_penalty=noop_penalty\n",
    "        )\n",
    "\n",
    "        consumed_edges, goal_list = A_t\n",
    "        successful_goals = set(goal_list) if succ else set()\n",
    "        \n",
    "        for gh in goalEdges:\n",
    "            goal_success_queues[gh].append(1 if gh in successful_goals else 0)\n",
    "\n",
    "        reward_buffer.append(r)\n",
    "\n",
    "        edr_snap = {g: sum(goal_success_queues[g]) / len(goal_success_queues[g]) for g in goalEdges}\n",
    "        next_state = (ns[0], tuple(edr_snap[g] for g in goalEdges))\n",
    "        if t % log_interval == 0:\n",
    "            edr_tracking_steps.append(t)\n",
    "            for g in goalEdges:\n",
    "                edr_tracking_history[g].append(edr_snap[g])\n",
    "                # Compute Jain's Index at each log point\n",
    "            fairness = jains_index(edr_snap)\n",
    "            fairness_history.append(fairness)\n",
    "\n",
    "\n",
    "        A_next = select_action(next_state, temperature)\n",
    "\n",
    "        state_buffer.append(next_state)\n",
    "        action_buffer.append(A_next)\n",
    "\n",
    "        if len(reward_buffer) >= nLookahead:\n",
    "            G = sum((gamma**i) * reward_buffer[i] for i in range(nLookahead))\n",
    "            s_n = state_buffer[nLookahead]\n",
    "            a_n = action_buffer[nLookahead]\n",
    "            feats_n = featurize_state(s_n, goalEdges, master_edge_list)\n",
    "            G += (gamma**nLookahead) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "            s_tau = state_buffer[0]\n",
    "            a_tau = action_buffer[0]\n",
    "            feats_tau = featurize_state(s_tau, goalEdges, master_edge_list)\n",
    "            old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "            diff = abs(G - old_q)\n",
    "            q_value_diffs.append(diff)\n",
    "            \n",
    "            \n",
    "            for gg in goalEdges:\n",
    "                if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                    q_value_diffs_per_goal[gg].append(diff)\n",
    "                else:\n",
    "                    q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "            # Special handling for no-op\n",
    "            if a_tau[1] is None:  # If it's no-op action\n",
    "                q_value_diffs_per_goal['noop'].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal['noop'].append(float('nan'))\n",
    "\n",
    "\n",
    "            Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "            state_buffer.popleft()\n",
    "            action_buffer.popleft()\n",
    "            reward_buffer.popleft()\n",
    "\n",
    "    while reward_buffer:\n",
    "        n = len(reward_buffer)\n",
    "        G = sum((gamma**i) * reward_buffer[i] for i in range(n))\n",
    "        if n < len(state_buffer):\n",
    "            s_n = state_buffer[n]\n",
    "            a_n = action_buffer[n]\n",
    "            feats_n = featurize_state(s_n, goalEdges, master_edge_list)\n",
    "            G += (gamma**n) * Q.get_q_value(feats_n, a_n)\n",
    "\n",
    "        s_tau = state_buffer[0]\n",
    "        a_tau = action_buffer[0]\n",
    "        feats_tau = featurize_state(s_tau, goalEdges, master_edge_list)\n",
    "        old_q = Q.get_q_value(feats_tau, a_tau)\n",
    "        diff = abs(G - old_q)\n",
    "        q_value_diffs.append(diff)\n",
    "\n",
    "        for gg in goalEdges:\n",
    "            if a_tau[1] is not None and gg in a_tau[1]:\n",
    "                q_value_diffs_per_goal[gg].append(diff)\n",
    "            else:\n",
    "                q_value_diffs_per_goal[gg].append(float('nan'))\n",
    "\n",
    "        Q.update(feats_tau, a_tau, G, alpha)\n",
    "\n",
    "        state_buffer.popleft()\n",
    "        action_buffer.popleft()\n",
    "        reward_buffer.popleft()\n",
    "\n",
    "    return Q, q_value_diffs, q_value_diffs_per_goal, edr_tracking_steps, edr_tracking_history, fairness_history, edge_creation_counter\n",
    "\n",
    "\n",
    "def train_sarsa_linear_policy(\n",
    "    edges, goal_edges, pSwap, pGen, max_age,\n",
    "    seed, totalSteps, nLookahead,\n",
    "    alpha, gamma,\n",
    "    edr_window_size, reward_mode,\n",
    "    log_interval,\n",
    "    initial_temperature, temperature_decay, nestedSwaps,\n",
    "    noop_penalty=0.0, plotTraining=False\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    log_interval = edr_window_size\n",
    "\n",
    "    result = run_n_step_sarsa_linear_multi(\n",
    "        initialEdges=edges,\n",
    "        goalEdges=goal_edges,\n",
    "        totalSteps=totalSteps,\n",
    "        nLookahead=nLookahead,\n",
    "        gamma=gamma,\n",
    "        alpha=alpha,\n",
    "        pGen=pGen,\n",
    "        pSwap=pSwap,\n",
    "        maxAge=max_age,\n",
    "        edr_window_size=edr_window_size,\n",
    "        reward_mode=reward_mode,\n",
    "        noop_penalty=noop_penalty,\n",
    "        log_interval=log_interval,\n",
    "        initial_temperature=initial_temperature,\n",
    "        temperature_decay=temperature_decay,\n",
    "        nestedSwaps=nestedSwaps\n",
    "    )\n",
    "\n",
    "    if result is None:\n",
    "        print(\"Error: run_n_step_sarsa_linear_multi returned None.\")\n",
    "        return\n",
    "\n",
    "    Q, q_diffs, q_diffs_per_goal, edr_steps, edr_hist, fairness_history, edge_creation_counter = result\n",
    "\n",
    "    if plotTraining:\n",
    "        plot_training_results(\n",
    "            q_diffs, q_diffs_per_goal, edr_steps, edr_hist,\n",
    "            goal_edges, fairness_history, edge_creation_counter,\n",
    "            method_name=\"SARSA\"\n",
    "        )\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7d281",
   "metadata": {},
   "source": [
    "# **RUNNING CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b821668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100000\n",
      "Step 200000\n",
      "Step 300000\n",
      "Step 400000\n",
      "Step 500000\n",
      "Step 600000\n",
      "Step 700000\n",
      "Step 800000\n",
      "Step 900000\n",
      "Step 1000000\n",
      "Step 1100000\n",
      "Step 1200000\n",
      "Step 1300000\n",
      "Step 1400000\n",
      "Step 1500000\n",
      "Step 1600000\n",
      "Step 1700000\n"
     ]
    }
   ],
   "source": [
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "#####\n",
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "\n",
    "edges = [(0,1), (1,2), (2,3), (3,4)]\n",
    "goal_edges = [(1,4),(0,2)]\n",
    "pSwap       = 0.7\n",
    "pGen        = 0.7\n",
    "maxAge      =  3\n",
    "totalSteps     = 4_000_000\n",
    "nestedSwap = False\n",
    "#####\n",
    "nLookahead     = 5\n",
    "gamma          = 0.995\n",
    "alpha          = 0.01\n",
    "windowSize     = 1000\n",
    "reward_mode    = 'basic'\n",
    "initial_temperature = 6.0\n",
    "final_temperature   = 0.2\n",
    "temperature_decay   = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 0.99))\n",
    "temperature_decay = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 1.2))\n",
    "\n",
    "\n",
    "\n",
    "# --- Train N-step SARSA policy (with temperature-based softmax) ---\n",
    "Q1 = train_sarsa_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    noop_penalty=0.0,\n",
    "    log_interval=1000,\n",
    "    initial_temperature=initial_temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    nestedSwaps=nestedSwap,\n",
    "    plotTraining=True\n",
    ")\n",
    "print('done training')\n",
    "simulate_policy(\n",
    "    Q_table=Q1,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=100_000,\n",
    "    edr_window_size=windowSize,\n",
    "    plot=True,\n",
    "    nestedSwaps=nestedSwap\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317a3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Q-values per goal after training:\n",
      "Goal (0, 4): Best Q = 89.1089\n",
      "Goal (0, 2): Best Q = 91.3557\n"
     ]
    }
   ],
   "source": [
    "# --- Print top Q-values for each goal after training ---\n",
    "print(\"\\nTop Q-values per goal after training:\")\n",
    "\n",
    "master_edge_list = []\n",
    "nodes = set()\n",
    "for u, v in edges:\n",
    "    nodes.add(u)\n",
    "    nodes.add(v)\n",
    "nodes = sorted(list(nodes))\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(i+1, len(nodes)):\n",
    "        master_edge_list.append((nodes[i], nodes[j]))\n",
    "\n",
    "# Create an artificial \"perfect state\" where all initial edges are alive\n",
    "ent_state = [(edge, 1) for edge in edges]\n",
    "edr_vector = tuple(0.1 for _ in goal_edges)  # dummy EDRs\n",
    "state = (tuple(ent_state), edr_vector)\n",
    "\n",
    "feats = featurize_state(state, goal_edges, master_edge_list)\n",
    "acts_all = get_possible_multi_actions(ent_state, goal_edges, nestedSwaps=nestedSwap)\n",
    "\n",
    "goal_qs = {g: [] for g in goal_edges}\n",
    "for a in acts_all:\n",
    "    consumed_paths, goals = a\n",
    "    if goals is not None:\n",
    "        for g in goals:\n",
    "            goal_qs[g].append(Q1.get_q_value(feats, a))\n",
    "\n",
    "for g in goal_edges:\n",
    "    if goal_qs[g]:\n",
    "        print(f\"Goal {g}: Best Q = {max(goal_qs[g]):.4f}\")\n",
    "    else:\n",
    "        print(f\"Goal {g}: No available actions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8398a00e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_q_learning_linear_policy() got an unexpected keyword argument 'epsilon'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m random.seed(seed)\n\u001b[32m     23\u001b[39m np.random.seed(seed)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m Q = \u001b[43mtrain_q_learning_linear_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoal_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpSwap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpGen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_age\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxAge\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotalSteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43medr_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindowSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoop_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_temperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnestedSwaps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnestedSwap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplotTraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdone training\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# --- Evaluate learned policy ---\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: train_q_learning_linear_policy() got an unexpected keyword argument 'epsilon'"
     ]
    }
   ],
   "source": [
    "edges = [(0,1), (1,2), (2,3), (3,4), (2,5)]\n",
    "goal_edges = [(0,4), (3,5)]\n",
    "pSwap       = 0.6\n",
    "pGen        = 0.6\n",
    "maxAge      =  3\n",
    "totalSteps     = 4_000_000\n",
    "nestedSwap = False\n",
    "\n",
    "\n",
    "gamma          = 0.99\n",
    "alpha          = 0.01\n",
    "windowSize     = 1000\n",
    "reward_mode    = 'basic'\n",
    "softmax= True\n",
    "# Softmax temperature parameters\n",
    "initial_temperature = 6.0\n",
    "final_temperature   = 0.1\n",
    "temperature_decay   = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 0.9))\n",
    "\n",
    "# Seed\n",
    "seed = 30\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "Q = train_q_learning_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    noop_penalty=0.0,          \n",
    "    softmax=softmax,\n",
    "    temperature=initial_temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    epsilon=0.05,       \n",
    "    nestedSwaps=nestedSwap,\n",
    "    plotTraining=True\n",
    ")\n",
    "\n",
    "    \n",
    "print('done training')\n",
    "# --- Evaluate learned policy ---\n",
    "simulate_policy(\n",
    "    Q_table=Q,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=100_000,\n",
    "    edr_window_size=windowSize,\n",
    "    plot=True,\n",
    "    nestedSwaps=nestedSwap\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43791e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nestedSwap = True\n",
    "\n",
    "# --- Train N-step SARSA policy (with temperature-based softmax) ---\n",
    "Q1 = train_sarsa_linear_policy(\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    seed=seed,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=nLookahead,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    edr_window_size=windowSize,\n",
    "    reward_mode=reward_mode,\n",
    "    noop_penalty=0.0,\n",
    "    log_interval=1000,\n",
    "    initial_temperature=initial_temperature,\n",
    "    temperature_decay=temperature_decay,\n",
    "    nestedSwaps=nestedSwap,\n",
    "    plotTraining=True\n",
    ")\n",
    "print('done training')\n",
    "# --- Evaluate learned policy ---\n",
    "simulate_policy(\n",
    "    Q_table=Q1,\n",
    "    edges=edges,\n",
    "    goal_edges=goal_edges,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    max_age=maxAge,\n",
    "    num_steps=100_000,\n",
    "    edr_window_size=windowSize,\n",
    "    plot=True,\n",
    "    nestedSwaps=nestedSwap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ba000",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramValues = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "totalSteps = 500_000\n",
    "seed = 11\n",
    "final_temperature= 0.1\n",
    "initial_temperature = 5\n",
    "temperature_decay   = (final_temperature / initial_temperature) ** (1.0 / (totalSteps * 0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareOverParamRobust(\n",
    "    param_name='pGen',\n",
    "    param_values=paramValues,\n",
    "    edges=[(0,1), (1,2), (2,3), (3,4), (0,5), (5,3)],\n",
    "    goal_edges=[(0,4), (1,3)],\n",
    "    pSwap=0.6,\n",
    "    pGen=0.6,\n",
    "    max_age=5,\n",
    "    totalSteps=totalSteps,\n",
    "    nLookahead=3,\n",
    "    alpha=0.01,\n",
    "    gamma=0.99,\n",
    "    edr_window_size=1000,\n",
    "    reward_mode='basic',\n",
    "    initial_temperature=5.0,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=30,\n",
    "    nestedSwaps=False,\n",
    "    training_function=train_q_learning_linear_policy,\n",
    "    softmax=True,          # passed into training function\n",
    "    temperature=5.0,       # passed into training function\n",
    "    epsilon=0.01,          # passed into training function\n",
    "    trainCount=3,          # new robust arg\n",
    "    simulateCount=3        # new robust arg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compareOverParamRobust(\n",
    "    param_name='pGen',\n",
    "    param_values=paramValues,\n",
    "    edges=[(0,1), (1,2), (2,3), (3,4), (0,5), (5,3)],\n",
    "    goal_edges=[(0,4), (1,3)],\n",
    "    pSwap=0.6,\n",
    "    pGen=0.6,\n",
    "    max_age=5,\n",
    "    totalSteps=500000,\n",
    "    nLookahead=3,\n",
    "    alpha=0.01,\n",
    "    gamma=0.99,\n",
    "    edr_window_size=1000,\n",
    "    reward_mode='basic',\n",
    "    initial_temperature=5.0,\n",
    "    temperature_decay=temperature_decay,\n",
    "    seed=30,\n",
    "    nestedSwaps=False,\n",
    "    training_function=train_sarsa_linear_policy,\n",
    "    trainCount=2,          # new robust arg\n",
    "    simulateCount=2        # new robust arg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3df0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb17739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5813da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
