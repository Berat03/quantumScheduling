{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from itertools import product, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPossibleStates(edges, max_age):\n",
    "    \"\"\"\n",
    "    Returns: list of states, where each state is a tuple of ((edge), age)\n",
    "    \"\"\"\n",
    "    sorted_edges = sorted(tuple(sorted(e)) for e in edges)\n",
    "    possible_ages = [-1] + list(range(1, max_age + 1))\n",
    "\n",
    "    all_states = [\n",
    "        tuple(zip(sorted_edges, age_combo))\n",
    "        for age_combo in product(possible_ages, repeat=len(sorted_edges))\n",
    "    ]\n",
    "\n",
    "    return all_states\n",
    "\n",
    "def getAgedStates(state, maxAge):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        New state as a tuple of ((edge), age) pairs\n",
    "    \"\"\"\n",
    "    new_state = []\n",
    "    for edge, age in state:\n",
    "        if age == -1:\n",
    "            new_state.append((edge, -1))  # not entangled\n",
    "        else:\n",
    "            new_age = age + 1\n",
    "            if new_age <= maxAge:\n",
    "                new_state.append((edge, new_age))\n",
    "            else:\n",
    "                new_state.append((edge, -1))  # entanglement expired\n",
    "\n",
    "    return tuple(sorted(new_state))\n",
    "\n",
    "\n",
    "def generateAllOutcomes(state, pGen):\n",
    "    empty_edges = [edge for edge, age in state if age == -1]\n",
    "    outcomes = []\n",
    "\n",
    "    for pattern in product([0, 1], repeat=len(empty_edges)):\n",
    "        prob = 1.0\n",
    "        new_state = []\n",
    "\n",
    "        # First, update the empty edges based on entanglement outcomes\n",
    "        outcome_map = {}\n",
    "        for (edge, outcome) in zip(empty_edges, pattern):\n",
    "            if outcome:\n",
    "                prob *= pGen\n",
    "                outcome_map[edge] = 1\n",
    "            else:\n",
    "                prob *= (1 - pGen)\n",
    "                outcome_map[edge] = -1\n",
    "\n",
    "        # Now build the full new state\n",
    "        for edge, age in state:\n",
    "            if edge in outcome_map:\n",
    "                new_state.append((edge, outcome_map[edge]))\n",
    "            else:\n",
    "                new_state.append((edge, age))  # unchanged\n",
    "\n",
    "        outcomes.append((tuple(sorted(new_state)), prob))\n",
    "\n",
    "    return outcomes\n",
    "\n",
    "def generateAllSwappingOutcomes(state, goalEdges, pSwap):\n",
    "    \"\"\"\n",
    "    Generate all possible outcomes from swapping operations or doing nothing.\n",
    "    Can attempt multiple non-overlapping swaps simultaneously.\n",
    "    \"\"\"\n",
    "    def find_path(current, target, visited):\n",
    "        if current == target:\n",
    "            return [current]\n",
    "        visited.add(current)\n",
    "        for next_node in graph.get(current, []):\n",
    "            if next_node not in visited:\n",
    "                path = find_path(next_node, target, visited)\n",
    "                if path:\n",
    "                    return [current] + path\n",
    "        return None\n",
    "\n",
    "    # First find all possible individual swap attempts\n",
    "    swap_attempts = []\n",
    "    entangled_edges = [(edge, age) for edge, age in state if age > 0]\n",
    "    \n",
    "    # Create graph once for all path finding\n",
    "    graph = {}\n",
    "    for (edge, _) in entangled_edges:\n",
    "        if edge[0] not in graph: graph[edge[0]] = []\n",
    "        if edge[1] not in graph: graph[edge[1]] = []\n",
    "        graph[edge[0]].append(edge[1])\n",
    "        graph[edge[1]].append(edge[0])\n",
    "    \n",
    "    # Find all possible paths for each goal\n",
    "    for goal_edge in goalEdges:\n",
    "        start, end = goal_edge\n",
    "        path = find_path(start, end, set())\n",
    "        \n",
    "        if path and len(path) > 1:\n",
    "            path_edges = list(zip(path[:-1], path[1:]))\n",
    "            used_edges = []\n",
    "            for p_edge in path_edges:\n",
    "                for e, age in entangled_edges:\n",
    "                    if (e[0] == p_edge[0] and e[1] == p_edge[1]) or \\\n",
    "                       (e[0] == p_edge[1] and e[1] == p_edge[0]):\n",
    "                        used_edges.append((e, age))\n",
    "            \n",
    "            swap_attempts.append({\n",
    "                'goal': goal_edge,\n",
    "                'used_edges': used_edges,\n",
    "                'num_swaps': len(path_edges) - 1\n",
    "            })\n",
    "    \n",
    "    outcomes = []\n",
    "    \n",
    "    # Case 1: Do nothing\n",
    "    outcomes.append((state, 1.0, None))\n",
    "    \n",
    "    # Case 2: Try all possible combinations of non-overlapping swaps\n",
    "    for r in range(1, len(swap_attempts) + 1):\n",
    "        for attempt_combo in combinations(swap_attempts, r):\n",
    "            # Check if attempts are compatible (don't share edges)\n",
    "            all_used_edges = set()\n",
    "            edge_overlap = False\n",
    "            for attempt in attempt_combo:\n",
    "                attempt_edges = set(e for e, _ in attempt['used_edges'])\n",
    "                if any(e in all_used_edges for e in attempt_edges):\n",
    "                    edge_overlap = True\n",
    "                    break\n",
    "                all_used_edges.update(attempt_edges)\n",
    "            \n",
    "            if edge_overlap:\n",
    "                continue\n",
    "                \n",
    "            # Generate all success/failure combinations for this set of attempts\n",
    "            for success_pattern in product([True, False], repeat=len(attempt_combo)):\n",
    "                new_state = []\n",
    "                total_prob = 1.0\n",
    "                achieved_goals = []\n",
    "                \n",
    "                # Calculate probability and track achieved goals\n",
    "                for attempt, succeeded in zip(attempt_combo, success_pattern):\n",
    "                    prob = pSwap ** attempt['num_swaps'] if succeeded else \\\n",
    "                          (1 - pSwap ** attempt['num_swaps'])\n",
    "                    total_prob *= prob\n",
    "                    if succeeded:\n",
    "                        achieved_goals.append((attempt['goal'], True))\n",
    "                    else:\n",
    "                        achieved_goals.append((attempt['goal'], False))\n",
    "                \n",
    "                # Create new state\n",
    "                for edge, age in state:\n",
    "                    if any(edge == e for attempt in attempt_combo \n",
    "                          for e, _ in attempt['used_edges']):\n",
    "                        new_state.append((edge, -1))  # Used edges become unentangled\n",
    "                    else:\n",
    "                        new_state.append((edge, age))  # Other edges remain unchanged\n",
    "                \n",
    "                new_state = tuple(sorted(new_state))\n",
    "                outcomes.append((new_state, total_prob, achieved_goals))\n",
    "    \n",
    "    return outcomes\n",
    "\n",
    "def getAllTransitionProbabilities(state, goalEdges, pSwap, pGen, maxAge):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        List of (new_state, probability, achieved_goals) tuples\n",
    "    \"\"\"\n",
    "    all_transitions = []\n",
    "    \n",
    "    # Get all possible swap action outcomes\n",
    "    swap_outcomes = generateAllSwappingOutcomes(state, goalEdges, pSwap)\n",
    "    \n",
    "    # For each swap outcome, generate all possible generation outcomes\n",
    "    for swap_state, swap_prob, achieved_goals in swap_outcomes:\n",
    "        # Skip if swap probability is 0\n",
    "        if swap_prob == 0:\n",
    "            continue\n",
    "            \n",
    "        # First age the state\n",
    "        aged_state = getAgedStates(swap_state, maxAge)\n",
    "        \n",
    "        # Then get all possible generation outcomes\n",
    "        gen_outcomes = generateAllOutcomes(aged_state, pGen)\n",
    "        \n",
    "        # Combine the probabilities and add to transitions\n",
    "        for final_state, gen_prob in gen_outcomes:\n",
    "            # Skip if either probability is 0\n",
    "            if gen_prob == 0:\n",
    "                continue\n",
    "                \n",
    "            all_transitions.append((\n",
    "                final_state,\n",
    "                swap_prob * gen_prob,\n",
    "                achieved_goals\n",
    "            ))\n",
    "    \n",
    "    return all_transitions\n",
    "\n",
    "def generateAllStateTransitions(edges, goalEdges, pSwap, pGen, maxAge):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Dictionary mapping state -> list of (next_state, prob, achieved_goals)\n",
    "    \"\"\"\n",
    "    # Get all possible states\n",
    "    all_states = getPossibleStates(edges, maxAge)\n",
    "    transitions = {}\n",
    "    \n",
    "    # For each state, calculate all possible transitions\n",
    "    for state in all_states:\n",
    "        transitions[state] = getAllTransitionProbabilities(state, goalEdges, pSwap, pGen, maxAge)\n",
    "    \n",
    "    return transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100\n",
      "episode 200\n",
      "episode 300\n",
      "episode 400\n",
      "episode 500\n",
      "episode 600\n",
      "episode 700\n",
      "episode 800\n",
      "episode 900\n",
      "episode 1000\n",
      "episode 1100\n",
      "episode 1200\n",
      "episode 1300\n",
      "episode 1400\n",
      "episode 1500\n",
      "episode 1600\n",
      "episode 1700\n",
      "episode 1800\n",
      "episode 1900\n",
      "episode 2000\n",
      "episode 2100\n",
      "episode 2200\n",
      "episode 2300\n",
      "episode 2400\n",
      "episode 2500\n",
      "episode 2600\n",
      "episode 2700\n",
      "episode 2800\n",
      "episode 2900\n",
      "episode 3000\n",
      "episode 3100\n",
      "episode 3200\n",
      "episode 3300\n",
      "episode 3400\n",
      "episode 3500\n",
      "episode 3600\n",
      "episode 3700\n",
      "episode 3800\n",
      "episode 3900\n",
      "episode 4000\n",
      "episode 4100\n",
      "episode 4200\n",
      "episode 4300\n",
      "episode 4400\n",
      "episode 4500\n",
      "episode 4600\n",
      "episode 4700\n",
      "episode 4800\n",
      "episode 4900\n",
      "episode 5000\n",
      "episode 5100\n",
      "episode 5200\n",
      "episode 5300\n",
      "episode 5400\n",
      "episode 5500\n",
      "episode 5600\n",
      "episode 5700\n",
      "episode 5800\n",
      "episode 5900\n",
      "episode 6000\n",
      "episode 6100\n",
      "episode 6200\n",
      "episode 6300\n",
      "episode 6400\n",
      "episode 6500\n",
      "episode 6600\n",
      "episode 6700\n",
      "episode 6800\n",
      "episode 6900\n",
      "episode 7000\n",
      "episode 7100\n",
      "episode 7200\n",
      "episode 7300\n",
      "episode 7400\n",
      "episode 7500\n",
      "episode 7600\n",
      "episode 7700\n",
      "episode 7800\n",
      "episode 7900\n",
      "episode 8000\n",
      "episode 8100\n",
      "episode 8200\n",
      "episode 8300\n",
      "episode 8400\n",
      "episode 8500\n",
      "episode 8600\n",
      "episode 8700\n",
      "episode 8800\n",
      "episode 8900\n",
      "episode 9000\n",
      "episode 9100\n",
      "episode 9200\n",
      "episode 9300\n",
      "episode 9400\n",
      "episode 9500\n",
      "episode 9600\n",
      "episode 9700\n",
      "episode 9800\n",
      "episode 9900\n",
      "episode 10000\n",
      "episode 10100\n",
      "episode 10200\n",
      "episode 10300\n",
      "episode 10400\n",
      "episode 10500\n",
      "episode 10600\n",
      "episode 10700\n",
      "episode 10800\n",
      "episode 10900\n",
      "episode 11000\n",
      "episode 11100\n",
      "episode 11200\n",
      "episode 11300\n",
      "episode 11400\n",
      "episode 11500\n",
      "episode 11600\n",
      "episode 11700\n",
      "episode 11800\n",
      "episode 11900\n",
      "episode 12000\n",
      "episode 12100\n",
      "episode 12200\n",
      "episode 12300\n",
      "episode 12400\n",
      "episode 12500\n",
      "episode 12600\n",
      "episode 12700\n",
      "episode 12800\n",
      "episode 12900\n",
      "episode 13000\n",
      "episode 13100\n",
      "episode 13200\n",
      "episode 13300\n",
      "episode 13400\n",
      "episode 13500\n",
      "episode 13600\n",
      "episode 13700\n",
      "episode 13800\n",
      "episode 13900\n",
      "episode 14000\n",
      "episode 14100\n",
      "episode 14200\n",
      "episode 14300\n",
      "episode 14400\n",
      "episode 14500\n",
      "episode 14600\n",
      "episode 14700\n",
      "episode 14800\n",
      "episode 14900\n",
      "episode 15000\n",
      "episode 15100\n",
      "episode 15200\n"
     ]
    }
   ],
   "source": [
    "def getImmediateReward(achieved_goals, goal_success_counts, total_timesteps):\n",
    "    if achieved_goals is None:\n",
    "        return 0\n",
    "\n",
    "    total_reward = 0\n",
    "    for goal_edge, success in achieved_goals:\n",
    "        if success:\n",
    "            start, end = goal_edge\n",
    "            num_edges = abs(end - start)\n",
    "            instant_rate = pSwap ** (num_edges - 1)\n",
    "\n",
    "            edr = max(0.0001, goal_success_counts[goal_edge] / max(1, total_timesteps))\n",
    "\n",
    "            if instant_rate > 0 and edr > 0:\n",
    "                total_reward += instant_rate / edr\n",
    "    return total_reward\n",
    "\n",
    "def calculate_fairness_metrics(edrs):\n",
    "    \"\"\"Calculate various fairness metrics for the given EDRs\"\"\"\n",
    "    if not edrs:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    values = list(edrs.values())\n",
    "    \n",
    "    # Jain's Fairness Index\n",
    "    numerator = sum(values) ** 2\n",
    "    denominator = len(values) * sum(x * x for x in values)\n",
    "    jains_index = numerator / denominator if denominator != 0 else 0\n",
    "    \n",
    "    # Min-Max Ratio\n",
    "    min_val = min(values)\n",
    "    max_val = max(values)\n",
    "    min_max_ratio = min_val / max_val if max_val != 0 else 0\n",
    "    \n",
    "    # Coefficient of Variation (CV)\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    cv = std / mean if mean != 0 else 0\n",
    "    \n",
    "    return jains_index, min_max_ratio, cv\n",
    "\n",
    "class QuantumNetworkQLearning:\n",
    "    def __init__(self, edges, goalEdges, all_transitions, pSwap, pGen, maxAge):\n",
    "        self.edges = edges\n",
    "        self.goalEdges = goalEdges\n",
    "        self.all_transitions = all_transitions\n",
    "        self.pSwap = pSwap\n",
    "        self.pGen = pGen\n",
    "        self.maxAge = maxAge\n",
    "        \n",
    "        # Q-learning parameters\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.98\n",
    "        self.epsilon = 0.3\n",
    "        \n",
    "        # Initialize Q-table using your transition structure\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.goal_success_counts = {goal: 0 for goal in goalEdges}\n",
    "        self.edr_history = {goal: [] for goal in goalEdges}\n",
    "        self.reward_history = []\n",
    "        \n",
    "        # New convergence tracking variables\n",
    "        self.episode_avg_q_values = []\n",
    "        self.episode_avg_rewards = []\n",
    "        self.episode_edrs = {goal: [] for goal in goalEdges}\n",
    "        \n",
    "        # Fairness tracking\n",
    "        self.fairness_history = {\n",
    "            'jains_index': [],\n",
    "            'min_max_ratio': [],\n",
    "            'cv': []\n",
    "        }\n",
    "        self.episode_fairness = {\n",
    "            'jains_index': [],\n",
    "            'min_max_ratio': [],\n",
    "            'cv': []\n",
    "        }\n",
    "        \n",
    "    def group_transitions_by_action(self, state_transitions):\n",
    "        \"\"\"Group transitions by the attempted goals, ignoring success/failure\"\"\"\n",
    "        action_groups = defaultdict(list)\n",
    "        \n",
    "        for next_state, prob, achieved_goals in state_transitions:\n",
    "            if achieved_goals is None:\n",
    "                action_key = None\n",
    "            else:\n",
    "                action_key = tuple(sorted(goal for goal, _ in achieved_goals))\n",
    "            action_groups[action_key].append((next_state, prob, achieved_goals))\n",
    "        \n",
    "        return action_groups\n",
    "    \n",
    "    def choose_action(self, state, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy\"\"\"\n",
    "        action_groups = self.group_transitions_by_action(self.all_transitions[state])\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Explore: random action group\n",
    "            action_keys = list(action_groups.keys())\n",
    "            chosen_key = random.choice(action_keys)\n",
    "            transitions = action_groups[chosen_key]\n",
    "        else:\n",
    "            # Exploit: best action group based on Q-values\n",
    "            best_value = float('-inf')\n",
    "            best_transitions = None\n",
    "            \n",
    "            for action_key, transitions in action_groups.items():\n",
    "                # Get average Q-value for this action group\n",
    "                q_value = np.mean([self.Q[state][next_state] for next_state, _, _ in transitions])\n",
    "                if q_value > best_value:\n",
    "                    best_value = q_value\n",
    "                    best_transitions = transitions\n",
    "            \n",
    "            transitions = best_transitions\n",
    "        \n",
    "        # Choose specific transition based on probabilities\n",
    "        probs = [t[1] for t in transitions]\n",
    "        return random.choices(transitions, weights=probs)[0]\n",
    "    \n",
    "    def train(self, num_episodes=1000, max_steps=100):\n",
    "        window_size = 100  # For moving averages\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = tuple((edge, -1) for edge in self.edges)\n",
    "            episode_reward = 0\n",
    "            total_timesteps = 1\n",
    "            episode_q_values = []\n",
    "            self.goal_success_counts = {goal: 0 for goal in self.goalEdges}\n",
    "            \n",
    "            # Reset episode fairness tracking\n",
    "            episode_fairness_values = {\n",
    "                'jains_index': [],\n",
    "                'min_max_ratio': [],\n",
    "                'cv': []\n",
    "            }\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # Choose and execute action\n",
    "                next_state, prob, achieved_goals = self.choose_action(state, training=True)\n",
    "                \n",
    "                # Update goal successes\n",
    "                if achieved_goals is not None:\n",
    "                    for goal_edge, success in achieved_goals:\n",
    "                        if success:\n",
    "                            self.goal_success_counts[goal_edge] += 1\n",
    "                \n",
    "                # Get reward\n",
    "                reward = getImmediateReward(achieved_goals, self.goal_success_counts, total_timesteps)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Q-learning update\n",
    "                old_q = self.Q[state][next_state]\n",
    "                next_q = max([self.Q[next_state][ns] for ns, _, _ in self.all_transitions[next_state]], default=0)\n",
    "                self.Q[state][next_state] = old_q + self.alpha * (reward + self.gamma * next_q - old_q)\n",
    "                \n",
    "                # Track Q-values for convergence\n",
    "                episode_q_values.append(self.Q[state][next_state])\n",
    "                \n",
    "                # Calculate current EDRs and fairness metrics\n",
    "                current_edrs = {goal: self.goal_success_counts[goal] / total_timesteps for goal in self.goalEdges}\n",
    "                jains, minmax, cv = calculate_fairness_metrics(current_edrs)\n",
    "                \n",
    "                # Store fairness metrics for this step\n",
    "                episode_fairness_values['jains_index'].append(jains)\n",
    "                episode_fairness_values['min_max_ratio'].append(minmax)\n",
    "                episode_fairness_values['cv'].append(cv)\n",
    "                \n",
    "                # Update EDR tracking\n",
    "                for goal in self.goalEdges:\n",
    "                    current_edr = self.goal_success_counts[goal] / total_timesteps\n",
    "                    if episode == num_episodes - 1:  # Only track last episode\n",
    "                        self.edr_history[goal].append(current_edr)\n",
    "                \n",
    "                # Transition\n",
    "                state = next_state\n",
    "                total_timesteps += 1\n",
    "            \n",
    "            # Store episode metrics\n",
    "            self.reward_history.append(episode_reward)\n",
    "            self.episode_avg_q_values.append(np.mean(episode_q_values))\n",
    "            \n",
    "            # Store EDRs for each goal at the end of episode\n",
    "            for goal in self.goalEdges:\n",
    "                self.episode_edrs[goal].append(self.goal_success_counts[goal] / total_timesteps)\n",
    "            \n",
    "            # Store average fairness metrics for this episode\n",
    "            self.fairness_history['jains_index'].append(np.mean(episode_fairness_values['jains_index']))\n",
    "            self.fairness_history['min_max_ratio'].append(np.mean(episode_fairness_values['min_max_ratio']))\n",
    "            self.fairness_history['cv'].append(np.mean(episode_fairness_values['cv']))\n",
    "            \n",
    "            # Calculate moving averages for reward\n",
    "            if len(self.reward_history) >= window_size:\n",
    "                avg_reward = np.mean(self.reward_history[-window_size:])\n",
    "                self.episode_avg_rewards.append(avg_reward)\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print('episode', episode + 1)\n",
    "    \n",
    "    def plot_results(self):\n",
    "        fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 18))\n",
    "        \n",
    "        # Plot 1: EDR evolution (last episode)\n",
    "        for goal, edr_values in self.edr_history.items():\n",
    "            ax1.plot(range(len(edr_values)), edr_values, label=f'Goal {goal}')\n",
    "        ax1.set_xlabel('Step')\n",
    "        ax1.set_ylabel('EDR')\n",
    "        ax1.set_title('EDR Evolution (Last Episode)')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Plot 2: Reward history\n",
    "        ax2.plot(self.reward_history, alpha=0.3, label='Raw')\n",
    "        if len(self.episode_avg_rewards) > 0:\n",
    "            ax2.plot(range(99, len(self.episode_avg_rewards) + 99), \n",
    "                    self.episode_avg_rewards, \n",
    "                    label='100-episode moving avg')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Total Reward')\n",
    "        ax2.set_title('Training Progress (Reward)')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Plot 3: Average Q-value evolution\n",
    "        ax3.plot(self.episode_avg_q_values)\n",
    "        ax3.set_xlabel('Episode')\n",
    "        ax3.set_ylabel('Average Q-value')\n",
    "        ax3.set_title('Q-value Convergence')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        # Plot 4: EDR evolution across episodes\n",
    "        for goal, edr_values in self.episode_edrs.items():\n",
    "            ax4.plot(range(len(edr_values)), edr_values, label=f'Goal {goal}')\n",
    "        ax4.set_xlabel('Episode')\n",
    "        ax4.set_ylabel('EDR')\n",
    "        ax4.set_title('EDR Evolution Across Episodes')\n",
    "        ax4.grid(True)\n",
    "        ax4.legend()\n",
    "        ax4.set_ylim(0, 1)\n",
    "        \n",
    "        # Plot 5: Jain's Fairness Index evolution\n",
    "        ax5.plot(self.fairness_history['jains_index'], label=\"Jain's Index\")\n",
    "        ax5.set_xlabel('Episode')\n",
    "        ax5.set_ylabel('Fairness Index')\n",
    "        ax5.set_title(\"Jain's Fairness Index Evolution\")\n",
    "        ax5.grid(True)\n",
    "        ax5.set_ylim(0, 1)\n",
    "        \n",
    "        # Plot 6: Other fairness metrics\n",
    "        ax6.plot(self.fairness_history['min_max_ratio'], label='Min-Max Ratio')\n",
    "        ax6.plot(self.fairness_history['cv'], label='CV')\n",
    "        ax6.set_xlabel('Episode')\n",
    "        ax6.set_ylabel('Metric Value')\n",
    "        ax6.set_title('Fairness Metrics Evolution')\n",
    "        ax6.grid(True)\n",
    "        ax6.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print convergence statistics\n",
    "        print(\"\\n=== Convergence Statistics ===\")\n",
    "        window_sizes = [100, 500, 1000]\n",
    "        for window in window_sizes:\n",
    "            if len(self.reward_history) >= window:\n",
    "                print(f\"\\nLast {window} episodes:\")\n",
    "                print(f\"Average reward: {np.mean(self.reward_history[-window:]):.2f}\")\n",
    "                print(f\"Reward std dev: {np.std(self.reward_history[-window:]):.2f}\")\n",
    "                print(f\"Average Q-value: {np.mean(self.episode_avg_q_values[-window:]):.2f}\")\n",
    "                print(\"\\nFinal EDRs:\")\n",
    "                for goal in self.goalEdges:\n",
    "                    print(f\"Goal {goal}: {np.mean(self.episode_edrs[goal][-window:]):.4f}\")\n",
    "                print(\"\\nFinal Fairness Metrics:\")\n",
    "                print(f\"Jain's Index: {np.mean(self.fairness_history['jains_index'][-window:]):.4f}\")\n",
    "                print(f\"Min-Max Ratio: {np.mean(self.fairness_history['min_max_ratio'][-window:]):.4f}\")\n",
    "                print(f\"CV: {np.mean(self.fairness_history['cv'][-window:]):.4f}\")\n",
    "\n",
    "# Example usage\n",
    "edges = [(0, 1), (1, 2), (2, 3), (3,4)]\n",
    "goalEdges = [(0, 2), (1, 4)]\n",
    "pSwap = 0.8\n",
    "pGen = 0.8\n",
    "maxAge = 2\n",
    "\n",
    "# First generate all transitions using your existing code\n",
    "all_transitions = generateAllStateTransitions(edges, goalEdges, pSwap, pGen, maxAge)\n",
    "\n",
    "# Create and train agent\n",
    "agent = QuantumNetworkQLearning(\n",
    "    edges=edges,\n",
    "    goalEdges=goalEdges,\n",
    "    all_transitions=all_transitions,\n",
    "    pSwap=pSwap,\n",
    "    pGen=pGen,\n",
    "    maxAge=maxAge\n",
    ")\n",
    "agent.train(num_episodes=100000, max_steps=3000)\n",
    "agent.plot_results()\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\n=== Final Statistics ===\")\n",
    "print(f\"Parameters: pSwap={pSwap}, pGen={pGen}, maxAge={maxAge}\")\n",
    "print(f\"Network: edges={edges}, goals={goalEdges}\")\n",
    "\n",
    "print(\"\\nFinal EDRs (Last Episode):\")\n",
    "total_timesteps = len(next(iter(agent.edr_history.values())))\n",
    "for goal in agent.goalEdges:\n",
    "    final_edr = agent.goal_success_counts[goal] / total_timesteps\n",
    "    print(f\"Goal {goal}: EDR = {final_edr:.6f}\")\n",
    "    print(f\"  - Successes: {agent.goal_success_counts[goal]}\")\n",
    "    print(f\"  - Total timesteps: {total_timesteps}\")\n",
    "\n",
    "print(f\"\\nFinal Average Reward (last 100 episodes): {np.mean(agent.reward_history[-100:]):.4f}\")\n",
    "\n",
    "# Print final fairness metrics\n",
    "final_edrs = {goal: agent.goal_success_counts[goal] / total_timesteps for goal in agent.goalEdges}\n",
    "jains, minmax, cv = calculate_fairness_metrics(final_edrs)\n",
    "print(\"\\nFinal Fairness Metrics (Last Episode):\")\n",
    "print(f\"Jain's Fairness Index: {jains:.4f}\")\n",
    "print(f\"Min-Max Ratio: {minmax:.4f}\")\n",
    "print(f\"Coefficient of Variation: {cv:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
